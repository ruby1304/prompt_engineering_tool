import streamlit as st
import json
import pandas as pd
import asyncio
from datetime import datetime
import time
# ä¿®æ”¹å¯¼å…¥æ–¹å¼
from config import get_template_list, load_template, get_test_set_list, load_test_set, save_result, get_available_models, load_config
from models.api_clients import get_client, get_provider_from_model
from models.token_counter import count_tokens, estimate_cost
from utils.evaluator import PromptEvaluator
from utils.common import render_prompt_template, run_test

def render_test_runner():
    st.title("ğŸ§ª æµ‹è¯•è¿è¡Œ")
    
    # é€‰æ‹©è¦æµ‹è¯•çš„æç¤ºè¯æ¨¡æ¿å’Œæµ‹è¯•é›†
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("é€‰æ‹©æç¤ºè¯æ¨¡æ¿")
        
        template_list = get_template_list()
        
        if not template_list:
            st.warning("æœªæ‰¾åˆ°æç¤ºè¯æ¨¡æ¿ï¼Œè¯·å…ˆåˆ›å»ºæ¨¡æ¿")
            return
        
        selected_templates = []
        
        if "test_mode" not in st.session_state:
            st.session_state.test_mode = "single_prompt_multi_model"
        
        test_mode = st.radio(
            "æµ‹è¯•æ¨¡å¼",
            ["single_prompt_multi_model", "multi_prompt_single_model"],
            format_func=lambda x: "å•æç¤ºè¯å¤šæ¨¡å‹" if x == "single_prompt_multi_model" else "å¤šæç¤ºè¯å•æ¨¡å‹"
        )
        st.session_state.test_mode = test_mode
        
        if test_mode == "single_prompt_multi_model":
            selected_template = st.selectbox(
                "é€‰æ‹©æç¤ºè¯æ¨¡æ¿",
                template_list
            )
            if selected_template:
                selected_templates = [selected_template]
        else:
            # å¤šé€‰æç¤ºè¯æ¨¡æ¿
            for template_name in template_list:
                if st.checkbox(template_name, key=f"temp_{template_name}"):
                    selected_templates.append(template_name)
    
    with col2:
        st.subheader("é€‰æ‹©æµ‹è¯•é›†")
        
        test_set_list = get_test_set_list()
        
        if not test_set_list:
            st.warning("æœªæ‰¾åˆ°æµ‹è¯•é›†ï¼Œè¯·å…ˆåˆ›å»ºæµ‹è¯•é›†")
            return
        
        selected_test_set = st.selectbox(
            "é€‰æ‹©æµ‹è¯•é›†",
            test_set_list
        )
    
    if not selected_templates or not selected_test_set:
        st.warning("è¯·é€‰æ‹©æç¤ºè¯æ¨¡æ¿å’Œæµ‹è¯•é›†")
        return
    
    # åŠ è½½é€‰æ‹©çš„æ¨¡æ¿å’Œæµ‹è¯•é›†
    templates = [load_template(name) for name in selected_templates]
    test_set = load_test_set(selected_test_set)
    
    # æ¨¡å‹é€‰æ‹©å’Œå‚æ•°è®¾ç½®
    st.subheader("æ¨¡å‹å’Œå‚æ•°è®¾ç½®")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("é€‰æ‹©æ¨¡å‹")
        
        # ä½¿ç”¨ç»„ä»¶é€‰æ‹©æ¨¡å‹
        if test_mode == "single_prompt_multi_model":
            # å¤šæ¨¡å‹é€‰æ‹©
            from ui.components import select_multiple_models
            selected_model_pairs = select_multiple_models(key_prefix="test_run", label="é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹")
            
            # æå–æ¨¡å‹åç§°å’Œæä¾›å•†ä¿¡æ¯
            selected_models = [model for model, _ in selected_model_pairs]
            model_provider_map = {model: provider for model, provider in selected_model_pairs}
            
            # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.model_provider_map = model_provider_map
        else:
            # å•æ¨¡å‹é€‰æ‹©
            from ui.components import select_single_model
            model, provider = select_single_model(key_prefix="test_run_single", help_text="é€‰æ‹©ç”¨äºæµ‹è¯•çš„æ¨¡å‹")
            
            selected_models = [model] if model else []
            if model:
                st.session_state.model_provider_map = {model: provider}
        
        if not selected_models:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹")
            return
    
    with col2:
        st.subheader("è¿è¡Œå‚æ•°")
        
        temperature = st.slider("Temperature", 0.0, 2.0, 0.7, 0.1)
        max_tokens = st.slider("æœ€å¤§è¾“å‡ºToken", 100, 4000, 1000, 100)
        repeat_count = st.slider("æ¯ä¸ªæµ‹è¯•é‡å¤æ¬¡æ•°", 1, 5, 2, 1)
    
    # æ˜¾ç¤ºå½“å‰çš„è¯„ä¼°å™¨è®¾ç½®ï¼ˆè€Œä¸æ˜¯å…è®¸æ›´æ”¹ï¼‰
    config = load_config()
    current_evaluator = config.get("evaluator_model", "gpt-4")
    use_local_eval = config.get("use_local_evaluation", False)
    provider = get_provider_from_model(current_evaluator)
    
    with st.expander("å½“å‰è¯„ä¼°å™¨è®¾ç½®", expanded=False):
        st.info(f"""
        - è¯„ä¼°æ¨¡å‹: **{current_evaluator}** ({provider})
        - æœ¬åœ°è¯„ä¼°: **{"å¯ç”¨" if use_local_eval else "ç¦ç”¨"}**
        
        *è¦æ›´æ”¹è¯„ä¼°æ¨¡å‹è®¾ç½®ï¼Œè¯·å‰å¾€ [APIå¯†é’¥ä¸æä¾›å•†ç®¡ç† > è¯„ä¼°æ¨¡å‹æµ‹è¯•] é¡µé¢*
        """)
    
    # é¢„è§ˆæµ‹è¯•é…ç½®
    st.subheader("æµ‹è¯•é¢„è§ˆ")
    
    # è·å–æ¨¡å‹æ˜¾ç¤ºä¿¡æ¯
    model_display_info = []
    for model in selected_models:
        provider = st.session_state.model_provider_map.get(model, "æœªçŸ¥æä¾›å•†")
        model_display_info.append(f"{model} ({provider})")
    
    preview_data = {
        "æç¤ºè¯æ¨¡æ¿": [t["name"] for t in templates],
        "æµ‹è¯•é›†": test_set["name"],
        "æµ‹è¯•ç”¨ä¾‹æ•°": len(test_set["cases"]),
        "é€‰æ‹©çš„æ¨¡å‹": model_display_info,
        "é‡å¤æ¬¡æ•°": repeat_count,
        "è¯„ä¼°å™¨æ¨¡å‹": current_evaluator
    }
    
    st.json(preview_data)
    
    # ä¼°ç®—æµ‹è¯•æˆæœ¬å’Œæ—¶é—´
    total_calls = len(templates) * len(test_set["cases"]) * len(selected_models) * repeat_count
    avg_token_count = 1000  # å‡è®¾å¹³å‡æ¯æ¬¡è°ƒç”¨1000ä¸ªtoken
    total_tokens = total_calls * avg_token_count
    
    # ä¼°ç®—æˆæœ¬ï¼ˆéå¸¸ç²—ç•¥ï¼‰
    estimated_cost = sum([estimate_cost(avg_token_count, model) * len(test_set["cases"]) * repeat_count for model in selected_models])
    
    # ä¼°ç®—æ—¶é—´ï¼ˆå‡è®¾æ¯æ¬¡è°ƒç”¨å¹³å‡2ç§’ï¼‰
    estimated_time = total_calls * 2
    
    st.info(f"""
    ### æµ‹è¯•ä¼°ç®—
    - æ€»APIè°ƒç”¨æ¬¡æ•°: {total_calls}
    - é¢„ä¼°Tokenæ•°é‡: {total_tokens}
    - é¢„ä¼°æˆæœ¬: ${estimated_cost:.2f}
    - é¢„ä¼°å®Œæˆæ—¶é—´: {estimated_time} ç§’ (çº¦ {estimated_time//60}åˆ†{estimated_time%60}ç§’)
    """)
    
    # è¿è¡Œæµ‹è¯•
    if st.button("â–¶ï¸ è¿è¡Œæµ‹è¯•", type="primary"):
        run_tests(
            templates=templates,
            test_set=test_set,
            selected_models=selected_models,
            temperature=temperature,
            max_tokens=max_tokens,
            repeat_count=repeat_count,
            test_mode=test_mode
        )

def run_tests(templates, test_set, selected_models, temperature, max_tokens, repeat_count, test_mode):
    """è¿è¡Œæµ‹è¯•å¹¶æ˜¾ç¤ºè¿›åº¦ï¼ˆå¹¶å‘é‡æ„ç‰ˆï¼‰"""
    st.subheader("æµ‹è¯•è¿è¡Œä¸­...")
    progress_bar = st.progress(0)
    status_text = st.empty()
    result_area = st.empty()
    total_tasks = len(templates) * len(selected_models)
    completed_tasks = 0
    results = {}
    for template in templates:
        template_name = template["name"]
        results[template_name] = {
            "template": template,
            "test_set": test_set["name"],
            "models": selected_models,
            "params": {
                "temperature": temperature,
                "max_tokens": max_tokens
            },
            "test_cases": []
        }
        for model in selected_models:
            provider = st.session_state.model_provider_map.get(model) if hasattr(st.session_state, 'model_provider_map') else None
            # è°ƒç”¨ run_test å¹¶å‘æ‰¹é‡æµ‹è¯•
            test_result = run_test(
                template=template,
                model=model,
                test_set=test_set,
                model_provider=provider,
                repeat_count=repeat_count,
                temperature=temperature
            )
            # åˆå¹¶ç»“æœ
            if test_result and "test_cases" in test_result:
                # æŒ‰æ¨¡å‹åŒºåˆ†ç»“æœ
                for idx, case in enumerate(test_result["test_cases"]):
                    case["model"] = model
                    results[template_name]["test_cases"].append(case)
            completed_tasks += 1
            progress_bar.progress(completed_tasks / total_tasks)
            result_area.text(f"å·²å®Œæˆ: {completed_tasks}/{total_tasks} ç»„ (æ¨¡æ¿Ã—æ¨¡å‹) æµ‹è¯•")
    progress_bar.progress(1.0)
    status_text.text("âœ… æµ‹è¯•å®Œæˆ!")
    result_name = f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    save_result(result_name, results)
    st.success(f"æµ‹è¯•ç»“æœå·²ä¿å­˜: {result_name}")
    from ui.components import display_test_case_details
    st.subheader("æµ‹è¯•ç»“æœé¢„è§ˆ")
    for template_name, template_result in results.items():
        st.markdown(f"#### æç¤ºè¯æ¨¡æ¿: {template_name}")
        for i, case in enumerate(template_result["test_cases"]):
            st.markdown(f"æµ‹è¯•ç”¨ä¾‹ {i+1}: {case.get('case_description', case.get('case_id', '') )} (æ¨¡å‹: {case.get('model')})")
            display_test_case_details(case, show_system_prompt=True, inside_expander=False)
    st.session_state.last_result = result_name
    st.session_state.page = "results_viewer"
    st.rerun()