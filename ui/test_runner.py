import streamlit as st
import json
import pandas as pd
import asyncio
from datetime import datetime
import time
# ä¿®æ”¹å¯¼å…¥æ–¹å¼
from config import get_template_list, load_template, get_test_set_list, load_test_set, save_result, get_available_models
from models.api_clients import get_client, get_provider_from_model
from models.token_counter import count_tokens, estimate_cost
from utils.evaluator import PromptEvaluator

def render_test_runner():
    st.title("ğŸ§ª æµ‹è¯•è¿è¡Œ")
    
    # é€‰æ‹©è¦æµ‹è¯•çš„æç¤ºè¯æ¨¡æ¿å’Œæµ‹è¯•é›†
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("é€‰æ‹©æç¤ºè¯æ¨¡æ¿")
        
        template_list = get_template_list()
        
        if not template_list:
            st.warning("æœªæ‰¾åˆ°æç¤ºè¯æ¨¡æ¿ï¼Œè¯·å…ˆåˆ›å»ºæ¨¡æ¿")
            return
        
        selected_templates = []
        
        if "test_mode" not in st.session_state:
            st.session_state.test_mode = "single_prompt_multi_model"
        
        test_mode = st.radio(
            "æµ‹è¯•æ¨¡å¼",
            ["single_prompt_multi_model", "multi_prompt_single_model"],
            format_func=lambda x: "å•æç¤ºè¯å¤šæ¨¡å‹" if x == "single_prompt_multi_model" else "å¤šæç¤ºè¯å•æ¨¡å‹"
        )
        st.session_state.test_mode = test_mode
        
        if test_mode == "single_prompt_multi_model":
            selected_template = st.selectbox(
                "é€‰æ‹©æç¤ºè¯æ¨¡æ¿",
                template_list
            )
            if selected_template:
                selected_templates = [selected_template]
        else:
            # å¤šé€‰æç¤ºè¯æ¨¡æ¿
            for template_name in template_list:
                if st.checkbox(template_name, key=f"temp_{template_name}"):
                    selected_templates.append(template_name)
    
    with col2:
        st.subheader("é€‰æ‹©æµ‹è¯•é›†")
        
        test_set_list = get_test_set_list()
        
        if not test_set_list:
            st.warning("æœªæ‰¾åˆ°æµ‹è¯•é›†ï¼Œè¯·å…ˆåˆ›å»ºæµ‹è¯•é›†")
            return
        
        selected_test_set = st.selectbox(
            "é€‰æ‹©æµ‹è¯•é›†",
            test_set_list
        )
    
    if not selected_templates or not selected_test_set:
        st.warning("è¯·é€‰æ‹©æç¤ºè¯æ¨¡æ¿å’Œæµ‹è¯•é›†")
        return
    
    # åŠ è½½é€‰æ‹©çš„æ¨¡æ¿å’Œæµ‹è¯•é›†
    templates = [load_template(name) for name in selected_templates]
    test_set = load_test_set(selected_test_set)
    
    # æ¨¡å‹é€‰æ‹©å’Œå‚æ•°è®¾ç½®
    st.subheader("æ¨¡å‹å’Œå‚æ•°è®¾ç½®")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("é€‰æ‹©æ¨¡å‹")
        
        # ä½¿ç”¨ç»„ä»¶é€‰æ‹©æ¨¡å‹
        if test_mode == "single_prompt_multi_model":
            # å¤šæ¨¡å‹é€‰æ‹©
            from ui.components import select_multiple_models
            selected_model_pairs = select_multiple_models(key_prefix="test_run", label="é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹")
            
            # æå–æ¨¡å‹åç§°å’Œæä¾›å•†ä¿¡æ¯
            selected_models = [model for model, _ in selected_model_pairs]
            model_provider_map = {model: provider for model, provider in selected_model_pairs}
            
            # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.model_provider_map = model_provider_map
        else:
            # å•æ¨¡å‹é€‰æ‹©
            from ui.components import select_single_model
            model, provider = select_single_model(key_prefix="test_run_single", help_text="é€‰æ‹©ç”¨äºæµ‹è¯•çš„æ¨¡å‹")
            
            selected_models = [model] if model else []
            if model:
                st.session_state.model_provider_map = {model: provider}
        
        if not selected_models:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹")
            return
    
    
    with col2:
        st.subheader("è¿è¡Œå‚æ•°")
        
        temperature = st.slider("Temperature", 0.0, 2.0, 0.7, 0.1)
        max_tokens = st.slider("æœ€å¤§è¾“å‡ºToken", 100, 4000, 1000, 100)
        repeat_count = st.slider("æ¯ä¸ªæµ‹è¯•é‡å¤æ¬¡æ•°", 1, 5, 2, 1)
    
    # é¢„è§ˆæµ‹è¯•é…ç½®
    st.subheader("æµ‹è¯•é¢„è§ˆ")
    
    # è·å–æ¨¡å‹æ˜¾ç¤ºä¿¡æ¯
    model_display_info = []
    for model in selected_models:
        provider = st.session_state.model_provider_map.get(model, "æœªçŸ¥æä¾›å•†")
        model_display_info.append(f"{model} ({provider})")
    
    preview_data = {
        "æç¤ºè¯æ¨¡æ¿": [t["name"] for t in templates],
        "æµ‹è¯•é›†": test_set["name"],
        "æµ‹è¯•ç”¨ä¾‹æ•°": len(test_set["cases"]),
        "é€‰æ‹©çš„æ¨¡å‹": model_display_info,
        "é‡å¤æ¬¡æ•°": repeat_count
    }
    
    st.json(preview_data)
    
    # ä¼°ç®—æµ‹è¯•æˆæœ¬å’Œæ—¶é—´
    total_calls = len(templates) * len(test_set["cases"]) * len(selected_models) * repeat_count
    avg_token_count = 1000  # å‡è®¾å¹³å‡æ¯æ¬¡è°ƒç”¨1000ä¸ªtoken
    total_tokens = total_calls * avg_token_count
    
    # ä¼°ç®—æˆæœ¬ï¼ˆéå¸¸ç²—ç•¥ï¼‰
    estimated_cost = sum([estimate_cost(avg_token_count, model) * len(test_set["cases"]) * repeat_count for model in selected_models])
    
    # ä¼°ç®—æ—¶é—´ï¼ˆå‡è®¾æ¯æ¬¡è°ƒç”¨å¹³å‡2ç§’ï¼‰
    estimated_time = total_calls * 2
    
    st.info(f"""
    ### æµ‹è¯•ä¼°ç®—
    - æ€»APIè°ƒç”¨æ¬¡æ•°: {total_calls}
    - é¢„ä¼°Tokenæ•°é‡: {total_tokens}
    - é¢„ä¼°æˆæœ¬: ${estimated_cost:.2f}
    - é¢„ä¼°å®Œæˆæ—¶é—´: {estimated_time} ç§’ (çº¦ {estimated_time//60}åˆ†{estimated_time%60}ç§’)
    """)
    
    # è¿è¡Œæµ‹è¯•
    if st.button("â–¶ï¸ è¿è¡Œæµ‹è¯•", type="primary"):
        run_tests(
            templates=templates,
            test_set=test_set,
            selected_models=selected_models,
            temperature=temperature,
            max_tokens=max_tokens,
            repeat_count=repeat_count,
            test_mode=test_mode
        )

def run_tests(templates, test_set, selected_models, temperature, max_tokens, repeat_count, test_mode):
    """è¿è¡Œæµ‹è¯•å¹¶æ˜¾ç¤ºè¿›åº¦"""
    st.subheader("æµ‹è¯•è¿è¡Œä¸­...")
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    result_area = st.empty()
    
    # è®¡ç®—æ€»ä»»åŠ¡æ•°
    total_tasks = len(templates) * len(test_set["cases"]) * len(selected_models) * repeat_count
    completed_tasks = 0
    
    # å‡†å¤‡ç»“æœå­˜å‚¨
    results = {}
    for template in templates:
        results[template["name"]] = {
            "template": template,
            "test_set": test_set["name"],
            "models": selected_models,
            "params": {
                "temperature": temperature,
                "max_tokens": max_tokens
            },
            "test_cases": []
        }
    
    # è®¾ç½®è¯„ä¼°å™¨
    evaluator = PromptEvaluator()
    
    # è¿è¡Œæµ‹è¯•
    for template in templates:
        template_name = template["name"]
        status_text.text(f"æ­£åœ¨æµ‹è¯•æç¤ºè¯æ¨¡æ¿: {template_name}")
        
        for case in test_set["cases"]:
            case_id = case["id"]
            status_text.text(f"æ­£åœ¨æµ‹è¯•æ¨¡æ¿ '{template_name}' çš„ç”¨ä¾‹ '{case_id}'")
            
            # æ¸²æŸ“æç¤ºè¯ï¼ˆæ›¿æ¢å˜é‡ï¼‰
            prompt_template = template["template"]
            
            # åº”ç”¨å…¨å±€å˜é‡å’Œç”¨ä¾‹å˜é‡
            variables = {**test_set.get("variables", {}), **case.get("variables", {})}
            
            # å¦‚æœå˜é‡æœªæä¾›ï¼Œä½¿ç”¨æç¤ºè¯æ¨¡æ¿ä¸­çš„é»˜è®¤å€¼
            for var_name in template.get("variables", {}):
                if var_name not in variables:
                    variables[var_name] = template["variables"][var_name].get("default", "")

            # åº”ç”¨å˜é‡åˆ°æç¤ºè¯æ¨¡æ¿  
            for var_name, var_value in variables.items():
                prompt_template = prompt_template.replace(f"{{{{{var_name}}}}}", var_value)
            
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = case.get("user_input", "")
            
            # ä¿å­˜å½“å‰æµ‹è¯•ç”¨ä¾‹çš„ç»“æœ
            case_results = {
                "case_id": case_id,
                "case_description": case.get("description", ""),
                "prompt": prompt_template,
                "user_input": user_input,
                "expected_output": case.get("expected_output", ""),
                "model_responses": [],
                "evaluation": None
            }
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹è¿è¡Œæµ‹è¯•
            for model in selected_models:
                # è·å–æ¨¡å‹å¯¹åº”çš„æä¾›å•†
                if hasattr(st.session_state, 'model_provider_map') and model in st.session_state.model_provider_map:
                    provider = st.session_state.model_provider_map[model]
                else:
                    # å…¼å®¹æ—§ä»£ç ï¼Œå°è¯•ä»æ¨¡å‹åç§°æ¨æ–­æä¾›å•†
                    try:
                        provider = get_provider_from_model(model)
                    except ValueError:
                        st.error(f"æ— æ³•ç¡®å®šæ¨¡å‹ '{model}' çš„æä¾›å•†")
                        continue
                
                client = get_client(provider)
                
                # é‡å¤æµ‹è¯•
                for i in range(repeat_count):
                    status_text.text(f"æ­£åœ¨æµ‹è¯•æ¨¡æ¿ '{template_name}' çš„ç”¨ä¾‹ '{case_id}', æ¨¡å‹ '{model}', é‡å¤ #{i+1}")
                    
                    try:
                        # ä¿®æ”¹è°ƒç”¨æ¨¡å‹APIçš„æ–¹å¼ï¼Œå°†æç¤ºè¯ä½œä¸ºç³»ç»Ÿæç¤ºï¼Œç”¨æˆ·è¾“å…¥ä½œä¸ºç”¨æˆ·æ¶ˆæ¯
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        
                        # æ ¹æ®ä¸åŒå®¢æˆ·ç«¯ç±»å‹æ„å»ºä¸åŒçš„æ¶ˆæ¯æ ¼å¼
                        if provider in ["openai", "xai"]:
                            response = loop.run_until_complete(client.generate_with_messages(
                                [
                                    {"role": "system", "content": prompt_template},
                                    {"role": "user", "content": user_input}
                                ],
                                model, 
                                {"temperature": temperature, "max_tokens": max_tokens}
                            ))
                        else:
                            # å¯¹äºå…¶ä»–APIå®¢æˆ·ç«¯ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦è°ƒæ•´æ¶ˆæ¯æ ¼å¼æˆ–è€…åˆå¹¶å†…å®¹
                            combined_prompt = f"System: {prompt_template}\n\nUser: {user_input}"
                            response = loop.run_until_complete(client.generate(
                                combined_prompt, 
                                model, 
                                {"temperature": temperature, "max_tokens": max_tokens}
                            ))
                        
                        loop.close()
                        
                        # å­˜å‚¨å“åº”
                        case_results["model_responses"].append({
                            "model": model,
                            "attempt": i+1,
                            "response": response.get("text", ""),
                            "error": response.get("error", None),
                            "usage": response.get("usage", {})
                        })
                        
                    except Exception as e:
                        # å­˜å‚¨é”™è¯¯
                        case_results["model_responses"].append({
                            "model": model,
                            "attempt": i+1,
                            "response": "",
                            "error": str(e),
                            "usage": {}
                        })
                    
                    # æ›´æ–°è¿›åº¦
                    completed_tasks += 1
                    progress_bar.progress(completed_tasks / total_tasks)
                    
                    # æ¨¡æ‹ŸAPIè°ƒç”¨å»¶è¿Ÿ
                    time.sleep(0.5)
            
            # å¯¹æµ‹è¯•ç»“æœè¿›è¡Œè¯„ä¼°
            # é€‰æ‹©æœ€åä¸€æ¬¡å“åº”è¿›è¡Œè¯„ä¼°
            response_text = ""
            for resp in reversed(case_results["model_responses"]):
                if not resp.get("error") and resp.get("response"):
                    response_text = resp.get("response")
                    break
            if response_text:
                try:
                    # ä½¿ç”¨åŒæ­¥æ–¹æ³•æ›¿ä»£
                    evaluation = evaluator.evaluate_response_sync(
                        response_text,
                        case.get("expected_output", ""),
                        case.get("evaluation_criteria", {}),
                        prompt_template
                    )
                    case_results["evaluation"] = evaluation
                except Exception as e:
                    case_results["evaluation"] = {"error": str(e)}
            
            # æ·»åŠ åˆ°ç»“æœ
            results[template_name]["test_cases"].append(case_results)
            
            # æ˜¾ç¤ºä¸­é—´ç»“æœ
            result_summary = f"å·²å®Œæˆ: {completed_tasks}/{total_tasks} æµ‹è¯•"
            result_area.text(result_summary)
    
    # æµ‹è¯•å®Œæˆ
    progress_bar.progress(1.0)
    status_text.text("âœ… æµ‹è¯•å®Œæˆ!")
    
    # ä¿å­˜ç»“æœ
    result_name = f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    save_result(result_name, results)
    
    st.success(f"æµ‹è¯•ç»“æœå·²ä¿å­˜: {result_name}")
    
    # å»ºè®®è·³è½¬åˆ°ç»“æœæŸ¥çœ‹é¡µé¢
    st.session_state.last_result = result_name
    if st.button("ğŸ“Š æŸ¥çœ‹è¯¦ç»†ç»“æœ"):
        st.session_state.page = "results_viewer"
        st.experimental_rerun()