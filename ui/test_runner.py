# ui/test_runner.py
import streamlit as st
import pandas as pd
import asyncio
from datetime import datetime
import time
import json
from config import get_template_list, load_template, get_test_set_list, load_test_set, save_result, get_available_models
from models.api_clients import get_client, get_provider_from_model
from models.token_counter import count_tokens, estimate_cost
from utils.evaluator import PromptEvaluator
from utils.common import run_test
from ui.components.layout import page_header, tabs_section
from ui.components.selectors import select_single_model, select_multiple_models, select_template, select_test_set
from ui.components.cards import info_card, result_card, display_test_summary, display_response_tabs, display_evaluation_results
from ui.components.tables import results_table
from ui.components.forms import test_config_form

def render_test_runner():
    """æµ‹è¯•è¿è¡Œé¡µé¢"""
    # ä½¿ç”¨å¸ƒå±€ç»„ä»¶æ˜¾ç¤ºé¡µé¢æ ‡é¢˜
    page_header("æµ‹è¯•è¿è¡Œ", "è¿è¡Œæç¤ºè¯æµ‹è¯•å¹¶è¯„ä¼°ç»“æœ", "ğŸ§ª")
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "test_mode" not in st.session_state:
        st.session_state.test_mode = "single_prompt_multi_model"
    
    if "test_results" not in st.session_state:
        st.session_state.test_results = None
    
    if "test_is_running" not in st.session_state:
        st.session_state.test_is_running = False
    
    # å®šä¹‰å„æ ‡ç­¾é¡µæ¸²æŸ“å‡½æ•°
    def render_test_config():
        """æ¸²æŸ“æµ‹è¯•é…ç½®æ ‡ç­¾é¡µ"""
        st.markdown("## æµ‹è¯•é…ç½®")
        
        # æµ‹è¯•æ¨¡å¼é€‰æ‹©
        test_mode = st.radio(
            "æµ‹è¯•æ¨¡å¼", 
            ["single_prompt_multi_model", "multi_prompt_single_model"],
            format_func=lambda x: "å•æ¨¡æ¿å¤šæ¨¡å‹" if x == "single_prompt_multi_model" else "å¤šæ¨¡æ¿å•æ¨¡å‹",
            key="test_mode_selector",
            horizontal=True
        )
        
        st.session_state.test_mode = test_mode
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("é€‰æ‹©æç¤ºè¯æ¨¡æ¿")
            template_list = get_template_list()
            
            if not template_list:
                st.warning("æœªæ‰¾åˆ°æç¤ºè¯æ¨¡æ¿ï¼Œè¯·å…ˆåˆ›å»ºæ¨¡æ¿")
                return
            
            # æ ¹æ®æµ‹è¯•æ¨¡å¼é€‰æ‹©å•ä¸ªæˆ–å¤šä¸ªæ¨¡æ¿
            if test_mode == "single_prompt_multi_model":
                selected_template = select_template(
                    template_list, 
                    "é€‰æ‹©æ¨¡æ¿", 
                    "test_template", 
                    "é€‰æ‹©è¦æµ‹è¯•çš„æç¤ºè¯æ¨¡æ¿"
                )
                if selected_template:
                    st.session_state.selected_templates = [selected_template]
                
                # æ˜¾ç¤ºé€‰ä¸­æ¨¡æ¿çš„ä¿¡æ¯
                if "selected_templates" in st.session_state and st.session_state.selected_templates:
                    try:
                        template = load_template(st.session_state.selected_templates[0])
                        if template:
                            st.success(f"å·²é€‰æ‹©æ¨¡æ¿: {template.get('name', '')}")
                            with st.expander("æŸ¥çœ‹æ¨¡æ¿è¯¦æƒ…", expanded=False):
                                st.markdown(f"**æè¿°**: {template.get('description', 'æ— æè¿°')}")
                                st.markdown("**æ¨¡æ¿å†…å®¹**:")
                                st.code(template.get("template", ""), language="markdown")
                                
                                # æ˜¾ç¤ºå˜é‡
                                if template.get("variables"):
                                    st.markdown("**å˜é‡**:")
                                    for var_name, var_config in template.get("variables", {}).items():
                                        st.markdown(f"- **{var_name}**: {var_config.get('description', '')} (é»˜è®¤: `{var_config.get('default', '')}`)")
                    except Exception as e:
                        st.error(f"åŠ è½½æ¨¡æ¿æ—¶å‡ºé”™: {str(e)}")
            else:
                selected_templates = select_template(
                    template_list, 
                    "é€‰æ‹©å¤šä¸ªæ¨¡æ¿", 
                    "test_templates", 
                    "é€‰æ‹©è¦æµ‹è¯•çš„å¤šä¸ªæç¤ºè¯æ¨¡æ¿",
                    allow_multiple=True
                )
                st.session_state.selected_templates = selected_templates
                
                # æ˜¾ç¤ºé€‰ä¸­æ¨¡æ¿æ•°é‡
                if "selected_templates" in st.session_state and st.session_state.selected_templates:
                    st.success(f"å·²é€‰æ‹© {len(st.session_state.selected_templates)} ä¸ªæ¨¡æ¿")
                    with st.expander("æŸ¥çœ‹é€‰ä¸­çš„æ¨¡æ¿", expanded=False):
                        for template_name in st.session_state.selected_templates:
                            st.markdown(f"- {template_name}")
        
        with col2:
            st.subheader("é€‰æ‹©æµ‹è¯•é›†")
            test_set_list = get_test_set_list()
            
            if not test_set_list:
                st.warning("æœªæ‰¾åˆ°æµ‹è¯•é›†ï¼Œè¯·å…ˆåˆ›å»ºæµ‹è¯•é›†")
                return
            
            selected_test_set = select_test_set(
                test_set_list, 
                "é€‰æ‹©æµ‹è¯•é›†", 
                "test_set", 
                "é€‰æ‹©è¦ä½¿ç”¨çš„æµ‹è¯•é›†"
            )
            st.session_state.selected_test_set = selected_test_set
            
            # æ˜¾ç¤ºé€‰ä¸­æµ‹è¯•é›†çš„ä¿¡æ¯
            if "selected_test_set" in st.session_state and st.session_state.selected_test_set:
                try:
                    test_set = load_test_set(st.session_state.selected_test_set)
                    if test_set:
                        st.success(f"å·²é€‰æ‹©æµ‹è¯•é›†: {test_set.get('name', '')}")
                        with st.expander("æŸ¥çœ‹æµ‹è¯•é›†è¯¦æƒ…", expanded=False):
                            st.markdown(f"**æè¿°**: {test_set.get('description', 'æ— æè¿°')}")
                            st.markdown(f"**æµ‹è¯•ç”¨ä¾‹æ•°**: {len(test_set.get('cases', []))}")
                            
                            # æ˜¾ç¤ºæµ‹è¯•ç”¨ä¾‹æ‘˜è¦
                            if test_set.get("cases"):
                                test_cases = []
                                for case in test_set.get("cases", []):
                                    test_cases.append({
                                        "ID": case.get("id", ""),
                                        "æè¿°": case.get("description", ""),
                                        "è¯„ä¼°æ ‡å‡†æ•°": len(case.get("evaluation_criteria", {}))
                                    })
                                
                                if test_cases:
                                    st.dataframe(pd.DataFrame(test_cases), use_container_width=True)
                except Exception as e:
                    st.error(f"åŠ è½½æµ‹è¯•é›†æ—¶å‡ºé”™: {str(e)}")
    
    def render_model_selection():
        """æ¸²æŸ“æ¨¡å‹é€‰æ‹©æ ‡ç­¾é¡µ"""
        st.markdown("## æ¨¡å‹é€‰æ‹©")
        
        # è·å–æµ‹è¯•æ¨¡å¼
        test_mode = st.session_state.get("test_mode", "single_prompt_multi_model")
        
        if test_mode == "single_prompt_multi_model":
            st.markdown("### é€‰æ‹©å¤šä¸ªæ¨¡å‹")
            st.markdown("åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œå°†ä½¿ç”¨å•ä¸ªæç¤ºè¯æ¨¡æ¿æµ‹è¯•å¤šä¸ªæ¨¡å‹")
            
            selected_models = select_multiple_models(
                "test_models", 
                "é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹ï¼ˆå¯å¤šé€‰ï¼‰"
            )
            
            st.session_state.selected_models = selected_models
            
            # æ˜¾ç¤ºé€‰ä¸­çš„æ¨¡å‹
            if selected_models:
                st.success(f"å·²é€‰æ‹© {len(selected_models)} ä¸ªæ¨¡å‹")
                with st.expander("æŸ¥çœ‹é€‰ä¸­çš„æ¨¡å‹", expanded=False):
                    for model_info in selected_models:
                        st.markdown(f"- {model_info['model']} ({model_info['provider']})")
            else:
                st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ¨¡å‹")
        else:
            st.markdown("### é€‰æ‹©å•ä¸ªæ¨¡å‹")
            st.markdown("åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œå°†ä½¿ç”¨å¤šä¸ªæç¤ºè¯æ¨¡æ¿æµ‹è¯•å•ä¸ªæ¨¡å‹")
            
            model, provider = select_single_model(
                "test_model", 
                "é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹"
            )
            
            if model and provider:
                st.session_state.selected_models = [{"model": model, "provider": provider}]
                st.success(f"å·²é€‰æ‹©æ¨¡å‹: {model} ({provider})")
            else:
                st.warning("è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹")
    
    def render_test_execution():
        """æ¸²æŸ“æµ‹è¯•æ‰§è¡Œæ ‡ç­¾é¡µ"""
        st.markdown("## æµ‹è¯•æ‰§è¡Œ")
        
        # æ£€æŸ¥æ˜¯å¦å·²é€‰æ‹©æ‰€éœ€å…ƒç´ 
        has_templates = "selected_templates" in st.session_state and st.session_state.selected_templates
        has_test_set = "selected_test_set" in st.session_state and st.session_state.selected_test_set
        has_models = "selected_models" in st.session_state and st.session_state.selected_models
        
        if not (has_templates and has_test_set and has_models):
            missing = []
            if not has_templates:
                missing.append("æç¤ºè¯æ¨¡æ¿")
            if not has_test_set:
                missing.append("æµ‹è¯•é›†")
            if not has_models:
                missing.append("æ¨¡å‹")
            
            st.warning(f"è¯·å…ˆåœ¨å‰é¢çš„æ ‡ç­¾é¡µä¸­é€‰æ‹©{', '.join(missing)}")
            return
        
        # æ˜¾ç¤ºæµ‹è¯•é…ç½®æ‘˜è¦
        with st.expander("æµ‹è¯•é…ç½®æ‘˜è¦", expanded=True):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.markdown("**æç¤ºè¯æ¨¡æ¿**")
                for template_name in st.session_state.selected_templates:
                    st.markdown(f"- {template_name}")
            
            with col2:
                st.markdown("**æµ‹è¯•é›†**")
                st.markdown(f"- {st.session_state.selected_test_set}")
                
                # åŠ è½½æµ‹è¯•é›†ä¿¡æ¯
                try:
                    test_set = load_test_set(st.session_state.selected_test_set)
                    if test_set:
                        st.markdown(f"- æµ‹è¯•ç”¨ä¾‹æ•°: {len(test_set.get('cases', []))}")
                except Exception:
                    pass
            
            with col3:
                st.markdown("**æ¨¡å‹**")
                for model_info in st.session_state.selected_models:
                    st.markdown(f"- {model_info['model']} ({model_info['provider']})")
        
        # æµ‹è¯•é€‰é¡¹è®¾ç½®
        with st.expander("æµ‹è¯•é€‰é¡¹", expanded=True):
            col1, col2 = st.columns(2)
            
            with col1:
                # æ¨¡å‹å‚æ•°
                st.markdown("**æ¨¡å‹å‚æ•°**")
                temperature = st.slider(
                    "Temperature", 
                    min_value=0.0, 
                    max_value=1.0, 
                    value=0.7, 
                    step=0.1,
                    help="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œè¾ƒé«˜çš„å€¼å°†ä½¿è¾“å‡ºæ›´éšæœºï¼Œè¾ƒä½çš„å€¼ä½¿è¾“å‡ºæ›´ç¡®å®š"
                )
                
                max_tokens = st.number_input(
                    "æœ€å¤§è¾“å‡ºTokenæ•°", 
                    min_value=1, 
                    max_value=4096, 
                    value=1024,
                    help="é™åˆ¶æ¨¡å‹å“åº”çš„æœ€å¤§é•¿åº¦"
                )
                
                # æ·»åŠ æµ‹è¯•è¿è¡Œæ¬¡æ•°é…ç½®
                num_runs = st.number_input(
                    "æ¯ä¸ªé…ç½®è¿è¡Œæ¬¡æ•°", 
                    min_value=1, 
                    max_value=10, 
                    value=1,
                    help="æ¯ä¸ªæ¨¡å‹-æ¨¡æ¿-æµ‹è¯•ç”¨ä¾‹ç»„åˆè¿è¡Œçš„æ¬¡æ•°"
                )
            
            with col2:
                # è¯„ä¼°é€‰é¡¹
                st.markdown("**è¯„ä¼°é€‰é¡¹**")
                run_evaluation = st.checkbox(
                    "è‡ªåŠ¨è¯„ä¼°å“åº”", 
                    value=True,
                    help="ä½¿ç”¨è¯„ä¼°æ¨¡å‹å¯¹ç”Ÿæˆçš„å“åº”è¿›è¡Œè¯„åˆ†"
                )
                
                if run_evaluation:
                    # ä½¿ç”¨ç»Ÿä¸€çš„è¯„ä¼°æ¨¡å‹é€‰æ‹©å™¨
                    from ui.components.selectors import select_evaluator_model
                    evaluator_model, evaluator_provider = select_evaluator_model(
                        "test_evaluator", 
                        "é€‰æ‹©ç”¨äºè¯„ä¼°å“åº”è´¨é‡çš„æ¨¡å‹"
                    )
                else:
                    evaluator_model = None
                    evaluator_provider = None
        
        # è¿è¡Œæµ‹è¯•æŒ‰é’®
        run_col1, run_col2 = st.columns([2, 1])
        
        with run_col1:
            if st.button("â–¶ï¸ å¼€å§‹æµ‹è¯•", key="start_test_btn", use_container_width=True, type="primary"):
                # è®¾ç½®çŠ¶æ€ä¸ºæ­£åœ¨è¿è¡Œ
                st.session_state.test_is_running = True
                st.session_state.test_results = None
                
                # æ”¶é›†æµ‹è¯•é…ç½®
                test_config = {
                    "templates": st.session_state.selected_templates,
                    "test_set": st.session_state.selected_test_set,
                    "models": st.session_state.selected_models,
                    "params": {
                        "temperature": temperature,
                        "max_tokens": max_tokens,
                        "num_runs": num_runs  # æ·»åŠ è¿è¡Œæ¬¡æ•°
                    },
                    "evaluation": {
                        "run": run_evaluation,
                        "model": evaluator_model,
                        "provider": evaluator_provider
                    },
                    "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S")
                }
                
                # å­˜å‚¨æµ‹è¯•é…ç½®
                st.session_state.test_config = test_config
                st.experimental_rerun()
        
        with run_col2:
            if st.button("ğŸ”„ é‡ç½®", key="reset_test_btn", use_container_width=True):
                # é‡ç½®æµ‹è¯•çŠ¶æ€
                st.session_state.test_is_running = False
                st.session_state.test_results = None
                st.session_state.test_config = None
                st.experimental_rerun()
        
        # æ‰§è¡Œæµ‹è¯•
        if st.session_state.get("test_is_running", False):
            with st.spinner("æ­£åœ¨è¿è¡Œæµ‹è¯•..."):
                # ä½¿ç”¨asyncioè¿è¡Œå¼‚æ­¥æµ‹è¯•å‡½æ•°
                test_results = run_test_with_progress(st.session_state.test_config)
                st.session_state.test_results = test_results
                st.session_state.test_is_running = False
                
                if test_results:
                    # ä¿å­˜æµ‹è¯•ç»“æœ
                    result_name = f"test_results_{st.session_state.test_config['timestamp']}.json"
                    save_result(result_name, test_results)
                    st.session_state.last_result = result_name
                    
                    # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
                    st.success("æµ‹è¯•å®Œæˆï¼")
                    
                    # è®¾ç½®å½“å‰æ ‡ç­¾é¡µç´¢å¼•ä¸ºç»“æœæ ‡ç­¾é¡µï¼ˆç´¢å¼•ä¸º3ï¼‰
                    st.session_state.active_tab = 3
                else:
                    st.error("æµ‹è¯•æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
                
                st.experimental_rerun()
    
    def render_test_results():
        """æ¸²æŸ“æµ‹è¯•ç»“æœæ ‡ç­¾é¡µ"""
        st.markdown("## æµ‹è¯•ç»“æœ")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•ç»“æœ
        if not st.session_state.get("test_results"):
            st.info('å°šæœªè¿è¡Œæµ‹è¯•æˆ–æ²¡æœ‰æµ‹è¯•ç»“æœã€‚è¯·åœ¨"æµ‹è¯•æ‰§è¡Œ"æ ‡ç­¾é¡µè¿è¡Œæµ‹è¯•ã€‚')
            return
        
        # è·å–æµ‹è¯•ç»“æœ
        results = st.session_state.test_results
        
        # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
        st.markdown("### æµ‹è¯•ç»“æœæ‘˜è¦")
        
        # åˆ›å»ºæ‘˜è¦å¡ç‰‡
        summary_cols = st.columns(4)
        
        with summary_cols[0]:
            result_card(
                "æµ‹è¯•å®Œæˆæ—¶é—´", 
                results.get("timestamp", "æœªçŸ¥"),
                "æµ‹è¯•æ‰§è¡Œçš„æ—¶é—´æˆ³"
            )
        
        with summary_cols[1]:
            result_card(
                "æµ‹è¯•æ¨¡æ¿æ•°", 
                len(results.get("templates", {})),
                "å‚ä¸æµ‹è¯•çš„æç¤ºè¯æ¨¡æ¿æ•°é‡"
            )
        
        with summary_cols[2]:
            result_card(
                "æµ‹è¯•ç”¨ä¾‹æ•°", 
                len(results.get("test_cases", [])),
                "æ‰§è¡Œçš„æµ‹è¯•ç”¨ä¾‹æ•°é‡"
            )
        
        with summary_cols[3]:
            result_card(
                "æµ‹è¯•æ¨¡å‹æ•°", 
                len(results.get("models", [])),
                "å‚ä¸æµ‹è¯•çš„æ¨¡å‹æ•°é‡"
            )
        
        # æ˜¾ç¤ºæµ‹è¯•ç”¨ä¾‹ç»“æœ
        st.markdown("### æµ‹è¯•ç”¨ä¾‹ç»“æœ")
        
        # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹é€‰æ‹©å™¨
        test_cases = results.get("test_cases", [])
        if not test_cases:
            st.warning("æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹ç»“æœ")
            return
        
        case_options = [f"{case.get('id', '')} - {case.get('description', '')}" for case in test_cases]
        selected_case_option = st.selectbox(
            "é€‰æ‹©æµ‹è¯•ç”¨ä¾‹æŸ¥çœ‹è¯¦ç»†ç»“æœ",
            case_options,
            key="select_result_case"
        )
        
        # è·å–é€‰ä¸­çš„æµ‹è¯•ç”¨ä¾‹
        selected_case_id = selected_case_option.split(" - ")[0] if selected_case_option else None
        selected_case = next((case for case in test_cases if case.get("id") == selected_case_id), None)
        
        if selected_case:
            # æ˜¾ç¤ºç”¨ä¾‹è¯¦æƒ…
            st.markdown(f"#### æµ‹è¯•ç”¨ä¾‹: {selected_case.get('description', '')}")
            
            # åˆ›å»ºå“åº”æ ‡ç­¾é¡µ
            if "responses" in selected_case and selected_case["responses"]:
                display_response_tabs(selected_case["responses"], key_prefix=f"case_{selected_case_id}")
            else:
                st.info("æ­¤æµ‹è¯•ç”¨ä¾‹æ²¡æœ‰å“åº”æ•°æ®")
            
            # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
            if "evaluation" in selected_case:
                st.markdown("#### è¯„ä¼°ç»“æœ")
                display_evaluation_results(selected_case["evaluation"], key_prefix=f"eval_{selected_case_id}")
            
        # æ¨¡å‹æ¯”è¾ƒ
        st.markdown("### æ¨¡å‹æ¯”è¾ƒ")
        
        # å‡†å¤‡æ¨¡å‹æ¯”è¾ƒæ•°æ®
        models_data = prepare_model_comparison_data(results)
        
        if models_data:
            # åˆ›å»ºæ¨¡å‹æ¯”è¾ƒè¡¨æ ¼
            st.dataframe(pd.DataFrame(models_data), use_container_width=True)
            
            # å¯ä»¥æ·»åŠ å›¾è¡¨å±•ç¤º
            # TODO: æ·»åŠ æ¨¡å‹æ¯”è¾ƒå›¾è¡¨
        else:
            st.info("æ— æ³•åˆ›å»ºæ¨¡å‹æ¯”è¾ƒæ•°æ®ï¼Œå¯èƒ½ç¼ºå°‘è¯„ä¼°ç»“æœ")
    
    # è®¾ç½®æ ‡ç­¾é¡µ
    tabs_config = [
        {"title": "æµ‹è¯•é…ç½®", "content": render_test_config},
        {"title": "æ¨¡å‹é€‰æ‹©", "content": render_model_selection},
        {"title": "æµ‹è¯•æ‰§è¡Œ", "content": render_test_execution},
        {"title": "æµ‹è¯•ç»“æœ", "content": render_test_results}
    ]
    
    tabs_section(tabs_config)

# è¾…åŠ©å‡½æ•°
def run_test_with_progress(test_config):
    """è¿è¡Œæµ‹è¯•å¹¶æ˜¾ç¤ºè¿›åº¦"""
    # åŠ è½½æµ‹è¯•é›†
    test_set = load_test_set(test_config["test_set"])
    if not test_set or "cases" not in test_set:
        st.error("æ— æ³•åŠ è½½æµ‹è¯•é›†æˆ–æµ‹è¯•é›†ä¸åŒ…å«æµ‹è¯•ç”¨ä¾‹")
        return None
    
    # å‡†å¤‡ç»“æœç»“æ„
    results = {
        "timestamp": test_config["timestamp"],
        "templates": {},
        "models": test_config["models"],
        "test_cases": [],
        "total_tokens": 0,
        "total_responses": 0,
        "total_evaluations": 0
    }
    
    # åŠ è½½æ¨¡æ¿
    for template_name in test_config["templates"]:
        template = load_template(template_name)
        if template:
            results["templates"][template_name] = template
    
    # å‡†å¤‡æµ‹è¯•ç”¨ä¾‹
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_cases = len(test_set["cases"])
    total_models = len(test_config["models"])
    total_templates = len(test_config["templates"])
    num_runs = test_config["params"].get("num_runs", 1)  # è·å–è¿è¡Œæ¬¡æ•°ï¼Œé»˜è®¤ä¸º1
    
    total_tests = total_cases * total_models * total_templates
    completed_tests = 0
    
    # å¤„ç†æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹
    for i, case in enumerate(test_set["cases"]):
        case_result = {
            "id": case.get("id", f"case_{i}"),
            "description": case.get("description", ""),
            "user_input": case.get("user_input", ""),
            "expected_output": case.get("expected_output", ""),
            "evaluation_criteria": case.get("evaluation_criteria", {}),
            "responses": []
        }
        
        status_text.text(f"æµ‹è¯•ç”¨ä¾‹ {i+1}/{total_cases}: {case.get('description', '')}")
        
        # å¯¹æ¯ä¸ªæ¨¡æ¿å’Œæ¨¡å‹ç»„åˆè¿è¡Œæµ‹è¯•
        for template_name in test_config["templates"]:
            template = results["templates"].get(template_name)
            if not template:
                st.warning(f"æ— æ³•åŠ è½½æ¨¡æ¿: {template_name}ï¼Œè·³è¿‡")
                continue
            
            for model_info in test_config["models"]:
                model = model_info["model"]
                provider = model_info["provider"]
                
                status_text.text(f"æµ‹è¯•ç”¨ä¾‹ {i+1}/{total_cases}, æ¨¡æ¿: {template_name}, æ¨¡å‹: {model}")
                
                try:
                    # è°ƒç”¨ä¿®æ”¹åçš„run_testå‡½æ•°
                    test_results = run_test(
                        template=template,  # æç¤ºè¯æ¨¡æ¿
                        model=model,  # æ¨¡å‹åç§°
                        test_set={  # å•ç‹¬ä¸ºè¿™ä¸ªæµ‹è¯•ç”¨ä¾‹åˆ›å»ºä¸€ä¸ªæµ‹è¯•é›†
                            "cases": [case],
                            "variables": test_set.get("variables", {})
                        },
                        model_provider=provider,  # æ¨¡å‹æä¾›å•†
                        repeat_count=num_runs,  # é‡å¤æ¬¡æ•°
                        temperature=test_config["params"].get("temperature", 0.7)  # æ¸©åº¦å‚æ•°
                    )
                    
                    # å¤„ç†æµ‹è¯•ç»“æœ
                    if test_results and "test_cases" in test_results and test_results["test_cases"]:
                        case_test_results = test_results["test_cases"][0]  # åªæœ‰ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹
                        
                        # æå–æ‰€æœ‰å“åº”
                        for resp_data in case_test_results.get("responses", []):
                            # åˆ›å»ºå“åº”å¯¹è±¡
                            response = {
                                "model": model,
                                "provider": provider,
                                "template": template_name,
                                "content": resp_data.get("response", ""),
                                "error": resp_data.get("error"),
                                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                                "run_index": resp_data.get("attempt", 1),
                                "tokens": {
                                    "prompt": resp_data.get("usage", {}).get("prompt_tokens", 0),
                                    "completion": resp_data.get("usage", {}).get("completion_tokens", 0),
                                    "total": resp_data.get("usage", {}).get("total_tokens", 0)
                                }
                            }
                            
                            # æ·»åŠ è¯„ä¼°ç»“æœ
                            if resp_data.get("evaluation"):
                                response["evaluation"] = resp_data["evaluation"]
                                results["total_evaluations"] += 1
                            
                            # æ·»åŠ åˆ°å“åº”åˆ—è¡¨
                            case_result["responses"].append(response)
                            results["total_responses"] += 1
                            
                            # æ›´æ–°tokenç»Ÿè®¡
                            results["total_tokens"] += response["tokens"]["total"]
                
                except Exception as e:
                    st.error(f"è¿è¡Œæµ‹è¯•æ—¶å‡ºé”™ (æ¨¡æ¿: {template_name}, æ¨¡å‹: {model}): {str(e)}")
                    # æ·»åŠ é”™è¯¯ä¿¡æ¯
                    case_result["responses"].append({
                        "model": model,
                        "provider": provider,
                        "template": template_name,
                        "content": f"é”™è¯¯: {str(e)}",
                        "error": True,
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "run_index": 1
                    })
                
                # æ›´æ–°è¿›åº¦
                completed_tests += 1
                progress_bar.progress(completed_tests / total_tests)
        
        # è®¡ç®—ç”¨ä¾‹çš„æ•´ä½“è¯„ä¼°ï¼ˆå¦‚æœæœ‰å¤šä¸ªå“åº”ï¼‰
        if case_result["responses"] and any("evaluation" in resp for resp in case_result["responses"]):
            # æ”¶é›†æœ‰è¯„ä¼°ç»“æœçš„å“åº”
            evaluated_responses = [resp for resp in case_result["responses"] if "evaluation" in resp]
            
            if evaluated_responses:
                # è®¡ç®—å¹³å‡åˆ†æ•°
                overall_scores = [resp["evaluation"].get("overall_score", 0) for resp in evaluated_responses]
                dimension_scores = {}
                
                # æ”¶é›†æ‰€æœ‰ç»´åº¦åˆ†æ•°
                for resp in evaluated_responses:
                    for dim, score in resp["evaluation"].get("scores", {}).items():
                        if dim not in dimension_scores:
                            dimension_scores[dim] = []
                        dimension_scores[dim].append(score)
                
                # è®¡ç®—å¹³å‡ç»´åº¦åˆ†æ•°
                avg_dimension_scores = {
                    dim: sum(scores) / len(scores) 
                    for dim, scores in dimension_scores.items()
                }
                
                # æ·»åŠ æ•´ä½“è¯„ä¼°
                case_result["evaluation"] = {
                    "overall_score": sum(overall_scores) / len(overall_scores),
                    "scores": avg_dimension_scores,
                    "num_responses": len(evaluated_responses)
                }
        
        # æ·»åŠ ç”¨ä¾‹ç»“æœ
        results["test_cases"].append(case_result)
    
    # æ¸…é™¤è¿›åº¦æ˜¾ç¤º
    progress_bar.empty()
    status_text.empty()
    
    # è®¡ç®—æ•´ä½“å¹³å‡åˆ†æ•°
    if results["total_evaluations"] > 0:
        # æ”¶é›†æ‰€æœ‰è¯„åˆ†
        all_scores = []
        for case in results["test_cases"]:
            if "evaluation" in case and "overall_score" in case["evaluation"]:
                all_scores.append(case["evaluation"]["overall_score"])
        
        if all_scores:
            results["average_score"] = sum(all_scores) / len(all_scores)
            results["max_score"] = max(all_scores)
            results["min_score"] = min(all_scores)
    
    return results


def prepare_model_comparison_data(results):
    """å‡†å¤‡æ¨¡å‹æ¯”è¾ƒæ•°æ®"""
    if not results or "test_cases" not in results:
        return []
    
    model_scores = {}
    
    # æ”¶é›†æ¯ä¸ªæ¨¡å‹åœ¨æ¯ä¸ªç”¨ä¾‹ä¸­çš„è¯„åˆ†
    for case in results.get("test_cases", []):
        for response in case.get("responses", []):
            model = response.get("model")
            template = response.get("template")
            run_index = response.get("run_index", 1)
            
            if "evaluation" not in response:
                continue
            
            eval_result = response["evaluation"]
            overall_score = eval_result.get("overall_score", 0)
            
            # åˆå§‹åŒ–æ¨¡å‹æ•°æ®
            if model not in model_scores:
                model_scores[model] = {
                    "æ¨¡å‹": model,
                    "å¹³å‡å¾—åˆ†": 0,
                    "å“åº”æ•°": 0,
                    "å“åº”æ€»æ•°": 0,
                    "è¿è¡Œæ¬¡æ•°": set()  # ä½¿ç”¨é›†åˆè·Ÿè¸ªä¸åŒçš„è¿è¡Œæ¬¡æ•°
                }
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            model_scores[model]["å“åº”æ€»æ•°"] += 1
            model_scores[model]["è¿è¡Œæ¬¡æ•°"].add(run_index)
            
            if overall_score > 0:
                model_scores[model]["å“åº”æ•°"] += 1
                # ç´¯è®¡å¾—åˆ†
                current_total = model_scores[model]["å¹³å‡å¾—åˆ†"] * (model_scores[model]["å“åº”æ•°"] - 1)
                model_scores[model]["å¹³å‡å¾—åˆ†"] = (current_total + overall_score) / model_scores[model]["å“åº”æ•°"]
    
    # è½¬æ¢ä¸ºåˆ—è¡¨
    model_data = list(model_scores.values())
    
    # æ ¼å¼åŒ–å¹³å‡å¾—åˆ†å’Œè¿è¡Œæ¬¡æ•°
    for item in model_data:
        item["å¹³å‡å¾—åˆ†"] = f"{item['å¹³å‡å¾—åˆ†']:.2f}"
        item["è¿è¡Œæ¬¡æ•°"] = len(item["è¿è¡Œæ¬¡æ•°"])  # è½¬æ¢é›†åˆä¸ºæ•°é‡
    
    return model_data
