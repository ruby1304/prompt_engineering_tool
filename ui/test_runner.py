import streamlit as st
import json
import pandas as pd
import asyncio
from datetime import datetime
import time
# ä¿®æ”¹å¯¼å…¥æ–¹å¼
from config import get_template_list, load_template, get_test_set_list, load_test_set, save_result, get_available_models, load_config
from models.api_clients import get_client, get_provider_from_model
from models.token_counter import count_tokens, estimate_cost
from utils.evaluator import PromptEvaluator
from utils.common import render_prompt_template, run_test

def render_test_runner():
    st.title("ğŸ§ª æµ‹è¯•è¿è¡Œ")
    
    # é€‰æ‹©è¦æµ‹è¯•çš„æç¤ºè¯æ¨¡æ¿å’Œæµ‹è¯•é›†
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("é€‰æ‹©æç¤ºè¯æ¨¡æ¿")
        
        template_list = get_template_list()
        
        if not template_list:
            st.warning("æœªæ‰¾åˆ°æç¤ºè¯æ¨¡æ¿ï¼Œè¯·å…ˆåˆ›å»ºæ¨¡æ¿")
            return
        
        selected_templates = []
        
        if "test_mode" not in st.session_state:
            st.session_state.test_mode = "single_prompt_multi_model"
        
        test_mode = st.radio(
            "æµ‹è¯•æ¨¡å¼",
            ["single_prompt_multi_model", "multi_prompt_single_model"],
            format_func=lambda x: "å•æç¤ºè¯å¤šæ¨¡å‹" if x == "single_prompt_multi_model" else "å¤šæç¤ºè¯å•æ¨¡å‹"
        )
        st.session_state.test_mode = test_mode
        
        if test_mode == "single_prompt_multi_model":
            selected_template = st.selectbox(
                "é€‰æ‹©æç¤ºè¯æ¨¡æ¿",
                template_list
            )
            if selected_template:
                selected_templates = [selected_template]
        else:
            # å¤šé€‰æç¤ºè¯æ¨¡æ¿
            for template_name in template_list:
                if st.checkbox(template_name, key=f"temp_{template_name}"):
                    selected_templates.append(template_name)
    
    with col2:
        st.subheader("é€‰æ‹©æµ‹è¯•é›†")
        
        test_set_list = get_test_set_list()
        
        if not test_set_list:
            st.warning("æœªæ‰¾åˆ°æµ‹è¯•é›†ï¼Œè¯·å…ˆåˆ›å»ºæµ‹è¯•é›†")
            return
        
        selected_test_set = st.selectbox(
            "é€‰æ‹©æµ‹è¯•é›†",
            test_set_list
        )
    
    if not selected_templates or not selected_test_set:
        st.warning("è¯·é€‰æ‹©æç¤ºè¯æ¨¡æ¿å’Œæµ‹è¯•é›†")
        return
    
    # åŠ è½½é€‰æ‹©çš„æ¨¡æ¿å’Œæµ‹è¯•é›†
    templates = [load_template(name) for name in selected_templates]
    test_set = load_test_set(selected_test_set)
    
    # æ¨¡å‹é€‰æ‹©å’Œå‚æ•°è®¾ç½®
    st.subheader("æ¨¡å‹å’Œå‚æ•°è®¾ç½®")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("é€‰æ‹©æ¨¡å‹")
        
        # ä½¿ç”¨ç»„ä»¶é€‰æ‹©æ¨¡å‹
        if test_mode == "single_prompt_multi_model":
            # å¤šæ¨¡å‹é€‰æ‹©
            from ui.components import select_multiple_models
            selected_model_pairs = select_multiple_models(key_prefix="test_run", label="é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹")
            
            # æå–æ¨¡å‹åç§°å’Œæä¾›å•†ä¿¡æ¯
            selected_models = [model for model, _ in selected_model_pairs]
            model_provider_map = {model: provider for model, provider in selected_model_pairs}
            
            # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.model_provider_map = model_provider_map
        else:
            # å•æ¨¡å‹é€‰æ‹©
            from ui.components import select_single_model
            model, provider = select_single_model(key_prefix="test_run_single", help_text="é€‰æ‹©ç”¨äºæµ‹è¯•çš„æ¨¡å‹")
            
            selected_models = [model] if model else []
            if model:
                st.session_state.model_provider_map = {model: provider}
        
        if not selected_models:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹")
            return
    
    with col2:
        st.subheader("è¿è¡Œå‚æ•°")
        
        temperature = st.slider("Temperature", 0.0, 2.0, 0.7, 0.1)
        max_tokens = st.slider("æœ€å¤§è¾“å‡ºToken", 100, 4000, 1000, 100)
        repeat_count = st.slider("æ¯ä¸ªæµ‹è¯•é‡å¤æ¬¡æ•°", 1, 5, 2, 1)
    
    # æ˜¾ç¤ºå½“å‰çš„è¯„ä¼°å™¨è®¾ç½®ï¼ˆè€Œä¸æ˜¯å…è®¸æ›´æ”¹ï¼‰
    config = load_config()
    current_evaluator = config.get("evaluator_model", "gpt-4")
    use_local_eval = config.get("use_local_evaluation", False)
    provider = get_provider_from_model(current_evaluator)
    
    with st.expander("å½“å‰è¯„ä¼°å™¨è®¾ç½®", expanded=False):
        st.info(f"""
        - è¯„ä¼°æ¨¡å‹: **{current_evaluator}** ({provider})
        - æœ¬åœ°è¯„ä¼°: **{"å¯ç”¨" if use_local_eval else "ç¦ç”¨"}**
        
        *è¦æ›´æ”¹è¯„ä¼°æ¨¡å‹è®¾ç½®ï¼Œè¯·å‰å¾€ [APIå¯†é’¥ä¸æä¾›å•†ç®¡ç† > è¯„ä¼°æ¨¡å‹æµ‹è¯•] é¡µé¢*
        """)
    
    # é¢„è§ˆæµ‹è¯•é…ç½®
    st.subheader("æµ‹è¯•é¢„è§ˆ")
    
    # è·å–æ¨¡å‹æ˜¾ç¤ºä¿¡æ¯
    model_display_info = []
    for model in selected_models:
        provider = st.session_state.model_provider_map.get(model, "æœªçŸ¥æä¾›å•†")
        model_display_info.append(f"{model} ({provider})")
    
    preview_data = {
        "æç¤ºè¯æ¨¡æ¿": [t["name"] for t in templates],
        "æµ‹è¯•é›†": test_set["name"],
        "æµ‹è¯•ç”¨ä¾‹æ•°": len(test_set["cases"]),
        "é€‰æ‹©çš„æ¨¡å‹": model_display_info,
        "é‡å¤æ¬¡æ•°": repeat_count,
        "è¯„ä¼°å™¨æ¨¡å‹": current_evaluator
    }
    
    st.json(preview_data)
    
    # ä¼°ç®—æµ‹è¯•æˆæœ¬å’Œæ—¶é—´
    total_calls = len(templates) * len(test_set["cases"]) * len(selected_models) * repeat_count
    avg_token_count = 1000  # å‡è®¾å¹³å‡æ¯æ¬¡è°ƒç”¨1000ä¸ªtoken
    total_tokens = total_calls * avg_token_count
    
    # ä¼°ç®—æˆæœ¬ï¼ˆéå¸¸ç²—ç•¥ï¼‰
    estimated_cost = sum([estimate_cost(avg_token_count, model) * len(test_set["cases"]) * repeat_count for model in selected_models])
    
    # ä¼°ç®—æ—¶é—´ï¼ˆå‡è®¾æ¯æ¬¡è°ƒç”¨å¹³å‡2ç§’ï¼‰
    estimated_time = total_calls * 2
    
    st.info(f"""
    ### æµ‹è¯•ä¼°ç®—
    - æ€»APIè°ƒç”¨æ¬¡æ•°: {total_calls}
    - é¢„ä¼°Tokenæ•°é‡: {total_tokens}
    - é¢„ä¼°æˆæœ¬: ${estimated_cost:.2f}
    - é¢„ä¼°å®Œæˆæ—¶é—´: {estimated_time} ç§’ (çº¦ {estimated_time//60}åˆ†{estimated_time%60}ç§’)
    """)
    
    # è¿è¡Œæµ‹è¯•
    if st.button("â–¶ï¸ è¿è¡Œæµ‹è¯•", type="primary"):
        run_tests(
            templates=templates,
            test_set=test_set,
            selected_models=selected_models,
            temperature=temperature,
            max_tokens=max_tokens,
            repeat_count=repeat_count,
            test_mode=test_mode
        )

def run_tests(templates, test_set, selected_models, temperature, max_tokens, repeat_count, test_mode):
    """è¿è¡Œæµ‹è¯•å¹¶æ˜¾ç¤ºè¿›åº¦ï¼ˆå¹¶å‘é‡æ„ç‰ˆï¼‰"""
    st.subheader("æµ‹è¯•è¿è¡Œä¸­...")
    progress_bar = st.progress(0)
    status_text = st.empty()
    result_area = st.empty() # Keep this for potential future detailed status
    
    # Calculate total attempts based on cases and repeats
    total_cases = len(test_set.get("cases", []))
    total_attempts = len(templates) * len(selected_models) * total_cases * repeat_count
    completed_attempts = 0
    
    # Define the progress callback function
    def update_progress():
        nonlocal completed_attempts
        completed_attempts += 1
        progress = completed_attempts / total_attempts if total_attempts > 0 else 0
        # Ensure progress doesn't exceed 1.0 due to potential floating point issues
        progress = min(progress, 1.0)
        progress_bar.progress(progress)
        status_text.text(f"è¿è¡Œä¸­... å·²å®Œæˆ {completed_attempts}/{total_attempts} æ¬¡æ¨¡å‹è°ƒç”¨")

    results = {}
    all_test_results = [] # Store results from run_test calls

    # --- Main Test Loop --- 
    # Iterate through templates and models to call run_test
    for template in templates:
        template_name = template["name"]
        template_results_for_models = []
        for model in selected_models:
            provider = st.session_state.model_provider_map.get(model) if hasattr(st.session_state, 'model_provider_map') else None
            
            status_text.text(f"æ­£åœ¨è¿è¡Œ: æ¨¡æ¿ '{template_name}' - æ¨¡å‹ '{model}'...")
            
            # Call run_test with the progress callback
            test_result = run_test(
                template=template,
                model=model,
                test_set=test_set,
                model_provider=provider,
                repeat_count=repeat_count,
                temperature=temperature,
                progress_callback=update_progress # Pass the callback here
            )
            
            if test_result:
                template_results_for_models.append(test_result)
            else:
                st.warning(f"æ¨¡æ¿ '{template_name}' - æ¨¡å‹ '{model}' æµ‹è¯•è¿è¡Œå¤±è´¥æˆ–æœªè¿”å›ç»“æœã€‚")
        
        # Store results grouped by template after processing all models for it
        if template_results_for_models:
             # Aggregate results for the current template from different models
            aggregated_cases = []
            for res in template_results_for_models:
                for case in res.get("test_cases", []):
                    # Add model info to each case if not already present (should be added by run_test)
                    if "model" not in case:
                         case["model"] = res.get("model")
                    aggregated_cases.append(case)
            
            results[template_name] = {
                "template": template,
                "test_set": test_set["name"],
                "models": selected_models, # List all models tested with this template
                "params": {
                    "temperature": temperature,
                    "max_tokens": max_tokens # Assuming max_tokens was intended here, though not passed to run_test
                },
                "test_cases": aggregated_cases # Combined cases from all models for this template
            }

    # --- Post-Test Processing --- 
    # Ensure progress bar reaches 100% and update status
    progress_bar.progress(1.0)
    status_text.text(f"âœ… æµ‹è¯•å®Œæˆ! å…±æ‰§è¡Œ {completed_attempts}/{total_attempts} æ¬¡æ¨¡å‹è°ƒç”¨ã€‚")
    result_area.empty() # Clear the intermediate status area

    # Save results
    result_name = f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    save_result(result_name, results)
    st.success(f"æµ‹è¯•ç»“æœå·²ä¿å­˜: {result_name}")

    # Display results preview
    from ui.components import display_test_case_details
    st.subheader("æµ‹è¯•ç»“æœé¢„è§ˆ")
    # Check if results dictionary is empty
    if not results:
        st.warning("æ²¡æœ‰ç”Ÿæˆä»»ä½•æµ‹è¯•ç»“æœã€‚è¯·æ£€æŸ¥æ¨¡å‹é€‰æ‹©å’Œæµ‹è¯•é…ç½®ã€‚")
        return
        
    for template_name, template_result in results.items():
        st.markdown(f"#### æç¤ºè¯æ¨¡æ¿: {template_name}")
        if not template_result.get("test_cases"):
            st.warning(f"æ¨¡æ¿ '{template_name}' æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç”¨ä¾‹ç»“æœã€‚")
            continue
            
        # Display results grouped by case ID first, then show different model responses/evals
        cases_grouped = {}
        for case in template_result["test_cases"]:
            case_id = case.get("case_id", "unknown_case")
            if case_id not in cases_grouped:
                cases_grouped[case_id] = {
                    "description": case.get("case_description", case_id),
                    "details": []
                }
            cases_grouped[case_id]["details"].append(case)
            
        case_counter = 1
        for case_id, group_data in cases_grouped.items():
            st.markdown(f"**æµ‹è¯•ç”¨ä¾‹ {case_counter}: {group_data['description']}**")
            # Display details for each model run for this case
            for case_detail in group_data["details"]:
                 st.markdown(f"*æ¨¡å‹: {case_detail.get('model', 'æœªçŸ¥')}*", help=f"Prompt used:\n```\n{case_detail.get('prompt', 'N/A')}\n```")
                 display_test_case_details(case_detail, show_system_prompt=False, inside_expander=True) # Use expander for cleaner look
            case_counter += 1
            st.divider()

    # Navigate to results viewer
    st.session_state.last_result = result_name
    st.session_state.page = "results_viewer"
    st.rerun()