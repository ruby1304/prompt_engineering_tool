# prompt_batch_ab_test.py

import streamlit as st
import json
import pandas as pd
import asyncio
from datetime import datetime
import time
import plotly.express as px
import numpy as np

from config import load_test_set, save_template
from utils.common import (
    calculate_average_score, 
    get_dimension_scores, 
    analyze_response_stability,
    create_score_bar_chart,
    run_test,
    save_optimized_template,
    compare_dimension_performance
)
from ui.components import (
    display_test_summary,
    display_response_tabs,
    display_evaluation_results,
    display_test_case_details
)

def render_prompt_batch_ab_test():
    st.title("ğŸ§ª æç¤ºè¯æ‰¹é‡è¯„ä¼°")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹é‡A/Bæµ‹è¯•æ•°æ®
    if (not hasattr(st.session_state, 'batch_ab_test_original') or 
        not hasattr(st.session_state, 'batch_ab_test_optimized')):
        st.warning("è¯·å…ˆä»æç¤ºè¯ä¸“é¡¹ä¼˜åŒ–é¡µé¢å¯åŠ¨æ‰¹é‡è¯„ä¼°")
        
        if st.button("è¿”å›æç¤ºè¯ä¸“é¡¹ä¼˜åŒ–"):
            st.session_state.page = "prompt_optimization"
            st.rerun()
        return
    
    # è·å–æ‰¹é‡A/Bæµ‹è¯•æ•°æ®
    original_template = st.session_state.batch_ab_test_original
    optimized_templates = st.session_state.batch_ab_test_optimized
    model = st.session_state.batch_ab_test_model
    model_provider = st.session_state.get("batch_ab_test_model_provider")
    test_set_name = st.session_state.batch_ab_test_test_set
    
    # è·å–åŸå§‹æµ‹è¯•ç»“æœï¼ˆä»ä¸“é¡¹ä¼˜åŒ–çš„æµ‹è¯•ç»“æœä¸­è·å–ï¼Œä¸é‡æ–°è¯„ä¼°ï¼‰
    original_results = None
    if hasattr(st.session_state, 'specialized_test_results'):
        original_results = st.session_state.specialized_test_results
        
        # è·å–æµ‹è¯•å‚æ•°
        test_params = original_results.get("test_params", {})
        temperature = test_params.get("temperature", 0.7)
        repeat_count = test_params.get("repeat_count", 2)
    else:
        # é»˜è®¤å€¼ï¼Œå®é™…ä¸Šè¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿ
        temperature = 0.7
        repeat_count = 2
    
    st.markdown(f"""
    ### æ‰¹é‡è¯„ä¼°: åŸå§‹æç¤ºè¯ vs {len(optimized_templates)}ä¸ªä¼˜åŒ–ç‰ˆæœ¬
    
    - **æ¨¡å‹**: {model} ({model_provider if model_provider else "æœªæŒ‡å®šæä¾›å•†"})
    - **æµ‹è¯•é›†**: {test_set_name}
    - **ä¼˜åŒ–ç‰ˆæœ¬æ•°**: {len(optimized_templates)}
    - **è¯„ä¼°å‚æ•°**: æ¸©åº¦ {temperature}, æ¯ä¸ªæµ‹è¯•é‡å¤ {repeat_count} æ¬¡
    """)
    
    # æ˜¾ç¤ºæç¤ºè¯æ¦‚è§ˆ
    with st.expander("æŸ¥çœ‹æ‰€æœ‰æç¤ºè¯"):
        st.subheader("åŸå§‹æç¤ºè¯")
        from utils.common import display_template_info
        display_template_info(original_template, inside_expander=True)
        
        for i, opt_template in enumerate(optimized_templates):
            st.subheader(f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}")
            display_template_info(opt_template, inside_expander=True)
    
    # ç§»é™¤æ‰‹åŠ¨æµ‹è¯•å‚æ•°è®¾ç½®ï¼Œä½¿ç”¨ä¸åŸå§‹ä¼˜åŒ–æµ‹è¯•ç›¸åŒçš„å‚æ•°
    st.info(f"""
    **æ³¨æ„**: æ‰¹é‡è¯„ä¼°ä½¿ç”¨ä¸æç¤ºè¯ä¸“é¡¹ä¼˜åŒ–ç›¸åŒçš„å‚æ•°ï¼š
    - æ¸©åº¦ (Temperature): **{temperature}**
    - æ¯ä¸ªæµ‹è¯•é‡å¤æ¬¡æ•°: **{repeat_count}**
    
    è¿™ç¡®ä¿äº†è¯„ä¼°ç»“æœçš„ä¸€è‡´æ€§å’Œå¯æ¯”æ€§ã€‚
    """)
    
    # è¿è¡Œæ‰¹é‡æµ‹è¯•
    if "batch_test_results" not in st.session_state:
        if st.button("â–¶ï¸ è¿è¡Œæ‰¹é‡è¯„ä¼°", type="primary"):
            # åŠ è½½æµ‹è¯•é›†
            test_set = load_test_set(test_set_name)
            
            if not test_set or not test_set.get("cases"):
                st.error(f"æ— æ³•åŠ è½½æµ‹è¯•é›† '{test_set_name}' æˆ–æµ‹è¯•é›†ä¸ºç©º")
                return
            
            # --- Progress Bar Setup ---
            total_cases = len(test_set.get("cases", []))
            total_templates_to_test = len(optimized_templates)
            total_attempts = total_templates_to_test * total_cases * repeat_count
            completed_attempts = 0
            progress_bar = st.progress(0)
            status_text = st.empty()
            status_text.text(f"å‡†å¤‡å¼€å§‹... æ€»å…± {total_attempts} æ¬¡æ¨¡å‹è°ƒç”¨")

            def update_progress(template_index):
                nonlocal completed_attempts
                completed_attempts += 1
                progress = completed_attempts / total_attempts if total_attempts > 0 else 0
                progress = min(progress, 1.0)
                progress_bar.progress(progress)
                status_text.text(f"è¿è¡Œä¸­ (ç‰ˆæœ¬ {template_index+1}/{total_templates_to_test})... å·²å®Œæˆ {completed_attempts}/{total_attempts} æ¬¡æ¨¡å‹è°ƒç”¨")
            # --- End Progress Bar Setup ---

            with st.spinner("æ‰¹é‡è¯„ä¼°è¿è¡Œä¸­..."): # Keep spinner for overall process
                opt_results_list = []
                all_tests_successful = True
                for i, opt_template in enumerate(optimized_templates):
                    status_text.text(f"å¼€å§‹æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬ {i+1}/{total_templates_to_test}...")
                    
                    # Define a specific callback for this template index
                    def specific_update_progress():
                        update_progress(i)

                    opt_results = run_test(
                        template=opt_template, 
                        model=model, 
                        test_set=test_set,
                        model_provider=model_provider,
                        repeat_count=repeat_count,
                        temperature=temperature,
                        progress_callback=specific_update_progress # Pass the specific callback
                    )
                    
                    if opt_results:
                        opt_results_list.append({
                            "template": opt_template,
                            "results": opt_results
                        })
                    else:
                        st.warning(f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1} æµ‹è¯•è¿è¡Œå¤±è´¥æˆ–æœªè¿”å›ç»“æœã€‚")
                        all_tests_successful = False # Mark if any test fails
                        # Append a placeholder or skip?
                        # For now, let's skip appending failed results to avoid errors later

                # Final progress update and status
                progress_bar.progress(1.0)
                if all_tests_successful:
                    status_text.text(f"âœ… æ‰¹é‡è¯„ä¼°å®Œæˆ! å…±æ‰§è¡Œ {completed_attempts}/{total_attempts} æ¬¡æ¨¡å‹è°ƒç”¨ã€‚")
                else:
                     status_text.warning(f"âš ï¸ æ‰¹é‡è¯„ä¼°éƒ¨åˆ†å®Œæˆã€‚å…±æ‰§è¡Œ {completed_attempts}/{total_attempts} æ¬¡æ¨¡å‹è°ƒç”¨ã€‚è¯·æ£€æŸ¥è­¦å‘Šä¿¡æ¯ã€‚")

                # Store results only if at least some were successful
                if opt_results_list:
                    batch_results = {
                        "original": {"template": original_template, "results": original_results},
                        "optimized": opt_results_list
                    }
                    st.session_state.batch_test_results = batch_results
                    st.rerun()
                else:
                    st.error("æ‰€æœ‰ä¼˜åŒ–ç‰ˆæœ¬çš„æµ‹è¯•å‡æœªæˆåŠŸè·å–ç»“æœã€‚")
                    # Clear potentially empty state if needed
                    if "batch_test_results" in st.session_state:
                        del st.session_state.batch_test_results
    
    # å¦‚æœå·²æœ‰æµ‹è¯•ç»“æœï¼Œæ˜¾ç¤ºç»“æœ
    if "batch_test_results" in st.session_state:
        batch_results = st.session_state.batch_test_results
        display_batch_test_results(batch_results)
        
        # æ·»åŠ æ¸…é™¤ç»“æœæŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æµ‹è¯•ç»“æœ", key="clear_batch_results"):
            if "batch_test_results" in st.session_state:
                del st.session_state.batch_test_results
            st.rerun()

def display_batch_test_results(batch_results):
    """æ˜¾ç¤ºæ‰¹é‡æµ‹è¯•ç»“æœå¯¹æ¯”"""
    st.subheader("æ‰¹é‡è¯„ä¼°ç»“æœ")
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    original_results = batch_results["original"]["results"]
    model = original_results.get("model", "æœªçŸ¥æ¨¡å‹")
    model_provider = original_results.get("model_provider", "æœªçŸ¥æä¾›å•†")
    
    # è·å–æµ‹è¯•å‚æ•°
    test_params = original_results.get("test_params", {})
    repeat_count = test_params.get("repeat_count", 1)
    temperature = test_params.get("temperature", 0.7)
    
    st.info(f"""
    **æµ‹è¯•ä¿¡æ¯**:
    - æµ‹è¯•æ¨¡å‹: **{model}** (æä¾›å•†: **{model_provider}**)
    - æ¯ä¸ªæµ‹è¯•é‡å¤æ¬¡æ•°: **{repeat_count}**
    - æ¸©åº¦è®¾ç½®: **{temperature}**
    """)
    
    # è®¡ç®—åŸå§‹æç¤ºè¯å’Œæ‰€æœ‰ä¼˜åŒ–ç‰ˆæœ¬çš„å¹³å‡åˆ†æ•°
    original_avg = calculate_average_score(original_results)
    optimized_results_list = [item["results"] for item in batch_results["optimized"]]
    optimized_avgs = [calculate_average_score(res) for res in optimized_results_list]
    
    # æ‰¾å‡ºæœ€ä½³ç‰ˆæœ¬
    all_scores = [original_avg] + optimized_avgs
    if all_scores and max(all_scores) > 0:
        best_index = all_scores.index(max(all_scores))
        
        if best_index == 0:
            best_template = batch_results["original"]["template"]
            best_score = original_avg
            best_label = "åŸå§‹æç¤ºè¯"
        else:
            best_template = batch_results["optimized"][best_index-1]["template"]
            best_score = optimized_avgs[best_index-1]
            best_label = f"ä¼˜åŒ–ç‰ˆæœ¬ {best_index}"
        
        # æ˜¾ç¤ºæœ€ä½³ç‰ˆæœ¬
        st.success(f"### æœ€ä½³æç¤ºè¯: {best_label} (å¾—åˆ†: {best_score:.1f})")
    else:
        st.warning("æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä¼°åˆ†æ•°")
        best_template = None
        best_score = 0
        best_label = "æœªç¡®å®š"
        best_index = -1
    
    # åˆ›å»ºæ€»ä½“å¯¹æ¯”å›¾è¡¨
    st.subheader("æ€»ä½“æ€§èƒ½å¯¹æ¯”")
    
    # å‡†å¤‡æ•°æ®
    labels = ["åŸå§‹æç¤ºè¯"] + [f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}" for i in range(len(optimized_avgs))]
    scores = [original_avg] + optimized_avgs
    
    # åˆ›å»ºæ¡å½¢å›¾
    fig = create_score_bar_chart(scores, labels, "æç¤ºè¯ç‰ˆæœ¬å¹³å‡å¾—åˆ†å¯¹æ¯”")
    st.plotly_chart(fig, use_container_width=True)
    
    # æ˜¾ç¤ºå“åº”ç¨³å®šæ€§åˆ†æ
    st.subheader("å“åº”ç¨³å®šæ€§åˆ†æ")
    
    stability_data = []
    
    # åˆ†æåŸå§‹æç¤ºè¯çš„ç¨³å®šæ€§
    original_stability = analyze_response_stability(original_results)
    stability_data.append({"ç‰ˆæœ¬": "åŸå§‹æç¤ºè¯", **original_stability})
    
    # åˆ†æä¼˜åŒ–æç¤ºè¯çš„ç¨³å®šæ€§
    for i, result in enumerate(optimized_results_list):
        opt_stability = analyze_response_stability(result)
        stability_data.append({"ç‰ˆæœ¬": f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}", **opt_stability})
    
    # åˆ›å»ºç¨³å®šæ€§å¯¹æ¯”è¡¨æ ¼
    stability_df = pd.DataFrame(stability_data)
    st.dataframe(stability_df, use_container_width=True)
    
    # ç»´åº¦å¯¹æ¯”ä¸æ”¹è¿›è¡¨æ ¼
    compare_dimension_performance([original_results] + optimized_results_list, ["åŸå§‹æç¤ºè¯"] + [f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}" for i in range(len(optimized_results_list))])
    
    # æ·»åŠ è¯¦ç»†æ¯”è¾ƒåŠŸèƒ½
    st.subheader("è¯¦ç»†å¯¹æ¯”åˆ†æ")
    
    # é€‰æ‹©è¦æ¯”è¾ƒçš„ç‰ˆæœ¬
    compare_versions = st.multiselect(
        "é€‰æ‹©è¦è¯¦ç»†æ¯”è¾ƒçš„ç‰ˆæœ¬",
        options=labels,
        default=[labels[0], labels[best_index]] if best_index > 0 else [labels[0]]
    )
    
    if len(compare_versions) >= 2:
        # è·å–è¦æ¯”è¾ƒçš„ç»“æœ
        compare_results = []
        for version in compare_versions:
            if version == "åŸå§‹æç¤ºè¯":
                compare_results.append(original_results)
            else:
                # æå–ç‰ˆæœ¬å·
                version_index = int(version.split()[-1]) - 1
                if version_index < len(optimized_results_list):
                    compare_results.append(optimized_results_list[version_index])
        
        # æ˜¾ç¤ºç”¨ä¾‹çº§æ¯”è¾ƒ
        display_case_comparisons(compare_results, compare_versions)
    
    # æ˜¾ç¤ºæ¨èä½¿ç”¨çš„æç¤ºè¯
    display_recommendation(best_template, best_score, best_label, original_avg)

def display_case_comparisons(compare_results, compare_versions):
    """æ˜¾ç¤ºç”¨ä¾‹çº§åˆ«çš„è¯¦ç»†æ¯”è¾ƒ"""
    for case_index in range(len(compare_results[0].get("test_cases", []))):
        case_exists = True
        for result in compare_results:
            if case_index >= len(result.get("test_cases", [])):
                case_exists = False
                break
        
        if not case_exists:
            continue
            
        # è·å–æ‰€æœ‰ç‰ˆæœ¬çš„è¯¥ç”¨ä¾‹ç»“æœ
        case_data = []
        
        for i, result in enumerate(compare_results):
            case = result["test_cases"][case_index]
            version_name = compare_versions[i]
            
            # è·å–æ‰€æœ‰å“åº”çš„è¯„ä¼°ç»“æœ
            responses = case.get("responses", [])
            
            # è®¡ç®—å¹³å‡è¯„åˆ†
            avg_score = 0
            score_count = 0
            
            for resp_data in responses:
                # ä½¿ç”¨ä¿å­˜çš„è¯„ä¼°ç»“æœ
                eval_result = resp_data.get("evaluation")
                if eval_result and "overall_score" in eval_result:
                    avg_score += eval_result["overall_score"]
                    score_count += 1
            
            if score_count > 0:
                avg_score /= score_count
            
            case_data.append({
                "version": version_name,
                "responses": responses,
                "avg_score": avg_score,
                "case": case
            })
        
        # æ‰¾å‡ºæœ€ä½³å“åº”
        best_case_index = 0
        best_case_score = 0
        for i, data in enumerate(case_data):
            if data["avg_score"] > best_case_score:
                best_case_score = data["avg_score"]
                best_case_index = i
        
        # æ˜¾ç¤ºç”¨ä¾‹è¯¦æƒ…
        with st.expander(f"ç”¨ä¾‹ {case_index+1}: {case_data[0]['case'].get('case_description', case_data[0]['case'].get('case_id', ''))}"):
            # ä¸ºæ¯ä¸ªæ¯”è¾ƒç‰ˆæœ¬åˆ›å»ºé€‰é¡¹å¡
            tabs = st.tabs([data["version"] for data in case_data])
            from ui.components import display_test_case_details
            for i, tab in enumerate(tabs):
                with tab:
                    data = case_data[i]
                    st.metric("å¹³å‡è¯„åˆ†", f"{data['avg_score']:.1f}")
                    if i == best_case_index and data["avg_score"] > 0:
                        st.success("âœ“ æ­¤ç‰ˆæœ¬åœ¨å½“å‰ç”¨ä¾‹ä¸­è¡¨ç°æœ€ä½³")
                    # ç”¨é€šç”¨ç»„ä»¶å±•ç¤ºç”¨ä¾‹è¯¦æƒ…ã€å“åº”ã€è¯„ä¼°
                    display_test_case_details(data["case"], show_system_prompt=True, inside_expander=True)

def display_recommendation(best_template, best_score, best_label, original_avg):
    """æ˜¾ç¤ºæ¨èä½¿ç”¨çš„æç¤ºè¯"""
    st.subheader("æ¨èä½¿ç”¨çš„æç¤ºè¯")
    
    if best_template and best_score > 0:
        improvement = ((best_score - original_avg) / original_avg * 100) if original_avg > 0 else 0
        improvement_text = f"æå‡äº† **{improvement:.1f}%**" if best_score > original_avg else f"ä¸‹é™äº† **{abs(improvement):.1f}%**"
        
        st.info(f"""
        æ ¹æ®è¯„ä¼°ç»“æœï¼Œæ¨èä½¿ç”¨ **{best_label}**ã€‚
        
        - å¹³å‡å¾—åˆ†: **{best_score:.1f}**
        - ç›¸æ¯”åŸå§‹æç¤ºè¯: {improvement_text}
        """)
        
        # ä¿å­˜æ¨èæç¤ºè¯
        if st.button("ä¿å­˜æ¨èæç¤ºè¯ä¸ºæ¨¡æ¿", type="primary"):
            new_name = save_optimized_template(best_template, {"prompt": best_template.get("template", ""), "strategy": best_label})
            st.success(f"å·²å°†æ¨èæç¤ºè¯ä¿å­˜ä¸ºæ–°æ¨¡æ¿: {new_name}")
    else:
        st.warning("æ— æ³•ç¡®å®šæ¨èæç¤ºè¯ï¼Œè¯·æ£€æŸ¥è¯„ä¼°ç»“æœ")
