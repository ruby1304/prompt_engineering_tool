# prompt_batch_ab_test.py

import streamlit as st
import json
import pandas as pd
import asyncio
from datetime import datetime
import time
import plotly.express as px
import numpy as np

from config import load_test_set, save_template
from utils.common import (
    calculate_average_score, 
    get_dimension_scores, 
    analyze_response_stability,
    create_dimension_radar_chart,
    create_score_bar_chart,
    run_test
)
from ui.components import (
    display_test_summary,
    display_response_tabs,
    display_evaluation_results,
    display_test_case_details
)

def render_prompt_batch_ab_test():
    st.title("ğŸ§ª æç¤ºè¯æ‰¹é‡è¯„ä¼°")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹é‡A/Bæµ‹è¯•æ•°æ®
    if (not hasattr(st.session_state, 'batch_ab_test_original') or 
        not hasattr(st.session_state, 'batch_ab_test_optimized')):
        st.warning("è¯·å…ˆä»æç¤ºè¯ä¸“é¡¹ä¼˜åŒ–é¡µé¢å¯åŠ¨æ‰¹é‡è¯„ä¼°")
        
        if st.button("è¿”å›æç¤ºè¯ä¸“é¡¹ä¼˜åŒ–"):
            st.session_state.page = "prompt_optimization"
            st.experimental_rerun()
        return
    
    # è·å–æ‰¹é‡A/Bæµ‹è¯•æ•°æ®
    original_template = st.session_state.batch_ab_test_original
    optimized_templates = st.session_state.batch_ab_test_optimized
    model = st.session_state.batch_ab_test_model
    model_provider = st.session_state.get("batch_ab_test_model_provider")
    test_set_name = st.session_state.batch_ab_test_test_set
    
    st.markdown(f"""
    ### æ‰¹é‡è¯„ä¼°: åŸå§‹æç¤ºè¯ vs {len(optimized_templates)}ä¸ªä¼˜åŒ–ç‰ˆæœ¬
    
    - **æ¨¡å‹**: {model} ({model_provider if model_provider else "æœªæŒ‡å®šæä¾›å•†"})
    - **æµ‹è¯•é›†**: {test_set_name}
    - **ä¼˜åŒ–ç‰ˆæœ¬æ•°**: {len(optimized_templates)}
    """)
    
    # æ˜¾ç¤ºæç¤ºè¯æ¦‚è§ˆ
    with st.expander("æŸ¥çœ‹æ‰€æœ‰æç¤ºè¯"):
        st.subheader("åŸå§‹æç¤ºè¯")
        from utils.common import display_template_info
        display_template_info(original_template)
        
        for i, opt_template in enumerate(optimized_templates):
            st.subheader(f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}")
            display_template_info(opt_template)
    
    # æµ‹è¯•å‚æ•°è®¾ç½®
    st.subheader("æµ‹è¯•å‚æ•°")
    
    col1, col2 = st.columns(2)
    
    with col1:
        repeat_count = st.slider(
            "æ¯ä¸ªæµ‹è¯•é‡å¤æ¬¡æ•°", 
            min_value=1, 
            max_value=5, 
            value=2, 
            help="å¢åŠ é‡å¤æ¬¡æ•°å¯æé«˜ç»“æœç¨³å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜æ¸©åº¦è®¾ç½®ä¸‹"
        )
    
    with col2:
        temperature = st.slider(
            "Temperature", 
            min_value=0.0, 
            max_value=2.0, 
            value=0.7, 
            step=0.1,
            help="æ§åˆ¶æ¨¡å‹è¾“å‡ºçš„éšæœºæ€§ã€‚è¾ƒé«˜çš„å€¼ä¼šäº§ç”Ÿæ›´å¤šæ ·åŒ–ä½†å¯èƒ½ä¸ä¸€è‡´çš„è¾“å‡º"
        )
    
    # è¿è¡Œæ‰¹é‡æµ‹è¯•
    if "batch_test_results" not in st.session_state:
        if st.button("â–¶ï¸ è¿è¡Œæ‰¹é‡è¯„ä¼°", type="primary"):
            # åŠ è½½æµ‹è¯•é›†
            test_set = load_test_set(test_set_name)
            
            if not test_set or not test_set.get("cases"):
                st.error(f"æ— æ³•åŠ è½½æµ‹è¯•é›† '{test_set_name}' æˆ–æµ‹è¯•é›†ä¸ºç©º")
                return
            
            with st.spinner("æ‰¹é‡è¯„ä¼°è¿è¡Œä¸­..."):
                # åˆ›å»ºè¿›åº¦æ¡
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # è®¡ç®—æ€»ä»»åŠ¡æ•°
                total_templates = 1 + len(optimized_templates)
                total_tasks = total_templates
                completed_tasks = 0
                
                # å‡†å¤‡ç»“æœå­˜å‚¨
                batch_results = {
                    "original": {"template": original_template, "results": None},
                    "optimized": []
                }
                
                # æµ‹è¯•åŸå§‹æç¤ºè¯
                status_text.text("æµ‹è¯•åŸå§‹æç¤ºè¯...")
                original_results = run_test(
                    original_template, 
                    model, 
                    test_set, 
                    model_provider=model_provider,
                    repeat_count=repeat_count,
                    temperature=temperature
                )
                batch_results["original"]["results"] = original_results
                completed_tasks += 1
                progress_bar.progress(completed_tasks / total_tasks)
                
                # æµ‹è¯•æ‰€æœ‰ä¼˜åŒ–ç‰ˆæœ¬
                for i, opt_template in enumerate(optimized_templates):
                    status_text.text(f"æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬ {i+1}...")
                    opt_results = run_test(
                        opt_template, 
                        model, 
                        test_set,
                        model_provider=model_provider,
                        repeat_count=repeat_count,
                        temperature=temperature
                    )
                    batch_results["optimized"].append({
                        "template": opt_template,
                        "results": opt_results
                    })
                    completed_tasks += 1
                    progress_bar.progress(completed_tasks / total_tasks)
                
                # å®Œæˆè¿›åº¦æ¡
                progress_bar.progress(1.0)
                status_text.text("âœ… æ‰¹é‡è¯„ä¼°å®Œæˆ!")
                
                # ä¿å­˜ç»“æœ
                st.session_state.batch_test_results = batch_results
                
                # åˆ·æ–°é¡µé¢ä»¥æ˜¾ç¤ºç»“æœ
                st.experimental_rerun()
    
    # å¦‚æœå·²æœ‰æµ‹è¯•ç»“æœï¼Œæ˜¾ç¤ºç»“æœ
    if "batch_test_results" in st.session_state:
        batch_results = st.session_state.batch_test_results
        display_batch_test_results(batch_results)
        
        # æ·»åŠ æ¸…é™¤ç»“æœæŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æµ‹è¯•ç»“æœ", key="clear_batch_results"):
            if "batch_test_results" in st.session_state:
                del st.session_state.batch_test_results
            st.experimental_rerun()

def display_batch_test_results(batch_results):
    """æ˜¾ç¤ºæ‰¹é‡æµ‹è¯•ç»“æœå¯¹æ¯”"""
    st.subheader("æ‰¹é‡è¯„ä¼°ç»“æœ")
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    original_results = batch_results["original"]["results"]
    model = original_results.get("model", "æœªçŸ¥æ¨¡å‹")
    model_provider = original_results.get("model_provider", "æœªçŸ¥æä¾›å•†")
    
    # è·å–æµ‹è¯•å‚æ•°
    test_params = original_results.get("test_params", {})
    repeat_count = test_params.get("repeat_count", 1)
    temperature = test_params.get("temperature", 0.7)
    
    st.info(f"""
    **æµ‹è¯•ä¿¡æ¯**:
    - æµ‹è¯•æ¨¡å‹: **{model}** (æä¾›å•†: **{model_provider}**)
    - æ¯ä¸ªæµ‹è¯•é‡å¤æ¬¡æ•°: **{repeat_count}**
    - æ¸©åº¦è®¾ç½®: **{temperature}**
    """)
    
    # è®¡ç®—åŸå§‹æç¤ºè¯å’Œæ‰€æœ‰ä¼˜åŒ–ç‰ˆæœ¬çš„å¹³å‡åˆ†æ•°
    original_avg = calculate_average_score(original_results)
    optimized_results_list = [item["results"] for item in batch_results["optimized"]]
    optimized_avgs = [calculate_average_score(res) for res in optimized_results_list]
    
    # æ‰¾å‡ºæœ€ä½³ç‰ˆæœ¬
    all_scores = [original_avg] + optimized_avgs
    if all_scores and max(all_scores) > 0:
        best_index = all_scores.index(max(all_scores))
        
        if best_index == 0:
            best_template = batch_results["original"]["template"]
            best_score = original_avg
            best_label = "åŸå§‹æç¤ºè¯"
        else:
            best_template = batch_results["optimized"][best_index-1]["template"]
            best_score = optimized_avgs[best_index-1]
            best_label = f"ä¼˜åŒ–ç‰ˆæœ¬ {best_index}"
        
        # æ˜¾ç¤ºæœ€ä½³ç‰ˆæœ¬
        st.success(f"### æœ€ä½³æç¤ºè¯: {best_label} (å¾—åˆ†: {best_score:.1f})")
    else:
        st.warning("æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä¼°åˆ†æ•°")
        best_template = None
        best_score = 0
        best_label = "æœªç¡®å®š"
        best_index = -1
    
    # åˆ›å»ºæ€»ä½“å¯¹æ¯”å›¾è¡¨
    st.subheader("æ€»ä½“æ€§èƒ½å¯¹æ¯”")
    
    # å‡†å¤‡æ•°æ®
    labels = ["åŸå§‹æç¤ºè¯"] + [f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}" for i in range(len(optimized_avgs))]
    scores = [original_avg] + optimized_avgs
    
    # åˆ›å»ºæ¡å½¢å›¾
    fig = create_score_bar_chart(scores, labels, "æç¤ºè¯ç‰ˆæœ¬å¹³å‡å¾—åˆ†å¯¹æ¯”")
    st.plotly_chart(fig, use_container_width=True)
    
    # æ˜¾ç¤ºå“åº”ç¨³å®šæ€§åˆ†æ
    st.subheader("å“åº”ç¨³å®šæ€§åˆ†æ")
    
    stability_data = []
    
    # åˆ†æåŸå§‹æç¤ºè¯çš„ç¨³å®šæ€§
    original_stability = analyze_response_stability(original_results)
    stability_data.append({"ç‰ˆæœ¬": "åŸå§‹æç¤ºè¯", **original_stability})
    
    # åˆ†æä¼˜åŒ–æç¤ºè¯çš„ç¨³å®šæ€§
    for i, result in enumerate(optimized_results_list):
        opt_stability = analyze_response_stability(result)
        stability_data.append({"ç‰ˆæœ¬": f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}", **opt_stability})
    
    # åˆ›å»ºç¨³å®šæ€§å¯¹æ¯”è¡¨æ ¼
    stability_df = pd.DataFrame(stability_data)
    st.dataframe(stability_df, use_container_width=True)
    
    # åˆ›å»ºç»´åº¦é›·è¾¾å›¾å¯¹æ¯”
    st.subheader("ç»´åº¦è¡¨ç°å¯¹æ¯”")
    
    # è·å–åŸå§‹æç¤ºè¯çš„ç»´åº¦åˆ†æ•°
    original_dimensions = get_dimension_scores(original_results)
    
    # è·å–æ‰€æœ‰ä¼˜åŒ–ç‰ˆæœ¬çš„ç»´åº¦åˆ†æ•°
    optimized_dimensions = []
    for result in optimized_results_list:
        opt_dims = get_dimension_scores(result)
        optimized_dimensions.append(opt_dims)
    
    # å‡†å¤‡æ•°æ®
    dimension_scores_list = [original_dimensions] + optimized_dimensions
    labels = ["åŸå§‹æç¤ºè¯"] + [f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}" for i in range(len(optimized_dimensions))]
    
    # åˆ›å»ºé›·è¾¾å›¾
    fig = create_dimension_radar_chart(dimension_scores_list, labels, "æç¤ºè¯ç‰ˆæœ¬ç»´åº¦è¡¨ç°å¯¹æ¯”")
    st.plotly_chart(fig, use_container_width=True)
    
    # æ˜¾ç¤ºå„ç»´åº¦æå‡æƒ…å†µ
    st.subheader("å„ç»´åº¦æ”¹è¿›æƒ…å†µ")
    
    # åˆ›å»ºè¡¨æ ¼æ•°æ®
    improvement_data = []
    dimensions = list(original_dimensions.keys())
    
    for i, opt_dims in enumerate(optimized_dimensions):
        improvements = {}
        
        for dim in dimensions:
            if original_dimensions[dim] > 0:
                improvement = (opt_dims[dim] - original_dimensions[dim]) / original_dimensions[dim] * 100
            else:
                improvement = 0
                
            improvements[dim] = improvement
        
        # è®¡ç®—æ€»ä½“æ”¹è¿›
        avg_improvement = sum(improvements.values()) / len(improvements) if improvements else 0
        
        row = {
            "ç‰ˆæœ¬": f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}",
            "æ€»ä½“æ”¹è¿›": f"{avg_improvement:.1f}%"
        }
        
        for dim in dimensions:
            row[dim] = f"{improvements[dim]:.1f}%"
        
        improvement_data.append(row)
    
    # æ˜¾ç¤ºè¡¨æ ¼
    if improvement_data:
        st.dataframe(pd.DataFrame(improvement_data), use_container_width=True)
    else:
        st.info("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥è®¡ç®—æ”¹è¿›æƒ…å†µ")
    
    # æ·»åŠ è¯¦ç»†æ¯”è¾ƒåŠŸèƒ½
    st.subheader("è¯¦ç»†å¯¹æ¯”åˆ†æ")
    
    # é€‰æ‹©è¦æ¯”è¾ƒçš„ç‰ˆæœ¬
    compare_versions = st.multiselect(
        "é€‰æ‹©è¦è¯¦ç»†æ¯”è¾ƒçš„ç‰ˆæœ¬",
        options=labels,
        default=[labels[0], labels[best_index]] if best_index > 0 else [labels[0]]
    )
    
    if len(compare_versions) >= 2:
        # è·å–è¦æ¯”è¾ƒçš„ç»“æœ
        compare_results = []
        for version in compare_versions:
            if version == "åŸå§‹æç¤ºè¯":
                compare_results.append(original_results)
            else:
                # æå–ç‰ˆæœ¬å·
                version_index = int(version.split()[-1]) - 1
                if version_index < len(optimized_results_list):
                    compare_results.append(optimized_results_list[version_index])
        
        # æ˜¾ç¤ºç”¨ä¾‹çº§æ¯”è¾ƒ
        display_case_comparisons(compare_results, compare_versions)
    
    # æ˜¾ç¤ºæ¨èä½¿ç”¨çš„æç¤ºè¯
    display_recommendation(best_template, best_score, best_label, original_avg)

def display_case_comparisons(compare_results, compare_versions):
    """æ˜¾ç¤ºç”¨ä¾‹çº§åˆ«çš„è¯¦ç»†æ¯”è¾ƒ"""
    for case_index in range(len(compare_results[0].get("test_cases", []))):
        case_exists = True
        for result in compare_results:
            if case_index >= len(result.get("test_cases", [])):
                case_exists = False
                break
        
        if not case_exists:
            continue
            
        # è·å–æ‰€æœ‰ç‰ˆæœ¬çš„è¯¥ç”¨ä¾‹ç»“æœ
        case_data = []
        
        for i, result in enumerate(compare_results):
            case = result["test_cases"][case_index]
            version_name = compare_versions[i]
            
            # è·å–æ‰€æœ‰å“åº”çš„è¯„ä¼°ç»“æœ
            responses = case.get("responses", [])
            
            # è®¡ç®—å¹³å‡è¯„åˆ†
            avg_score = 0
            score_count = 0
            
            for resp_data in responses:
                # ä½¿ç”¨ä¿å­˜çš„è¯„ä¼°ç»“æœ
                eval_result = resp_data.get("evaluation")
                if eval_result and "overall_score" in eval_result:
                    avg_score += eval_result["overall_score"]
                    score_count += 1
            
            if score_count > 0:
                avg_score /= score_count
            
            case_data.append({
                "version": version_name,
                "responses": responses,
                "avg_score": avg_score,
                "case": case
            })
        
        # æ‰¾å‡ºæœ€ä½³å“åº”
        best_case_index = 0
        best_case_score = 0
        for i, data in enumerate(case_data):
            if data["avg_score"] > best_case_score:
                best_case_score = data["avg_score"]
                best_case_index = i
        
        # æ˜¾ç¤ºç”¨ä¾‹è¯¦æƒ…
        with st.expander(f"ç”¨ä¾‹ {case_index+1}: {case_data[0]['case'].get('case_description', case_data[0]['case'].get('case_id', ''))}"):
            # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥å’ŒæœŸæœ›è¾“å‡º
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ç”¨æˆ·è¾“å…¥:**")
                st.code(case_data[0]['case'].get("user_input", ""))
            
            with col2:
                st.markdown("**æœŸæœ›è¾“å‡º:**")
                st.code(case_data[0]['case'].get("expected_output", ""))
            
            # ä¸ºæ¯ä¸ªæ¯”è¾ƒç‰ˆæœ¬åˆ›å»ºé€‰é¡¹å¡
            tabs = st.tabs([data["version"] for data in case_data])
            
            # åœ¨æ¯ä¸ªé€‰é¡¹å¡ä¸­æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            for i, tab in enumerate(tabs):
                with tab:
                    data = case_data[i]
                    
                    # æ˜¾ç¤ºå¹³å‡åˆ†æ•°
                    st.metric("å¹³å‡è¯„åˆ†", f"{data['avg_score']:.1f}")
                    
                    if i == best_case_index and data["avg_score"] > 0:
                        st.success("âœ“ æ­¤ç‰ˆæœ¬åœ¨å½“å‰ç”¨ä¾‹ä¸­è¡¨ç°æœ€ä½³")
                    
                    # æ˜¾ç¤ºç³»ç»Ÿæç¤º
                    with st.expander("æŸ¥çœ‹ç³»ç»Ÿæç¤º"):
                        st.code(data["case"].get("prompt", ""))
                    
                    # æ˜¾ç¤ºæ‰€æœ‰å“åº”
                    st.subheader("å“åº”")
                    display_response_tabs(data["responses"])

def display_recommendation(best_template, best_score, best_label, original_avg):
    """æ˜¾ç¤ºæ¨èä½¿ç”¨çš„æç¤ºè¯"""
    st.subheader("æ¨èä½¿ç”¨çš„æç¤ºè¯")
    
    if best_template and best_score > 0:
        improvement = ((best_score - original_avg) / original_avg * 100) if original_avg > 0 else 0
        improvement_text = f"æå‡äº† **{improvement:.1f}%**" if best_score > original_avg else f"ä¸‹é™äº† **{abs(improvement):.1f}%**"
        
        st.info(f"""
        æ ¹æ®è¯„ä¼°ç»“æœï¼Œæ¨èä½¿ç”¨ **{best_label}**ã€‚
        
        - å¹³å‡å¾—åˆ†: **{best_score:.1f}**
        - ç›¸æ¯”åŸå§‹æç¤ºè¯: {improvement_text}
        """)
        
        # ä¿å­˜æ¨èæç¤ºè¯
        if st.button("ä¿å­˜æ¨èæç¤ºè¯ä¸ºæ¨¡æ¿", type="primary"):
            if best_template:
                new_template = dict(best_template)
                current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                if best_label == "åŸå§‹æç¤ºè¯":
                    new_template["name"] = f"{best_template.get('name', 'template')}_{current_time}_recommended"
                    new_template["description"] = f"æ¨èä½¿ç”¨çš„åŸå§‹æç¤ºè¯ (å¾—åˆ†: {best_score:.1f})"
                else:
                    new_template["name"] = f"{best_template.get('name', 'template')}_{current_time}_recommended"
                    new_template["description"] = f"æ¨èä½¿ç”¨çš„ä¼˜åŒ–æç¤ºè¯ (å¾—åˆ†: {best_score:.1f})"
                
                save_template(new_template["name"], new_template)
                st.success(f"å·²å°†æ¨èæç¤ºè¯ä¿å­˜ä¸ºæ–°æ¨¡æ¿: {new_template['name']}")
    else:
        st.warning("æ— æ³•ç¡®å®šæ¨èæç¤ºè¯ï¼Œè¯·æ£€æŸ¥è¯„ä¼°ç»“æœ")
