import streamlit as st
from config import get_available_models
from models.api_clients import get_provider_from_model
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import matplotlib.pyplot as plt

from utils.common import (
    calculate_average_score, 
    get_dimension_scores, 
    create_dimension_radar_chart
)

def select_single_model(key_prefix="model", help_text=None):
    """å•æ¨¡å‹é€‰æ‹©å™¨ç»„ä»¶
    
    è¿”å›: (model, provider)
    """
    # åŠ¨æ€è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
    available_models = get_available_models()
    all_models = []
    
    # åˆ›å»ºç»Ÿä¸€çš„æ¨¡å‹åˆ—è¡¨ï¼ŒåŒ…å«æä¾›å•†ä¿¡æ¯
    for provider, models in available_models.items():
        for model in models:
            all_models.append((provider, model))
    
    # åˆ›å»ºæ ¼å¼åŒ–çš„é€‰é¡¹åˆ—è¡¨ï¼Œæ˜¾ç¤ºæä¾›å•†ä¿¡æ¯
    model_options = [f"{model} ({provider})" for provider, model in all_models]
    model_map = {f"{model} ({provider})": (model, provider) for provider, model in all_models}
    
    selected_model_option = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        model_options,
        key=f"{key_prefix}_selector",
        help=help_text
    )
    
    if selected_model_option:
        return model_map[selected_model_option]
    else:
        return None, None

def select_multiple_models(key_prefix="models", label="é€‰æ‹©æ¨¡å‹"):
    """å¤šæ¨¡å‹é€‰æ‹©å™¨ç»„ä»¶
    
    è¿”å›: List[(model, provider)]
    """
    # åŠ¨æ€è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
    available_models = get_available_models()
    selected_models = []
    
    # æ˜¾ç¤ºæ ‡ç­¾
    st.markdown(f"**{label}:**")
    
    # æŒ‰æä¾›å•†åˆ†ç»„æ˜¾ç¤ºæ¨¡å‹
    for provider, provider_models in available_models.items():
        # æ˜¾ç¤ºæä¾›å•†åç§°
        st.markdown(f"**{provider.capitalize()}:**")
        
        # åˆ›å»ºåˆ—æ¥æ˜¾ç¤ºæ¨¡å‹é€‰æ‹©æ¡†
        cols = st.columns(2)
        col_idx = 0
        
        for model in provider_models:
            with cols[col_idx]:
                if st.checkbox(model, key=f"{key_prefix}_{provider}_{model}"):
                    selected_models.append((model, provider))
            
            # åˆ‡æ¢åˆ—
            col_idx = (col_idx + 1) % 2
        
        # æ·»åŠ åˆ†éš”çº¿
        st.divider()
    
    return selected_models


def display_test_summary(results, template, model):
    """æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦"""
    st.subheader("æµ‹è¯•ç»“æœæ‘˜è¦")
    
    # è®¡ç®—å¹³å‡åˆ†æ•°
    avg_score = calculate_average_score(results)
    
    # è·å–ç»´åº¦è¯„åˆ†
    dimension_scores = get_dimension_scores(results)
    
    # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
    col1, col2 = st.columns(2)
    
    with col1:
        if avg_score > 0:
            st.metric("å¹³å‡åˆ†æ•°", f"{avg_score:.1f}")
            st.write("ç»´åº¦è¯„åˆ†:")
            for dim, score in dimension_scores.items():
                st.metric(dim, f"{score:.1f}", label_visibility="visible")
        else:
            st.warning("æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä¼°åˆ†æ•°")
    
    with col2:
        if dimension_scores:
            # åˆ›å»ºé›·è¾¾å›¾
            fig = create_dimension_radar_chart(
                [dimension_scores], 
                [template.get("name", "å½“å‰æç¤ºè¯")],
                "æç¤ºè¯è¡¨ç°é›·è¾¾å›¾"
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("æ²¡æœ‰è¶³å¤Ÿçš„ç»´åº¦è¯„åˆ†æ¥ç”Ÿæˆé›·è¾¾å›¾")
    
    return avg_score, dimension_scores

def display_response_tabs(responses):
    """ä½¿ç”¨é€‰é¡¹å¡æ˜¾ç¤ºå¤šä¸ªå“åº”"""
    if not responses:
        st.info("æ²¡æœ‰å“åº”æ•°æ®")
        return
        
    for i, resp in enumerate(responses):
        st.markdown(f"**å“åº” #{i+1}:**")
        if resp.get("error"):
            st.error(resp.get("error"))
        else:
            st.code(resp.get("response", "æ— å“åº”"))
            
        # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
        eval_result = resp.get("evaluation")
        if eval_result:
            display_evaluation_results(eval_result)

def display_evaluation_results(eval_result):
    """æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
    if not eval_result:
        return
        
    st.markdown("**è¯„ä¼°ç»“æœ:**")
    
    if "error" in eval_result:
        st.error(f"è¯„ä¼°é”™è¯¯: {eval_result['error']}")
        return
        
    # æ˜¾ç¤ºæœ¬åœ°è¯„ä¼°æ ‡è®°
    if eval_result.get("is_local_evaluation", False):
        st.warning("âš ï¸ æœ¬åœ°è¯„ä¼°ç»“æœï¼ŒéAIè¯„ä¼°æ¨¡å‹ç”Ÿæˆ")
    
    # æ˜¾ç¤ºåˆ†æ•°
    if "scores" in eval_result:
        score_cols = st.columns(len(eval_result["scores"]))
        for i, (dim, score) in enumerate(eval_result["scores"].items()):
            with score_cols[i]:
                st.metric(dim, f"{score:.1f}")
    
    # æ˜¾ç¤ºæ€»åˆ†
    if "overall_score" in eval_result:
        st.metric("æ€»åˆ†", f"{eval_result['overall_score']:.1f}")
    
    # æ˜¾ç¤ºåˆ†æ
    if "analysis" in eval_result:
        st.markdown("**åˆ†æ:**")
        st.write(eval_result["analysis"])
    
    # æ˜¾ç¤ºTokenä¿¡æ¯
    if "prompt_info" in eval_result:
        st.info(f"æç¤ºè¯Tokenæ•°: {eval_result['prompt_info'].get('token_count', 'æœªçŸ¥')}")

def display_test_case_details(case, show_system_prompt=True, inside_expander=False):
    """æ˜¾ç¤ºæµ‹è¯•ç”¨ä¾‹è¯¦æƒ…"""
    if not case:
        st.info("æ²¡æœ‰æµ‹è¯•ç”¨ä¾‹æ•°æ®")
        return
        
    # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥å’ŒæœŸæœ›è¾“å‡º
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ç”¨æˆ·è¾“å…¥:**")
        st.code(case.get("user_input", ""))
    
    with col2:
        st.markdown("**æœŸæœ›è¾“å‡º:**")
        st.code(case.get("expected_output", ""))
    
    # æ˜¾ç¤ºç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
    if show_system_prompt:
        if inside_expander:
            # å¦‚æœå·²ç»åœ¨expanderå†…éƒ¨ï¼Œå°±ä¸ä½¿ç”¨åµŒå¥—expander
            st.markdown("**ç³»ç»Ÿæç¤º:**")
            st.code(case.get("prompt", ""))
        else:
            # æ­£å¸¸ä½¿ç”¨expander
            with st.expander("æŸ¥çœ‹ç³»ç»Ÿæç¤º"):
                st.code(case.get("prompt", ""))
    
    # æ˜¾ç¤ºå“åº”å’Œè¯„ä¼°ç»“æœ
    if "responses" in case and case["responses"]:
        st.markdown("**æ¨¡å‹å“åº”:**")
        for resp in case["responses"]:
            # å¦‚æœåœ¨expanderå†…éƒ¨ï¼Œåˆ™ä¸ä½¿ç”¨åµŒå¥—expander
            if inside_expander:
                st.markdown(f"**å“åº” (æ¨¡å‹: {resp.get('model', 'æœªçŸ¥')}, å°è¯•: #{resp.get('attempt', 0)}):**")
                if resp.get("error"):
                    st.error(resp.get("error"))
                else:
                    st.code(resp.get("response", ""))
                    if resp.get("usage"):
                        st.info(f"Tokenä½¿ç”¨: {resp.get('usage', {}).get('total_tokens', 'æœªçŸ¥')}")
                
                # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
                if resp.get("evaluation"):
                    display_evaluation_results(resp.get("evaluation"))
            else:
                with st.expander(f"å“åº” (æ¨¡å‹: {resp.get('model', 'æœªçŸ¥')}, å°è¯•: #{resp.get('attempt', 0)})"):
                    if resp.get("error"):
                        st.error(resp.get("error"))
                    else:
                        st.code(resp.get("response", ""))
                        if resp.get("usage"):
                            st.info(f"Tokenä½¿ç”¨: {resp.get('usage', {}).get('total_tokens', 'æœªçŸ¥')}")
                    
                    # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
                    if resp.get("evaluation"):
                        display_evaluation_results(resp.get("evaluation"))
    
    # å…¼å®¹æ—§æ ¼å¼ - å¦‚æœä½¿ç”¨model_responses
    elif "model_responses" in case:
        st.markdown("**æ¨¡å‹å“åº”:**")
        for resp in case["model_responses"]:
            # å¦‚æœåœ¨expanderå†…éƒ¨ï¼Œåˆ™ä¸ä½¿ç”¨åµŒå¥—expander
            if inside_expander:
                st.markdown(f"**å“åº” (æ¨¡å‹: {resp.get('model', 'æœªçŸ¥')}, å°è¯•: #{resp.get('attempt', 0)}):**")
                if resp.get("error"):
                    st.error(resp.get("error"))
                else:
                    st.code(resp.get("response", ""))
                    if resp.get("usage"):
                        st.info(f"Tokenä½¿ç”¨: {resp.get('usage', {}).get('total_tokens', 'æœªçŸ¥')}")
            else:
                with st.expander(f"å“åº” (æ¨¡å‹: {resp.get('model', 'æœªçŸ¥')}, å°è¯•: #{resp.get('attempt', 0)})"):
                    if resp.get("error"):
                        st.error(resp.get("error"))
                    else:
                        st.code(resp.get("response", ""))
                        if resp.get("usage"):
                            st.info(f"Tokenä½¿ç”¨: {resp.get('usage', {}).get('total_tokens', 'æœªçŸ¥')}")
    
    # æ˜¾ç¤ºè¯„ä¼°ç»“æœï¼ˆå¦‚æœä½¿ç”¨æ—§æ ¼å¼ï¼‰
    if "evaluation" in case:
        display_evaluation_results(case["evaluation"])

def show_evaluation_detail(evaluation: dict, turn_number: int):
    """æ˜¾ç¤ºå¯¹è¯è½®æ¬¡çš„è¯¦ç»†è¯„ä¼°ç»“æœ"""
    st.subheader(f"ç¬¬ {turn_number} è½®å¯¹è¯è¯„ä¼°ç»“æœ")
    
    # æ·»åŠ ä¸€ä¸ªå…³é—­æŒ‰é’®
    if st.button("æ”¶èµ·è¯„ä¼°è¯¦æƒ…", key=f"close_eval_{turn_number}", use_container_width=True):
        return False
        
    # å¦‚æœæœ‰é”™è¯¯ä¿¡æ¯ï¼Œæ˜¾ç¤ºé”™è¯¯
    if "error" in evaluation:
        st.warning(f"è¯„ä¼°è¿‡ç¨‹é‡åˆ°é—®é¢˜: {evaluation.get('error')}")
        return True
    
    # åˆ›å»ºé€‰é¡¹å¡å¸ƒå±€
    tab1, tab2, tab3 = st.tabs(["è¯„åˆ†è¯¦æƒ…", "é—®é¢˜è¯Šæ–­", "åˆ†ææŠ¥å‘Š"])
    
    # æ˜¾ç¤ºåˆ†æ•°
    with tab1:
        scores = evaluation["scores"]
        overall = evaluation["overall_score"]
        
        # ä»¥å½©è‰²æ–¹å—å’Œç™¾åˆ†æ¯”å½¢å¼æ˜¾ç¤ºåˆ†æ•°
        st.write("#### å„ç»´åº¦è¯„åˆ†")
        
        # ä¸ºæ¯ä¸ªåˆ†æ•°åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡æ ·å¼çš„æ˜¾ç¤º
        for dimension, score in scores.items():
            if dimension != "prompt_efficiency":  # æ’é™¤æç¤ºè¯æ•ˆç‡ï¼Œå› ä¸ºè¿™ä¸æ˜¯å¯¹è¯è´¨é‡çš„ç›´æ¥è¡¡é‡
                # ç¡®å®šé¢œè‰²
                color = "green" if score >= 80 else "orange" if score >= 60 else "red"
                
                # åˆ›å»ºå¯è§†åŒ–çš„åˆ†æ•°æ¡
                st.markdown(
                    f"**{dimension.capitalize()}**: "
                    f"<div style='background-color:#f0f2f6;border-radius:10px;height:25px;width:100%;margin-bottom:10px;'>"
                    f"<div style='background-color:{color};border-radius:10px;height:25px;width:{score}%;padding-left:10px;'>"
                    f"<span style='color:white;line-height:25px;'>{score}%</span>"
                    f"</div></div>",
                    unsafe_allow_html=True
                )
        
        # æ€»ä½“è¯„åˆ†
        st.write("#### æ€»ä½“è¯„åˆ†")
        overall_color = "green" if overall >= 80 else "orange" if overall >= 60 else "red"
        st.markdown(
            f"<div style='background-color:#f0f2f6;border-radius:10px;height:30px;width:100%;'>"
            f"<div style='background-color:{overall_color};border-radius:10px;height:30px;width:{overall}%;padding-left:10px;'>"
            f"<span style='color:white;line-height:30px;font-weight:bold;'>{overall}%</span>"
            f"</div></div>",
            unsafe_allow_html=True
        )
    
    # é—®é¢˜è¯Šæ–­
    with tab2:
        issues = evaluation.get("issues", [])
        
        if not issues:
            st.success("æœªæ£€æµ‹åˆ°æ˜æ˜¾é—®é¢˜ï¼Œæ­¤è½®å¯¹è¯è¡¨ç°è‰¯å¥½ï¼")
        else:
            st.write("#### æ£€æµ‹åˆ°çš„é—®é¢˜")
            
            # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤ºé—®é¢˜
            model_issues = [issue for issue in issues if issue["type"] == "model"]
            prompt_issues = [issue for issue in issues if issue["type"] == "prompt"]
            
            if model_issues:
                st.write("##### æ¨¡å‹é—®é¢˜")
                for issue in model_issues:
                    severity_color = "red" if issue["severity"] == "high" else "orange"
                    st.markdown(f"<div style='border-left:4px solid {severity_color};padding-left:10px;margin-bottom:10px;'>"
                               f"<p><strong>ä¸¥é‡ç¨‹åº¦:</strong> {issue['severity']}</p>"
                               f"<p><strong>é—®é¢˜:</strong> {issue['description']}</p>"
                               f"<p><strong>å»ºè®®:</strong> {issue['suggestion']}</p>"
                               f"</div>", unsafe_allow_html=True)
            
            if prompt_issues:
                st.write("##### æç¤ºè¯é—®é¢˜")
                for issue in prompt_issues:
                    severity_color = "red" if issue["severity"] == "high" else "orange"
                    st.markdown(f"<div style='border-left:4px solid {severity_color};padding-left:10px;margin-bottom:10px;'>"
                               f"<p><strong>ä¸¥é‡ç¨‹åº¦:</strong> {issue['severity']}</p>"
                               f"<p><strong>é—®é¢˜:</strong> {issue['description']}</p>"
                               f"<p><strong>å»ºè®®:</strong> {issue['suggestion']}</p>"
                               f"</div>", unsafe_allow_html=True)
    
    # åˆ†ææŠ¥å‘Š
    with tab3:
        if "summary" in evaluation:
            st.write("#### è¯„ä¼°æ€»ç»“")
            st.info(evaluation["summary"])
        
        if "analysis" in evaluation:
            st.write("#### è¯¦ç»†åˆ†æ")
            st.markdown(evaluation["analysis"])
            
        # Tokenä½¿ç”¨æƒ…å†µ
        if "prompt_info" in evaluation:
            st.write("#### æç¤ºè¯ä¿¡æ¯")
            st.write(f"æç¤ºè¯tokenæ•°é‡: {evaluation['prompt_info'].get('token_count', 'N/A')}")
            
    return True

def display_dialogue_analysis(dialogue_history, evaluation_results, prompt_ratings):
    """æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯çš„åˆ†æç»“æœ
    
    Args:
        dialogue_history: å¯¹è¯å†å²è®°å½•åˆ—è¡¨
        evaluation_results: è¯„ä¼°ç»“æœåˆ—è¡¨
        prompt_ratings: æç¤ºè¯è¯„åˆ†è®°å½•åˆ—è¡¨
    """
    st.subheader("ğŸ” å¯¹è¯åˆ†æ")
    
    # åˆ›å»ºé€‰é¡¹å¡å¸ƒå±€
    tab1, tab2, tab3 = st.tabs(["å¯¹è¯è´¨é‡è¶‹åŠ¿", "æç¤ºè¯æ•ˆæœåˆ†æ", "æ”¹è¿›å»ºè®®"])
    
    with tab1:
        st.write("#### å¯¹è¯è´¨é‡éšæ—¶é—´å˜åŒ–è¶‹åŠ¿")
        
        # æå–è¯„åˆ†æ•°æ®
        if prompt_ratings:
            # è½¬æ¢ä¸ºpandas DataFrameä»¥ä¾¿åˆ†æ
            df = pd.DataFrame([
                {
                    "turn": rating["turn"],
                    "overall": rating["overall"],
                    **rating["scores"]
                }
                for rating in prompt_ratings
            ])
            
            # ç»˜åˆ¶æ€»ä½“è¯„åˆ†è¶‹åŠ¿å›¾
            fig, ax = plt.subplots(figsize=(10, 4))
            ax.plot(df["turn"], df["overall"], marker='o', linewidth=2, label='æ€»ä½“è¯„åˆ†')
            ax.set_xlabel('å¯¹è¯å›åˆ')
            ax.set_ylabel('è¯„åˆ†')
            ax.set_title('å¯¹è¯è´¨é‡è¶‹åŠ¿')
            ax.grid(True, linestyle='--', alpha=0.7)
            ax.set_ylim(0, 100)
            st.pyplot(fig)
            
            # ç»˜åˆ¶å„ç»´åº¦è¯„åˆ†è¶‹åŠ¿
            dimensions = [col for col in df.columns if col not in ["turn", "overall", "prompt_efficiency"]]
            if dimensions:
                fig, ax = plt.subplots(figsize=(10, 6))
                for dim in dimensions:
                    ax.plot(df["turn"], df[dim], marker='o', linewidth=2, label=dim)
                ax.set_xlabel('å¯¹è¯å›åˆ')
                ax.set_ylabel('è¯„åˆ†')
                ax.set_title('å„ç»´åº¦è¯„åˆ†è¶‹åŠ¿')
                ax.grid(True, linestyle='--', alpha=0.7)
                ax.set_ylim(0, 100)
                ax.legend()
                st.pyplot(fig)
                
                # è®¡ç®—è¯„åˆ†çš„ç»Ÿè®¡æ•°æ®
                st.write("#### è¯„åˆ†ç»Ÿè®¡æ•°æ®")
                stats_df = df[dimensions + ["overall"]].describe().T[["mean", "std", "min", "max"]]
                stats_df = stats_df.round(2)
                
                # ä¸ºæ•°æ®æ·»åŠ é¢œè‰²æ ‡è®°
                def color_mean(val):
                    color = 'green' if val >= 80 else 'orange' if val >= 60 else 'red'
                    return f'color: {color}; font-weight: bold'
                
                # åº”ç”¨æ ·å¼å¹¶æ˜¾ç¤º
                st.dataframe(stats_df.style.applymap(color_mean, subset=['mean']))
        else:
            st.info("å°šæ— è¯„ä¼°æ•°æ®ï¼Œè¯·ç¡®ä¿å·²å¯ç”¨è‡ªåŠ¨è¯„ä¼°æˆ–æ‰‹åŠ¨è¯„ä¼°å¯¹è¯")
    
    with tab2:
        st.write("#### æç¤ºè¯æ•ˆæœåˆ†æ")
        
        # åˆ†æå„è½®å¯¹è¯ä¸­æç¤ºè¯éµå¾ªåº¦
        if prompt_ratings:
            # è®¡ç®—æç¤ºè¯éµå¾ªåº¦ç»Ÿè®¡
            prompt_following_scores = [rating["scores"].get("prompt_following", 0) for rating in prompt_ratings]
            avg_following = sum(prompt_following_scores) / len(prompt_following_scores) if prompt_following_scores else 0
            
            # æ˜¾ç¤ºæç¤ºè¯éµå¾ªåº¦è¯„åˆ† - é¿å…ä½¿ç”¨åµŒå¥—åˆ—å¸ƒå±€ï¼Œæ”¹ç”¨è¡Œå¸ƒå±€
            st.metric("å¹³å‡æç¤ºè¯éµå¾ªåº¦", f"{avg_following:.1f}/100")
            
            min_following = min(prompt_following_scores) if prompt_following_scores else 0
            st.metric("æœ€ä½æç¤ºè¯éµå¾ªåº¦", f"{min_following}/100")
            
            # æç¤ºè¯é—®é¢˜æ±‡æ€»
            prompt_issues = []
            for i, eval_result in enumerate(evaluation_results):
                if eval_result and "issues" in eval_result:
                    for issue in eval_result["issues"]:
                        if issue["type"] == "prompt":
                            prompt_issues.append({
                                "turn": i+1,
                                "severity": issue["severity"],
                                "description": issue["description"],
                                "suggestion": issue["suggestion"]
                            })
            
            if prompt_issues:
                st.write("#### æç¤ºè¯é—®é¢˜æ±‡æ€»")
                issue_df = pd.DataFrame(prompt_issues)
                st.dataframe(issue_df, use_container_width=True)
                
                # æŒ‰ä¸¥é‡ç¨‹åº¦è®¡æ•°
                severity_counts = issue_df["severity"].value_counts()
                
                # ç»˜åˆ¶é¥¼å›¾
                fig, ax = plt.subplots()
                ax.pie(severity_counts, labels=severity_counts.index, autopct='%1.1f%%',
                      colors=['red' if x == 'high' else 'orange' for x in severity_counts.index])
                ax.set_title('æç¤ºè¯é—®é¢˜ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ')
                st.pyplot(fig)
            else:
                st.success("æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æç¤ºè¯é—®é¢˜")
        else:
            st.info("å°šæ— è¯„ä¼°æ•°æ®ï¼Œè¯·ç¡®ä¿å·²å¯ç”¨è‡ªåŠ¨è¯„ä¼°æˆ–æ‰‹åŠ¨è¯„ä¼°å¯¹è¯")
    
    with tab3:
        st.write("#### æ”¹è¿›å»ºè®®")
        
        # æ±‡æ€»æ‰€æœ‰å»ºè®®
        all_suggestions = []
        model_suggestions = []
        prompt_suggestions = []
        
        for eval_result in evaluation_results:
            if eval_result and "issues" in eval_result:
                for issue in eval_result["issues"]:
                    if issue["type"] == "prompt" and issue["suggestion"] not in prompt_suggestions:
                        prompt_suggestions.append(issue["suggestion"])
                    elif issue["type"] == "model" and issue["suggestion"] not in model_suggestions:
                        model_suggestions.append(issue["suggestion"])
        
        # æç¤ºè¯æ”¹è¿›å»ºè®®
        st.write("##### æç¤ºè¯æ”¹è¿›å»ºè®®")
        if prompt_suggestions:
            for i, suggestion in enumerate(prompt_suggestions):
                st.markdown(f"{i+1}. {suggestion}")
        else:
            st.success("æç¤ºè¯è¡¨ç°è‰¯å¥½ï¼Œæ²¡æœ‰ç‰¹åˆ«éœ€è¦æ”¹è¿›çš„åœ°æ–¹")
        
        # æ¨¡å‹é€‰æ‹©å»ºè®®
        st.write("##### æ¨¡å‹ä½¿ç”¨å»ºè®®")
        if model_suggestions:
            for i, suggestion in enumerate(model_suggestions):
                st.markdown(f"{i+1}. {suggestion}")
        else:
            st.success("æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œæ²¡æœ‰ç‰¹åˆ«éœ€è¦è°ƒæ•´çš„åœ°æ–¹")
        
        return prompt_suggestions, model_suggestions
