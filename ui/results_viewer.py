import streamlit as st
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
from config import get_result_list, load_result
from utils.visualizer import (
    create_score_comparison_chart, 
    create_token_comparison_chart,
    create_radar_chart,
    generate_report,
    display_report
)
from utils.optimizer import PromptOptimizer

def render_results_viewer():
    st.title("ðŸ“ˆ æµ‹è¯•ç»“æžœæŸ¥çœ‹")
    
    # é€‰æ‹©è¦æŸ¥çœ‹çš„æµ‹è¯•ç»“æžœ
    result_list = get_result_list()
    
    if not result_list:
        st.warning("æœªæ‰¾åˆ°æµ‹è¯•ç»“æžœï¼Œè¯·å…ˆè¿è¡Œæµ‹è¯•")
        return
    
    # å¦‚æžœæœ‰ä¸Šæ¬¡æµ‹è¯•çš„ç»“æžœï¼Œé»˜è®¤é€‰æ‹©å®ƒ
    default_result = st.session_state.get("last_result", result_list[0]) if result_list else None
    
    selected_result = st.selectbox(
        "é€‰æ‹©æµ‹è¯•ç»“æžœ",
        result_list,
        index=result_list.index(default_result) if default_result in result_list else 0
    )
    
    if not selected_result:
        return
    
    # åŠ è½½é€‰æ‹©çš„ç»“æžœ
    results = load_result(selected_result)
    
    # å±•ç¤ºç»“æžœæ¦‚è§ˆ
    st.subheader("æµ‹è¯•æ¦‚è§ˆ")
    
    # æå–æ¦‚è§ˆä¿¡æ¯
    overview = {}
    for prompt_name, prompt_data in results.items():
        overview[prompt_name] = {
            "æµ‹è¯•é›†": prompt_data.get("test_set", ""),
            "æ¨¡åž‹": ", ".join(prompt_data.get("models", [])),
            "æµ‹è¯•ç”¨ä¾‹æ•°": len(prompt_data.get("test_cases", [])),
            "å¹³å‡åˆ†æ•°": calculate_average_score(prompt_data)
        }
    
    # æ˜¾ç¤ºæ¦‚è§ˆè¡¨æ ¼
    st.dataframe(pd.DataFrame.from_dict(overview, orient='index'))
    
    # å¯è§†åŒ–ç»“æžœ
    st.subheader("ç»“æžœå¯è§†åŒ–")
    
    tab1, tab2, tab3 = st.tabs(["è¯„åˆ†å¯¹æ¯”", "Tokenåˆ†æž", "å¤šç»´åº¦åˆ†æž"])
    
    with tab1:
        st.plotly_chart(create_score_comparison_chart(results), use_container_width=True)
    
    with tab2:
        st.plotly_chart(create_token_comparison_chart(results), use_container_width=True)
    
    with tab3:
        st.plotly_chart(create_radar_chart(results), use_container_width=True)
    
    # ç”Ÿæˆå¹¶æ˜¾ç¤ºæŠ¥å‘Š
    report = generate_report(results)
    display_report(report)
    
    # æ˜¾ç¤ºè¯¦ç»†æµ‹è¯•ç»“æžœ
    st.subheader("è¯¦ç»†æµ‹è¯•ç»“æžœ")
    from ui.components import display_test_case_details
    for prompt_name, prompt_data in results.items():
        with st.expander(f"æç¤ºè¯: {prompt_name}"):
            st.markdown(f"**æ¨¡æ¿æè¿°**: {prompt_data.get('template', {}).get('description', 'æ— æè¿°')}")
            st.markdown(f"**æµ‹è¯•é›†**: {prompt_data.get('test_set', 'æœªçŸ¥')}")
            st.markdown(f"**æµ‹è¯•æ¨¡åž‹**: {', '.join(prompt_data.get('models', []))}")
            # ç”¨é€šç”¨ç»„ä»¶å±•ç¤ºæ¯ä¸ªç”¨ä¾‹è¯¦æƒ…
            for i, case in enumerate(prompt_data.get("test_cases", [])):
                st.markdown(f"### æµ‹è¯•ç”¨ä¾‹ {i+1}: {case.get('case_description', case.get('case_id', ''))}")
                display_test_case_details(case, show_system_prompt=True, inside_expander=True)
    
    # æç¤ºè¯ä¼˜åŒ–åŠŸèƒ½
    st.divider()
    st.subheader("ðŸ“ æç¤ºè¯ä¼˜åŒ–")
    
    # æ‰¾å‡ºæœ€å¥½å’Œæœ€å·®çš„æç¤ºè¯
    avg_scores = {name: calculate_average_score(data) for name, data in results.items()}
    
    if avg_scores:
        best_prompt = max(avg_scores.items(), key=lambda x: x[1])
        worst_prompt = min(avg_scores.items(), key=lambda x: x[1])
        
        st.write(f"æœ€ä½³æç¤ºè¯: **{best_prompt[0]}** (å¹³å‡åˆ†: {best_prompt[1]:.1f})")
        st.write(f"æœ€å·®æç¤ºè¯: **{worst_prompt[0]}** (å¹³å‡åˆ†: {worst_prompt[1]:.1f})")
        
        # é€‰æ‹©è¦ä¼˜åŒ–çš„æç¤ºè¯
        prompt_to_optimize = st.selectbox(
            "é€‰æ‹©è¦ä¼˜åŒ–çš„æç¤ºè¯",
            list(results.keys()),
            index=list(results.keys()).index(worst_prompt[0]) if worst_prompt[0] in results else 0
        )
        
        optimization_strategy = st.selectbox(
            "ä¼˜åŒ–ç­–ç•¥",
            ["balanced", "accuracy", "completeness", "conciseness"],
            format_func=lambda x: {
                "balanced": "å¹³è¡¡ä¼˜åŒ– (å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œç®€æ´æ€§)",
                "accuracy": "ä¼˜åŒ–å‡†ç¡®æ€§",
                "completeness": "ä¼˜åŒ–å®Œæ•´æ€§",
                "conciseness": "ä¼˜åŒ–ç®€æ´æ€§"
            }.get(x, x)
        )
        
        if st.button("ç”Ÿæˆä¼˜åŒ–å»ºè®®", type="primary"):
            prompt_data = results[prompt_to_optimize]
            original_prompt = prompt_data.get("template", {}).get("template", "")
            
            # å‡†å¤‡è¯„ä¼°ç»“æžœåˆ—è¡¨
            evaluation_results = []
            for case in prompt_data.get("test_cases", []):
                if "evaluation" in case and not "error" in case["evaluation"]:
                    evaluation_results.append(case["evaluation"])
            
            with st.spinner("æ­£åœ¨ç”Ÿæˆä¼˜åŒ–å»ºè®®..."):
                # è°ƒç”¨ä¼˜åŒ–å™¨
                optimizer = PromptOptimizer()
                
                # å®žé™…åº”ç”¨ä¸­åº”ä½¿ç”¨é€‚å½“çš„å¼‚æ­¥å¤„ç†
                import asyncio
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                optimization_result = loop.run_until_complete(optimizer.optimize_prompt(
                    original_prompt,
                    evaluation_results,
                    optimization_strategy
                ))
                loop.close()
                
                if "error" in optimization_result:
                    st.error(f"ä¼˜åŒ–å¤±è´¥: {optimization_result['error']}")
                else:
                    # ä¿å­˜ä¼˜åŒ–ç»“æžœ
                    st.session_state.optimized_prompts = optimization_result.get("optimized_prompts", [])
                    
                    # æ˜¾ç¤ºä¼˜åŒ–ç»“æžœ
                    st.success("å·²ç”Ÿæˆä¼˜åŒ–å»ºè®®")
                    
                    for i, opt_prompt in enumerate(st.session_state.optimized_prompts):
                        with st.expander(f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}: {opt_prompt.get('strategy', 'æœªçŸ¥ç­–ç•¥')}"):
                            st.markdown("**ä¼˜åŒ–ç­–ç•¥**:")
                            st.write(opt_prompt.get("strategy", ""))
                            
                            st.markdown("**é¢„æœŸæ”¹è¿›**:")
                            st.write(opt_prompt.get("expected_improvements", ""))
                            
                            st.markdown("**ä¼˜åŒ–åŽçš„æç¤ºè¯**:")
                            st.code(opt_prompt.get("prompt", ""))
                            
                            # åˆ›å»ºæŒ‰é’®ï¼Œå°†ä¼˜åŒ–åŽçš„æç¤ºè¯ä¿å­˜ä¸ºæ–°æ¨¡æ¿
                            if st.button(f"ä¿å­˜ä¸ºæ–°æ¨¡æ¿", key=f"save_opt_{i}"):
                                from ..config import save_template
                                
                                # å¤åˆ¶åŽŸå§‹æ¨¡æ¿ï¼Œæ›¿æ¢æç¤ºè¯å†…å®¹
                                original_template = prompt_data.get("template", {})
                                new_template = dict(original_template)
                                
                                new_template["name"] = f"{original_template.get('name', 'template')}_{optimization_strategy}_v{i+1}"
                                new_template["description"] = f"ä»Ž '{original_template.get('name', 'unknown')}' ä¼˜åŒ–: {opt_prompt.get('strategy', '')}"
                                new_template["template"] = opt_prompt.get("prompt", "")
                                
                                save_template(new_template["name"], new_template)
                                st.success(f"å·²ä¿å­˜ä¸ºæ–°æ¨¡æ¿: {new_template['name']}")
    
    st.divider()
    st.subheader("ðŸ“ è¯„ä¼°æ—¥å¿—")

    # èŽ·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
    log_dir = Path("data/logs")
    if log_dir.exists():
        log_files = list(log_dir.glob("evaluator_log_*.txt"))
        if log_files:
            log_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # æ˜¾ç¤ºæœ€æ–°çš„å‡ ä¸ªæ—¥å¿—æ–‡ä»¶
            selected_log = st.selectbox(
                "é€‰æ‹©è¯„ä¼°æ—¥å¿—æ–‡ä»¶",
                options=log_files,
                format_func=lambda x: f"{x.name} ({datetime.fromtimestamp(x.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')})"
            )
            
            if selected_log:
                with open(selected_log, "r", encoding="utf-8") as f:
                    log_content = f.read()
                
                st.code(log_content)
                
                if st.button("åˆ é™¤é€‰ä¸­çš„æ—¥å¿—æ–‡ä»¶"):
                    try:
                        selected_log.unlink()
                        st.success("æ—¥å¿—æ–‡ä»¶å·²åˆ é™¤")
                        st.rerun()
                    except Exception as e:
                        st.error(f"åˆ é™¤æ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        else:
            st.info("æš‚æ— è¯„ä¼°æ—¥å¿—æ–‡ä»¶")
    else:
        st.info("æ—¥å¿—ç›®å½•ä¸å­˜åœ¨")

    # åˆ†äº«å’Œå¯¼å‡ºåŠŸèƒ½
    st.divider()
    st.subheader("ðŸ“¤ åˆ†äº«å’Œå¯¼å‡º")
    
    # å¯¼å‡ºä¸ºJSON
    if st.download_button(
        label="å¯¼å‡ºç»“æžœä¸ºJSON",
        data=json.dumps(results, ensure_ascii=False, indent=2),
        file_name=f"{selected_result}.json",
        mime="application/json"
    ):
        st.success("ç»“æžœå·²å¯¼å‡º")

def calculate_average_score(prompt_data):
    """è®¡ç®—æç¤ºè¯å¹³å‡åˆ†"""
    total_score = 0
    count = 0
    
    for case in prompt_data.get("test_cases", []):
        # æ£€æŸ¥evaluationæ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºNone
        if case.get("evaluation") is not None and "overall_score" in case["evaluation"]:
            total_score += case["evaluation"]["overall_score"]
            count += 1
    
    return total_score / count if count > 0 else 0