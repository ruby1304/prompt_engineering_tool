import streamlit as st
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
from config import get_result_list, load_result
from utils.visualizer import (
    create_score_comparison_chart, 
    create_token_comparison_chart,
    create_radar_chart,
    generate_report,
    display_report
)
from utils.optimizer import PromptOptimizer

def render_results_viewer():
    st.title("ðŸ“ˆ æµ‹è¯•ç»“æžœæŸ¥çœ‹")
    
    # é€‰æ‹©è¦æŸ¥çœ‹çš„æµ‹è¯•ç»“æžœ
    result_list = get_result_list()
    
    if not result_list:
        st.warning("æœªæ‰¾åˆ°æµ‹è¯•ç»“æžœï¼Œè¯·å…ˆè¿è¡Œæµ‹è¯•")
        return
    
    # å¦‚æžœæœ‰ä¸Šæ¬¡æµ‹è¯•çš„ç»“æžœï¼Œé»˜è®¤é€‰æ‹©å®ƒ
    default_result = st.session_state.get("last_result", result_list[0]) if result_list else None
    
    selected_result = st.selectbox(
        "é€‰æ‹©æµ‹è¯•ç»“æžœ",
        result_list,
        index=result_list.index(default_result) if default_result in result_list else 0
    )
    
    if not selected_result:
        return
    
    # åŠ è½½é€‰æ‹©çš„ç»“æžœ
    results = load_result(selected_result)
    
    # å±•ç¤ºç»“æžœæ¦‚è§ˆ
    st.subheader("æµ‹è¯•æ¦‚è§ˆ")
    
    # æå–æ¦‚è§ˆä¿¡æ¯
    overview = {}
    for prompt_name, prompt_data in results.items():
        overview[prompt_name] = {
            "æµ‹è¯•é›†": prompt_data.get("test_set", ""),
            "æ¨¡åž‹": ", ".join(prompt_data.get("models", [])),
            "æµ‹è¯•ç”¨ä¾‹æ•°": len(prompt_data.get("test_cases", [])),
            "å¹³å‡åˆ†æ•°": calculate_average_score(prompt_data)
        }
    
    # æ˜¾ç¤ºæ¦‚è§ˆè¡¨æ ¼
    st.dataframe(pd.DataFrame.from_dict(overview, orient='index'))
    
    # å¯è§†åŒ–ç»“æžœ
    st.subheader("ç»“æžœå¯è§†åŒ–")
    
    tab1, tab2, tab3 = st.tabs(["è¯„åˆ†å¯¹æ¯”", "Tokenåˆ†æž", "å¤šç»´åº¦åˆ†æž"])
    
    with tab1:
        st.plotly_chart(create_score_comparison_chart(results), use_container_width=True)
    
    with tab2:
        st.plotly_chart(create_token_comparison_chart(results), use_container_width=True)
    
    with tab3:
        st.plotly_chart(create_radar_chart(results), use_container_width=True)
    
    # ç”Ÿæˆå¹¶æ˜¾ç¤ºæŠ¥å‘Š
    report = generate_report(results)
    display_report(report)
    
    # æ˜¾ç¤ºè¯¦ç»†æµ‹è¯•ç»“æžœ
    st.subheader("è¯¦ç»†æµ‹è¯•ç»“æžœ")
    from ui.components import display_test_case_details
    for prompt_name, prompt_data in results.items():
        with st.expander(f"æç¤ºè¯: {prompt_name}"):
            st.markdown(f"**æ¨¡æ¿æè¿°**: {prompt_data.get('template', {}).get('description', 'æ— æè¿°')}")
            st.markdown(f"**æµ‹è¯•é›†**: {prompt_data.get('test_set', 'æœªçŸ¥')}")
            st.markdown(f"**æµ‹è¯•æ¨¡åž‹**: {', '.join(prompt_data.get('models', []))}")
            # ç”¨é€šç”¨ç»„ä»¶å±•ç¤ºæ¯ä¸ªç”¨ä¾‹è¯¦æƒ…
            for i, case in enumerate(prompt_data.get("test_cases", [])):
                st.markdown(f"### æµ‹è¯•ç”¨ä¾‹ {i+1}: {case.get('case_description', case.get('case_id', ''))}")
                display_test_case_details(case, show_system_prompt=True, inside_expander=True)
    
    # æç¤ºè¯ä¼˜åŒ–åŠŸèƒ½
    st.divider()
    st.subheader("ðŸ“ æç¤ºè¯ä¼˜åŒ–")
    
    # æ‰¾å‡ºæœ€å¥½å’Œæœ€å·®çš„æç¤ºè¯
    avg_scores = {name: calculate_average_score(data) for name, data in results.items()}
    
    if avg_scores:
        best_prompt = max(avg_scores.items(), key=lambda x: x[1])
        worst_prompt = min(avg_scores.items(), key=lambda x: x[1])
        
        st.write(f"æœ€ä½³æç¤ºè¯: **{best_prompt[0]}** (å¹³å‡åˆ†: {best_prompt[1]:.1f})")
        st.write(f"æœ€å·®æç¤ºè¯: **{worst_prompt[0]}** (å¹³å‡åˆ†: {worst_prompt[1]:.1f})")

    st.divider()
    st.subheader("ðŸ“ è¯„ä¼°æ—¥å¿—")

    # èŽ·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
    log_dir = Path("data/logs")
    if log_dir.exists():
        log_files = list(log_dir.glob("evaluator_log_*.txt"))
        if log_files:
            log_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # æ˜¾ç¤ºæœ€æ–°çš„å‡ ä¸ªæ—¥å¿—æ–‡ä»¶
            selected_log = st.selectbox(
                "é€‰æ‹©è¯„ä¼°æ—¥å¿—æ–‡ä»¶",
                options=log_files,
                format_func=lambda x: f"{x.name} ({datetime.fromtimestamp(x.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')})"
            )
            
            if selected_log:
                with open(selected_log, "r", encoding="utf-8") as f:
                    log_content = f.read()
                
                st.code(log_content)
                
                if st.button("åˆ é™¤é€‰ä¸­çš„æ—¥å¿—æ–‡ä»¶"):
                    try:
                        selected_log.unlink()
                        st.success("æ—¥å¿—æ–‡ä»¶å·²åˆ é™¤")
                        st.rerun()
                    except Exception as e:
                        st.error(f"åˆ é™¤æ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        else:
            st.info("æš‚æ— è¯„ä¼°æ—¥å¿—æ–‡ä»¶")
    else:
        st.info("æ—¥å¿—ç›®å½•ä¸å­˜åœ¨")

    # åˆ†äº«å’Œå¯¼å‡ºåŠŸèƒ½
    st.divider()
    st.subheader("ðŸ“¤ åˆ†äº«å’Œå¯¼å‡º")
    
    # å¯¼å‡ºä¸ºJSON
    if st.download_button(
        label="å¯¼å‡ºç»“æžœä¸ºJSON",
        data=json.dumps(results, ensure_ascii=False, indent=2),
        file_name=f"{selected_result}.json",
        mime="application/json"
    ):
        st.success("ç»“æžœå·²å¯¼å‡º")

def calculate_average_score(prompt_data):
    """è®¡ç®—æç¤ºè¯å¹³å‡åˆ†"""
    total_score = 0
    count = 0

    for case in prompt_data.get("test_cases", []):
        # ä»Ž responses[0] èŽ·å– evaluation
        response_list = case.get("responses", [])
        if not response_list:
            continue
        evaluation = response_list[0].get("evaluation") # Get evaluation from the first response

        # æ£€æŸ¥evaluationæ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºNone
        if evaluation is not None and "overall_score" in evaluation:
            total_score += evaluation["overall_score"]
            count += 1

    return total_score / count if count > 0 else 0