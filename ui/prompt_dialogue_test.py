import streamlit as st
import json
import time
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, Any, List, Tuple, Optional
import asyncio
import uuid

from config import load_template, get_template_list, load_test_set, get_test_set_list, save_test_set
from utils.common import render_prompt_template
from models.api_clients import get_provider_from_model, get_client
from ui.components import select_single_model
from utils.parallel_executor import execute_model_sync
from utils.evaluator import PromptEvaluator
from utils.test_set_manager import generate_unique_id, add_test_case


def render_prompt_dialogue_test():
    """æ¸²æŸ“æç¤ºè¯å¤šè½®å¯¹è¯æµ‹è¯•é¡µé¢"""
    st.title("ğŸ—£ï¸ æç¤ºè¯å¤šè½®å¯¹è¯æµ‹è¯•")
    
    st.markdown("""
    <div style="background-color: #f8f9fa; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
    <h3 style="color: #4b778d;">å¤šè½®å¯¹è¯äº¤äº’æµ‹è¯•</h3>
    <p>æµ‹è¯•æŸä¸ªå¯¹è¯æç¤ºè¯åœ¨å¤šè½®å¯¹è¯ä¸­çš„æ•ˆæœï¼Œè¯„ä¼°æ¯è½®å¯¹è¯è´¨é‡ï¼Œå¹¶åˆ†ææ¨¡å‹å’Œæç¤ºè¯å¯èƒ½å­˜åœ¨çš„é—®é¢˜ã€‚</p>
    </div>
    """, unsafe_allow_html=True)
    
    # åˆå§‹åŒ–å¯¹è¯å†å²
    if "dialogue_history" not in st.session_state:
        st.session_state.dialogue_history = []
    
    if "chat_turn" not in st.session_state:
        st.session_state.chat_turn = 0
        
    if "evaluation_results" not in st.session_state:
        st.session_state.evaluation_results = []
    
    if "prompt_ratings" not in st.session_state:
        st.session_state.prompt_ratings = []
        
    # åˆ›å»ºä¸¤åˆ—å¸ƒå±€ï¼šå·¦ä¾§è®¾ç½®ï¼Œå³ä¾§å¯¹è¯
    col_config, col_chat = st.columns([3, 4])
    
    with col_config:
        st.subheader("å¯¹è¯è®¾ç½®")
        
        # é€‰æ‹©æç¤ºè¯æ¨¡æ¿
        st.write("#### é€‰æ‹©æç¤ºè¯æ¨¡æ¿")
        template_list = get_template_list()
        if not template_list:
            st.warning("æœªæ‰¾åˆ°æç¤ºè¯æ¨¡æ¿ï¼Œè¯·å…ˆåˆ›å»ºæ¨¡æ¿")
            return
            
        selected_template_name = st.selectbox(
            "é€‰æ‹©æ¨¡æ¿",
            options=template_list,
            help="é€‰æ‹©è¦æµ‹è¯•çš„æç¤ºè¯æ¨¡æ¿"
        )
        
        # åŠ è½½æ¨¡æ¿
        if selected_template_name:
            template = load_template(selected_template_name)
            if template:
                st.success(f"å·²åŠ è½½æ¨¡æ¿: {selected_template_name}")
                
                # å±•ç¤ºæ¨¡æ¿å†…å®¹é¢„è§ˆ
                with st.expander("æŸ¥çœ‹æ¨¡æ¿å†…å®¹", expanded=False):
                    st.code(template.get("template", ""))
                    st.write(f"**æè¿°:** {template.get('description', 'æ— æè¿°')}")
            else:
                st.error(f"æ— æ³•åŠ è½½æ¨¡æ¿ {selected_template_name}")
                return
        
        # é€‰æ‹©æ¨¡å‹
        st.write("#### é€‰æ‹©è¯­è¨€æ¨¡å‹")
        model, provider = select_single_model(key_prefix="dialogue_test", help_text="é€‰æ‹©ç”¨äºæµ‹è¯•çš„æ¨¡å‹")
        
        if not model:
            st.warning("è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹")
        
        # æ¨¡å‹å‚æ•°è®¾ç½®
        st.write("#### æ¨¡å‹å‚æ•°")
        temperature = st.slider("Temperature", 0.0, 2.0, 0.7, 0.1, 
                              help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚è¾ƒé«˜çš„å€¼ä¼šäº§ç”Ÿæ›´å¤šæ ·åŒ–ä½†å¯èƒ½ä¸ä¸€è‡´çš„è¾“å‡º")
        
        # ä¼šè¯æ§åˆ¶æŒ‰é’®
        st.write("#### ä¼šè¯æ§åˆ¶")
        control_cols = st.columns(2)
        
        with control_cols[0]:
            if st.button("ğŸ”„ é‡ç½®å¯¹è¯", use_container_width=True):
                # é‡ç½®å¯¹è¯å†å²
                st.session_state.dialogue_history = []
                st.session_state.chat_turn = 0
                st.session_state.evaluation_results = []
                st.session_state.prompt_ratings = []
                st.experimental_rerun()
                
        with control_cols[1]:
            if st.button("ğŸ“Š åˆ†æå¯¹è¯", use_container_width=True, disabled=len(st.session_state.dialogue_history) < 2):
                show_dialogue_analysis()
                
        # è¯„ä¼°è®¾ç½®
        st.write("#### è¯„ä¼°è®¾ç½®")
        auto_evaluate = st.checkbox("è‡ªåŠ¨è¯„ä¼°æ¯è½®å¯¹è¯", value=True, help="æ¯è½®å¯¹è¯åè‡ªåŠ¨è¿›è¡Œè¯„ä¼°")
        
        # ç›®æ ‡æµ‹è¯•é›†é€‰æ‹© - ç”¨äºä¿å­˜å¯¹è¯è½®æ¬¡
        st.write("#### ç›®æ ‡æµ‹è¯•é›†")
        test_set_list = get_test_set_list()
        if not test_set_list:
            st.warning("æœªæ‰¾åˆ°æµ‹è¯•é›†ï¼Œè¯·å…ˆåˆ›å»ºæµ‹è¯•é›†")
            selected_test_set = None
        else:
            selected_test_set = st.selectbox(
                "é€‰æ‹©æµ‹è¯•é›†",
                options=test_set_list,
                help="é€‰æ‹©è¦å°†å¯¹è¯è½®æ¬¡ä¿å­˜åˆ°çš„æµ‹è¯•é›†"
            )
            
            if selected_test_set:
                test_set = load_test_set(selected_test_set)
                if test_set:
                    st.success(f"å·²åŠ è½½æµ‹è¯•é›†: {selected_test_set}")
                    with st.expander("æµ‹è¯•é›†ä¿¡æ¯", expanded=False):
                        st.write(f"**æè¿°:** {test_set.get('description', 'æ— æè¿°')}")
                        st.write(f"**æ¡ˆä¾‹æ•°é‡:** {len(test_set.get('cases', []))}")
                else:
                    st.error(f"æ— æ³•åŠ è½½æµ‹è¯•é›† {selected_test_set}")
        
        # æ·»åŠ æç¤ºè¯è‡ªå®šä¹‰åŒºåŸŸï¼Œå¯é€‰å¡«
        st.write("#### æç¤ºè¯è‡ªå®šä¹‰å˜é‡")
        prompt_vars = {}
        with st.expander("è‡ªå®šä¹‰å˜é‡", expanded=False):
            var_keys = st.text_area(
                "å˜é‡ååˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", 
                help="è¾“å…¥è¦è‡ªå®šä¹‰çš„å˜é‡åï¼Œæ¯è¡Œä¸€ä¸ª"
            ).strip().split("\n")
            
            for key in var_keys:
                if key and key.strip():
                    prompt_vars[key.strip()] = st.text_input(f"å˜é‡ '{key.strip()}'", key=f"var_{key.strip()}")
        
    with col_chat:
        st.subheader("å¯¹è¯æµ‹è¯•")
        
        # å¯¹è¯å®¹å™¨
        chat_container = st.container(height=500, border=True)
        
        # æ¸²æŸ“å¯¹è¯å†å²
        with chat_container:
            if not st.session_state.dialogue_history:
                st.info("å¯¹è¯å°šæœªå¼€å§‹ã€‚è¯·åœ¨ä¸‹æ–¹è¾“å…¥æ‚¨çš„ç¬¬ä¸€æ¡æ¶ˆæ¯ã€‚")
            else:
                for i, exchange in enumerate(st.session_state.dialogue_history):
                    st.markdown(f"**ç”¨æˆ·:** {exchange['user']}")
                    
                    with st.chat_message("assistant", avatar="ğŸ¤–"):
                        st.markdown(exchange['assistant'])
                        
                        # å¦‚æœæœ‰è¯„ä¼°ç»“æœï¼Œæ˜¾ç¤ºç®€è¦è¯„åˆ†
                        if i < len(st.session_state.evaluation_results) and st.session_state.evaluation_results[i]:
                            eval_result = st.session_state.evaluation_results[i]
                            if "overall_score" in eval_result:
                                score = eval_result["overall_score"]
                                score_color = "green" if score >= 80 else "orange" if score >= 60 else "red"
                                st.markdown(f"<span style='color:{score_color};font-size:0.8em;'>å›å¤è´¨é‡è¯„åˆ†: {score}/100</span>", unsafe_allow_html=True)
                                
                                # æ˜¾ç¤ºè¯„ä¼°è¯¦æƒ…æŒ‰é’®å’Œä¿å­˜è½®æ¬¡æŒ‰é’®
                                cols = st.columns(2)
                                with cols[0]:
                                    if st.button(f"æŸ¥çœ‹è¯¦ç»†è¯„ä¼° #{i+1}", key=f"detail_eval_{i}", use_container_width=True):
                                        show_evaluation_detail(eval_result, i+1)
                                
                                with cols[1]:
                                    if st.button(f"ä¿å­˜è½®æ¬¡ #{i+1}", key=f"save_turn_{i}", use_container_width=True):
                                        if selected_test_set:
                                            save_dialogue_turn_to_test_set(
                                                selected_test_set,
                                                i,
                                                st.session_state.dialogue_history,
                                                eval_result
                                            )
                                        else:
                                            st.error("è¯·å…ˆé€‰æ‹©ç›®æ ‡æµ‹è¯•é›†")
        
        # ç”¨æˆ·è¾“å…¥åŒº
        user_input = st.text_area("è¾“å…¥æ‚¨çš„æ¶ˆæ¯", key="user_msg_input", height=100)
        
        # æäº¤æŒ‰é’®
        if st.button("å‘é€", type="primary", use_container_width=True):
            if not user_input:
                st.warning("è¯·è¾“å…¥æ¶ˆæ¯")
            elif not model:
                st.warning("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªæ¨¡å‹")
            elif not selected_template_name:
                st.warning("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿")
            else:
                with st.spinner("æ¨¡å‹æ€è€ƒä¸­..."):
                    # å‡†å¤‡å¯¹è¯å†å²ä»¥æ·»åŠ åˆ°æç¤ºè¯ä¸­
                    chat_records = format_chat_history(st.session_state.dialogue_history)
                    
                    # å‡†å¤‡æ¨¡æ¿å˜é‡
                    template_vars = {
                        "chat_records": chat_records,
                        **prompt_vars  # æ·»åŠ ç”¨æˆ·è‡ªå®šä¹‰çš„å˜é‡
                    }
                    
                    # æ¸²æŸ“æç¤ºè¯æ¨¡æ¿
                    prompt_template = render_prompt_template(template, {"variables": template_vars}, {"variables": {}})
                    
                    # åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨
                    messages = []
                    
                    # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
                    messages.append({"role": "system", "content": prompt_template})
                    
                    # è°ƒç”¨æ¨¡å‹
                    client = get_client(provider)
                    params = {"temperature": temperature, "max_tokens": 2000}
                    
                    # æ·»åŠ æ–°çš„ç”¨æˆ·æ¶ˆæ¯
                    messages.append({"role": "user", "content": user_input})
                    
                    try:
                        # ä½¿ç”¨åŒæ­¥è°ƒç”¨
                        response = execute_model_sync(
                            model=model,
                            provider=provider,
                            messages=messages,
                            params=params
                        )
                        
                        if "error" in response and response["error"]:
                            st.error(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {response['error']}")
                            return
                        
                        # è·å–æ¨¡å‹å›å¤
                        assistant_response = response.get("text", "")
                        
                        # æ›´æ–°å¯¹è¯å†å²
                        st.session_state.dialogue_history.append({
                            "user": user_input,
                            "assistant": assistant_response,
                            "model": model,
                            "turn": st.session_state.chat_turn + 1,
                            "timestamp": int(time.time()),
                            "prompt_template": prompt_template,
                            "usage": response.get("usage", {}),
                            "complete_messages": messages,
                            "chat_records": chat_records  # ä¿å­˜å½“å‰è½®æ¬¡çš„å¯¹è¯å†å²
                        })
                        
                        # å¢åŠ å¯¹è¯å›åˆæ•°
                        st.session_state.chat_turn += 1
                        
                        # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨è¯„ä¼°ï¼Œè¯„ä¼°æ­¤è½®å¯¹è¯
                        if auto_evaluate:
                            evaluation = evaluate_dialogue_turn(
                                user_input, 
                                assistant_response, 
                                prompt_template,
                                st.session_state.chat_turn
                            )
                            st.session_state.evaluation_results.append(evaluation)
                            
                            # ä¿å­˜æç¤ºè¯è¯„åˆ†è®°å½•
                            if "scores" in evaluation:
                                st.session_state.prompt_ratings.append({
                                    "turn": st.session_state.chat_turn,
                                    "scores": evaluation["scores"],
                                    "overall": evaluation["overall_score"]
                                })
                        else:
                            # å¦‚æœæœªå¯ç”¨è‡ªåŠ¨è¯„ä¼°ï¼Œæ·»åŠ ä¸€ä¸ªç©ºçš„å ä½ç¬¦
                            st.session_state.evaluation_results.append(None)
                        
                        # æ¸…ç©ºè¾“å…¥æ¡† (é€šè¿‡é‡æ–°æ¸²æŸ“é¡µé¢å®ç°)
                        st.experimental_rerun()
                    
                    except Exception as e:
                        st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")


def format_chat_history(history: List[Dict]) -> str:
    """å°†å¯¹è¯å†å²æ ¼å¼åŒ–ä¸ºæ¨¡æ¿å¯ç”¨çš„æ ¼å¼ï¼Œåªä¿ç•™æœ€è¿‘5è½®å¯¹è¯"""
    # åªä¿ç•™æœ€è¿‘çš„5è½®å¯¹è¯
    recent_history = history[-5:] if len(history) > 5 else history
    
    formatted = ""
    
    for exchange in recent_history:
        formatted += f"ç”¨æˆ·: {exchange['user']}\n"
        formatted += f"åŠ©æ‰‹: {exchange['assistant']}\n\n"
    
    return formatted.strip()


def save_dialogue_turn_to_test_set(test_set_name: str, turn_index: int, dialogue_history: List[Dict], evaluation: Dict = None) -> None:
    """å°†æŒ‡å®šçš„å¯¹è¯è½®æ¬¡ä¿å­˜åˆ°æµ‹è¯•é›†ä¸­
    
    Args:
        test_set_name: æµ‹è¯•é›†åç§°
        turn_index: å¯¹è¯è½®æ¬¡ç´¢å¼•
        dialogue_history: å®Œæ•´å¯¹è¯å†å²
        evaluation: è¯„ä¼°ç»“æœï¼ˆå¯é€‰ï¼‰
    """
    if turn_index < 0 or turn_index >= len(dialogue_history):
        st.error(f"æ— æ•ˆçš„å¯¹è¯è½®æ¬¡ç´¢å¼•: {turn_index}")
        return
    
    # åŠ è½½ç›®æ ‡æµ‹è¯•é›†
    test_set = load_test_set(test_set_name)
    if not test_set:
        st.error(f"æ— æ³•åŠ è½½æµ‹è¯•é›† {test_set_name}")
        return
    
    # è·å–å½“å‰è½®æ¬¡å¯¹è¯
    turn_data = dialogue_history[turn_index]
    
    # è·å–å½“å‰è½®æ¬¡çš„å¯¹è¯å†å²ä¸Šä¸‹æ–‡
    chat_records = turn_data.get("chat_records", format_chat_history(dialogue_history[:turn_index]))
    
    # åˆ›å»ºæ–°çš„æµ‹è¯•ç”¨ä¾‹
    new_case = {
        "id": generate_unique_id(),
        "description": f"å¯¹è¯è½®æ¬¡ #{turn_index+1} - {time.strftime('%Y-%m-%d %H:%M:%S')}",
        "user_input": turn_data.get("user", ""),
        "expected_output": "",  # é»˜è®¤ä¸è®¾ç½®æœŸæœ›è¾“å‡º
        "model_response": turn_data.get("assistant", ""),  # ä¿å­˜æ¨¡å‹å®é™…å“åº”
        "evaluation_criteria": {
            "relevance": "æ¨¡å‹å“åº”ä¸ç”¨æˆ·æé—®çš„ç›¸å…³æ€§",
            "helpfulness": "æ¨¡å‹å“åº”å¯¹è§£å†³ç”¨æˆ·é—®é¢˜çš„å¸®åŠ©ç¨‹åº¦",
            "accuracy": "æ¨¡å‹å“åº”ä¸­ä¿¡æ¯çš„å‡†ç¡®æ€§",
            "prompt_following": "æ¨¡å‹éµå¾ªæç¤ºè¯æŒ‡ä»¤çš„ç¨‹åº¦",
            "consistency": "æ¨¡å‹å›å¤ä¸ä¹‹å‰å¯¹è¯çš„ä¸€è‡´æ€§",
            "coherence": "æ¨¡å‹å›å¤çš„è¿è´¯æ€§å’Œé€»è¾‘æ€§",
        },
        "variables": {
            "chat_records": chat_records,  # ä¿å­˜å¯¹è¯å†å²ä¸Šä¸‹æ–‡
            "model": turn_data.get("model", ""),
            "timestamp": turn_data.get("timestamp", int(time.time())),
        },
        "timestamp": int(time.time())
    }
    
    # å¦‚æœæœ‰è¯„ä¼°ç»“æœï¼Œä¹Ÿä¿å­˜ä¸‹æ¥
    if evaluation:
        new_case["evaluation"] = evaluation
    
    # æ·»åŠ åˆ°æµ‹è¯•é›†
    test_set = add_test_case(test_set, new_case)
    
    # ä¿å­˜æ›´æ–°çš„æµ‹è¯•é›†
    save_test_set(test_set_name, test_set)
    
    # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
    st.success(f"å·²å°†å¯¹è¯è½®æ¬¡ #{turn_index+1} ä¿å­˜åˆ°æµ‹è¯•é›† '{test_set_name}'")


def evaluate_dialogue_turn(user_input: str, model_response: str, prompt_template: str, turn_number: int) -> Dict:
    """è¯„ä¼°å•è½®å¯¹è¯è´¨é‡"""
    evaluator = PromptEvaluator()
    
    # è®¾è®¡é’ˆå¯¹å¯¹è¯çš„è¯„ä¼°æ ‡å‡†
    evaluation_criteria = {
        "relevance": "æ¨¡å‹å“åº”ä¸ç”¨æˆ·æé—®çš„ç›¸å…³æ€§(0-100åˆ†)",
        "helpfulness": "æ¨¡å‹å“åº”å¯¹è§£å†³ç”¨æˆ·é—®é¢˜çš„å¸®åŠ©ç¨‹åº¦(0-100åˆ†)",
        "accuracy": "æ¨¡å‹å“åº”ä¸­ä¿¡æ¯çš„å‡†ç¡®æ€§(0-100åˆ†)",
        "prompt_following": "æ¨¡å‹éµå¾ªæç¤ºè¯æŒ‡ä»¤çš„ç¨‹åº¦(0-100åˆ†)",
        "consistency": "æ¨¡å‹å›å¤ä¸ä¹‹å‰å¯¹è¯çš„ä¸€è‡´æ€§(0-100åˆ†)",
        "coherence": "æ¨¡å‹å›å¤çš„è¿è´¯æ€§å’Œé€»è¾‘æ€§(0-100åˆ†)",
    }
    
    # æ„å»ºè¯„ä¼°æç¤º
    combined_prompt = f"ç”¨æˆ·é—®é¢˜:\n{user_input}\n\næç¤ºè¯æ¨¡æ¿:\n{prompt_template}"
    
    # å› ä¸ºæ˜¯å¯¹è¯ï¼Œæ²¡æœ‰æ ‡å‡†ç­”æ¡ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªé€šç”¨çš„æœŸæœ›
    expected_output = f"å›åˆ {turn_number}ï¼šæ ¹æ®æç¤ºè¯å’Œç”¨æˆ·é—®é¢˜ç»™å‡ºæœ‰å¸®åŠ©ã€ç›¸å…³ä¸”å‡†ç¡®çš„å›ç­”"
    
    # è°ƒç”¨è¯„ä¼°å™¨
    evaluation = evaluator.evaluate_response_sync(
        model_response,
        expected_output,
        evaluation_criteria,
        combined_prompt
    )
    
    # è®¡ç®—é’ˆå¯¹æç¤ºè¯å’Œæ¨¡å‹çš„é—®é¢˜è¯Šæ–­
    if "scores" in evaluation:
        scores = evaluation["scores"]
        
        # åˆ†æå¯èƒ½çš„é—®é¢˜
        issues = []
        
        # æ¨¡å‹é—®é¢˜åˆ¤æ–­æ ‡å‡†
        if scores.get("accuracy", 0) < 70 or scores.get("coherence", 0) < 70:
            issues.append({
                "type": "model",
                "severity": "high" if scores.get("accuracy", 0) < 50 else "medium",
                "description": "æ¨¡å‹ç”Ÿæˆçš„å†…å®¹å¯èƒ½ä¸å‡†ç¡®æˆ–ä¸è¿è´¯",
                "suggestion": "è€ƒè™‘ä½¿ç”¨æ›´é«˜çº§çš„æ¨¡å‹æˆ–è°ƒä½temperatureå‚æ•°"
            })
            
        # æç¤ºè¯é—®é¢˜åˆ¤æ–­æ ‡å‡†
        if scores.get("prompt_following", 0) < 70:
            issues.append({
                "type": "prompt",
                "severity": "high" if scores.get("prompt_following", 0) < 50 else "medium",
                "description": "æ¨¡å‹æœªèƒ½è‰¯å¥½åœ°éµå¾ªæç¤ºè¯æŒ‡ä»¤",
                "suggestion": "æ˜ç¡®æç¤ºè¯ä¸­çš„æŒ‡ä»¤ï¼Œå¢åŠ è¯¦ç»†çš„æ ¼å¼è¦æ±‚å’Œçº¦æŸ"
            })
            
        if scores.get("consistency", 0) < 70:
            issues.append({
                "type": "prompt",
                "severity": "medium",
                "description": "æ¨¡å‹å›å¤ä¸ä¹‹å‰å¯¹è¯ç¼ºä¹ä¸€è‡´æ€§",
                "suggestion": "åœ¨æç¤ºè¯ä¸­å¼ºè°ƒä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼Œæˆ–å¢åŠ å¯¹è¯å†å²æ€»ç»“æŒ‡ä»¤"
            })
            
        # å°†é—®é¢˜åˆ†ææ·»åŠ åˆ°è¯„ä¼°ç»“æœä¸­
        evaluation["issues"] = issues
    
    return evaluation


def show_evaluation_detail(evaluation: Dict, turn_number: int):
    """æ˜¾ç¤ºè¯¦ç»†çš„è¯„ä¼°ç»“æœ"""
    st.subheader(f"ç¬¬ {turn_number} è½®å¯¹è¯è¯„ä¼°ç»“æœ")
    
    # å¦‚æœæœ‰é”™è¯¯ä¿¡æ¯ï¼Œæ˜¾ç¤ºé”™è¯¯
    if "error" in evaluation:
        st.warning(f"è¯„ä¼°è¿‡ç¨‹é‡åˆ°é—®é¢˜: {evaluation.get('error')}")
        return
    
    # åˆ›å»ºé€‰é¡¹å¡å¸ƒå±€
    tab1, tab2, tab3 = st.tabs(["è¯„åˆ†è¯¦æƒ…", "é—®é¢˜è¯Šæ–­", "åˆ†ææŠ¥å‘Š"])
    
    # æ˜¾ç¤ºåˆ†æ•°
    with tab1:
        scores = evaluation["scores"]
        overall = evaluation["overall_score"]
        
        # ä»¥å½©è‰²æ–¹å—å’Œç™¾åˆ†æ¯”å½¢å¼æ˜¾ç¤ºåˆ†æ•°
        st.write("#### å„ç»´åº¦è¯„åˆ†")
        
        # ä¸ºæ¯ä¸ªåˆ†æ•°åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡æ ·å¼çš„æ˜¾ç¤º
        for dimension, score in scores.items():
            if dimension != "prompt_efficiency":  # æ’é™¤æç¤ºè¯æ•ˆç‡ï¼Œå› ä¸ºè¿™ä¸æ˜¯å¯¹è¯è´¨é‡çš„ç›´æ¥è¡¡é‡
                # ç¡®å®šé¢œè‰²
                color = "green" if score >= 80 else "orange" if score >= 60 else "red"
                
                # åˆ›å»ºå¯è§†åŒ–çš„åˆ†æ•°æ¡
                st.markdown(
                    f"**{dimension.capitalize()}**: "
                    f"<div style='background-color:#f0f2f6;border-radius:10px;height:25px;width:100%;margin-bottom:10px;'>"
                    f"<div style='background-color:{color};border-radius:10px;height:25px;width:{score}%;padding-left:10px;'>"
                    f"<span style='color:white;line-height:25px;'>{score}%</span>"
                    f"</div></div>",
                    unsafe_allow_html=True
                )
        
        # æ€»ä½“è¯„åˆ†
        st.write("#### æ€»ä½“è¯„åˆ†")
        overall_color = "green" if overall >= 80 else "orange" if overall >= 60 else "red"
        st.markdown(
            f"<div style='background-color:#f0f2f6;border-radius:10px;height:30px;width:100%;'>"
            f"<div style='background-color:{overall_color};border-radius:10px;height:30px;width:{overall}%;padding-left:10px;'>"
            f"<span style='color:white;line-height:30px;font-weight:bold;'>{overall}%</span>"
            f"</div></div>",
            unsafe_allow_html=True
        )
    
    # é—®é¢˜è¯Šæ–­
    with tab2:
        issues = evaluation.get("issues", [])
        
        if not issues:
            st.success("æœªæ£€æµ‹åˆ°æ˜æ˜¾é—®é¢˜ï¼Œæ­¤è½®å¯¹è¯è¡¨ç°è‰¯å¥½ï¼")
        else:
            st.write("#### æ£€æµ‹åˆ°çš„é—®é¢˜")
            
            # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤ºé—®é¢˜
            model_issues = [issue for issue in issues if issue["type"] == "model"]
            prompt_issues = [issue for issue in issues if issue["type"] == "prompt"]
            
            if model_issues:
                st.write("##### æ¨¡å‹é—®é¢˜")
                for issue in model_issues:
                    severity_color = "red" if issue["severity"] == "high" else "orange"
                    st.markdown(f"<div style='border-left:4px solid {severity_color};padding-left:10px;margin-bottom:10px;'>"
                               f"<p><strong>ä¸¥é‡ç¨‹åº¦:</strong> {issue['severity']}</p>"
                               f"<p><strong>é—®é¢˜:</strong> {issue['description']}</p>"
                               f"<p><strong>å»ºè®®:</strong> {issue['suggestion']}</p>"
                               f"</div>", unsafe_allow_html=True)
            
            if prompt_issues:
                st.write("##### æç¤ºè¯é—®é¢˜")
                for issue in prompt_issues:
                    severity_color = "red" if issue["severity"] == "high" else "orange"
                    st.markdown(f"<div style='border-left:4px solid {severity_color};padding-left:10px;margin-bottom:10px;'>"
                               f"<p><strong>ä¸¥é‡ç¨‹åº¦:</strong> {issue['severity']}</p>"
                               f"<p><strong>é—®é¢˜:</strong> {issue['description']}</p>"
                               f"<p><strong>å»ºè®®:</strong> {issue['suggestion']}</p>"
                               f"</div>", unsafe_allow_html=True)
    
    # åˆ†ææŠ¥å‘Š
    with tab3:
        if "summary" in evaluation:
            st.write("#### è¯„ä¼°æ€»ç»“")
            st.info(evaluation["summary"])
        
        if "analysis" in evaluation:
            st.write("#### è¯¦ç»†åˆ†æ")
            st.markdown(evaluation["analysis"])
            
        # Tokenä½¿ç”¨æƒ…å†µ
        if "prompt_info" in evaluation:
            st.write("#### æç¤ºè¯ä¿¡æ¯")
            st.write(f"æç¤ºè¯tokenæ•°é‡: {evaluation['prompt_info'].get('token_count', 'N/A')}")


def show_dialogue_analysis():
    """æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯çš„åˆ†æç»“æœ"""
    if not st.session_state.dialogue_history or len(st.session_state.dialogue_history) < 2:
        st.warning("éœ€è¦è‡³å°‘ä¸¤è½®å¯¹è¯æ‰èƒ½è¿›è¡Œåˆ†æ")
        return
    
    st.subheader("ğŸ” å¯¹è¯åˆ†æ")
    
    # åˆ›å»ºé€‰é¡¹å¡å¸ƒå±€
    tab1, tab2, tab3 = st.tabs(["å¯¹è¯è´¨é‡è¶‹åŠ¿", "æç¤ºè¯æ•ˆæœåˆ†æ", "æ”¹è¿›å»ºè®®"])
    
    with tab1:
        st.write("#### å¯¹è¯è´¨é‡éšæ—¶é—´å˜åŒ–è¶‹åŠ¿")
        
        # æå–è¯„åˆ†æ•°æ®
        if st.session_state.prompt_ratings:
            # è½¬æ¢ä¸ºpandas DataFrameä»¥ä¾¿åˆ†æ
            df = pd.DataFrame([
                {
                    "turn": rating["turn"],
                    "overall": rating["overall"],
                    **rating["scores"]
                }
                for rating in st.session_state.prompt_ratings
            ])
            
            # ç»˜åˆ¶æ€»ä½“è¯„åˆ†è¶‹åŠ¿å›¾
            fig, ax = plt.subplots(figsize=(10, 4))
            ax.plot(df["turn"], df["overall"], marker='o', linewidth=2, label='æ€»ä½“è¯„åˆ†')
            ax.set_xlabel('å¯¹è¯å›åˆ')
            ax.set_ylabel('è¯„åˆ†')
            ax.set_title('å¯¹è¯è´¨é‡è¶‹åŠ¿')
            ax.grid(True, linestyle='--', alpha=0.7)
            ax.set_ylim(0, 100)
            st.pyplot(fig)
            
            # ç»˜åˆ¶å„ç»´åº¦è¯„åˆ†è¶‹åŠ¿
            dimensions = [col for col in df.columns if col not in ["turn", "overall", "prompt_efficiency"]]
            if dimensions:
                fig, ax = plt.subplots(figsize=(10, 6))
                for dim in dimensions:
                    ax.plot(df["turn"], df[dim], marker='o', linewidth=2, label=dim)
                ax.set_xlabel('å¯¹è¯å›åˆ')
                ax.set_ylabel('è¯„åˆ†')
                ax.set_title('å„ç»´åº¦è¯„åˆ†è¶‹åŠ¿')
                ax.grid(True, linestyle='--', alpha=0.7)
                ax.set_ylim(0, 100)
                ax.legend()
                st.pyplot(fig)
                
                # è®¡ç®—è¯„åˆ†çš„ç»Ÿè®¡æ•°æ®
                st.write("#### è¯„åˆ†ç»Ÿè®¡æ•°æ®")
                stats_df = df[dimensions + ["overall"]].describe().T[["mean", "std", "min", "max"]]
                stats_df = stats_df.round(2)
                
                # ä¸ºæ•°æ®æ·»åŠ é¢œè‰²æ ‡è®°
                def color_mean(val):
                    color = 'green' if val >= 80 else 'orange' if val >= 60 else 'red'
                    return f'color: {color}; font-weight: bold'
                
                # åº”ç”¨æ ·å¼å¹¶æ˜¾ç¤º
                st.dataframe(stats_df.style.applymap(color_mean, subset=['mean']))
        else:
            st.info("å°šæ— è¯„ä¼°æ•°æ®ï¼Œè¯·ç¡®ä¿å·²å¯ç”¨è‡ªåŠ¨è¯„ä¼°æˆ–æ‰‹åŠ¨è¯„ä¼°å¯¹è¯")
    
    with tab2:
        st.write("#### æç¤ºè¯æ•ˆæœåˆ†æ")
        
        # åˆ†æå„è½®å¯¹è¯ä¸­æç¤ºè¯éµå¾ªåº¦
        if st.session_state.prompt_ratings:
            # è®¡ç®—æç¤ºè¯éµå¾ªåº¦ç»Ÿè®¡
            prompt_following_scores = [rating["scores"].get("prompt_following", 0) for rating in st.session_state.prompt_ratings]
            avg_following = sum(prompt_following_scores) / len(prompt_following_scores) if prompt_following_scores else 0
            
            # æ˜¾ç¤ºæç¤ºè¯éµå¾ªåº¦è¯„åˆ†
            col1, col2 = st.columns(2)
            with col1:
                st.metric("å¹³å‡æç¤ºè¯éµå¾ªåº¦", f"{avg_following:.1f}/100")
                
            with col2:
                min_following = min(prompt_following_scores) if prompt_following_scores else 0
                st.metric("æœ€ä½æç¤ºè¯éµå¾ªåº¦", f"{min_following}/100")
            
            # æç¤ºè¯é—®é¢˜æ±‡æ€»
            prompt_issues = []
            for i, eval_result in enumerate(st.session_state.evaluation_results):
                if eval_result and "issues" in eval_result:
                    for issue in eval_result["issues"]:
                        if issue["type"] == "prompt":
                            prompt_issues.append({
                                "turn": i+1,
                                "severity": issue["severity"],
                                "description": issue["description"],
                                "suggestion": issue["suggestion"]
                            })
            
            if prompt_issues:
                st.write("#### æç¤ºè¯é—®é¢˜æ±‡æ€»")
                issue_df = pd.DataFrame(prompt_issues)
                st.dataframe(issue_df, use_container_width=True)
                
                # æŒ‰ä¸¥é‡ç¨‹åº¦è®¡æ•°
                severity_counts = issue_df["severity"].value_counts()
                
                # ç»˜åˆ¶é¥¼å›¾
                fig, ax = plt.subplots()
                ax.pie(severity_counts, labels=severity_counts.index, autopct='%1.1f%%',
                      colors=['red' if x == 'high' else 'orange' for x in severity_counts.index])
                ax.set_title('æç¤ºè¯é—®é¢˜ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ')
                st.pyplot(fig)
            else:
                st.success("æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æç¤ºè¯é—®é¢˜")
        else:
            st.info("å°šæ— è¯„ä¼°æ•°æ®ï¼Œè¯·ç¡®ä¿å·²å¯ç”¨è‡ªåŠ¨è¯„ä¼°æˆ–æ‰‹åŠ¨è¯„ä¼°å¯¹è¯")
    
    with tab3:
        st.write("#### æ”¹è¿›å»ºè®®")
        
        # æ±‡æ€»æ‰€æœ‰å»ºè®®
        all_suggestions = []
        model_suggestions = []
        prompt_suggestions = []
        
        for eval_result in st.session_state.evaluation_results:
            if eval_result and "issues" in eval_result:
                for issue in eval_result["issues"]:
                    if issue["type"] == "prompt" and issue["suggestion"] not in prompt_suggestions:
                        prompt_suggestions.append(issue["suggestion"])
                    elif issue["type"] == "model" and issue["suggestion"] not in model_suggestions:
                        model_suggestions.append(issue["suggestion"])
        
        # æç¤ºè¯æ”¹è¿›å»ºè®®
        st.write("##### æç¤ºè¯æ”¹è¿›å»ºè®®")
        if prompt_suggestions:
            for i, suggestion in enumerate(prompt_suggestions):
                st.markdown(f"{i+1}. {suggestion}")
        else:
            st.success("æç¤ºè¯è¡¨ç°è‰¯å¥½ï¼Œæ²¡æœ‰ç‰¹åˆ«éœ€è¦æ”¹è¿›çš„åœ°æ–¹")
        
        # æ¨¡å‹é€‰æ‹©å»ºè®®
        st.write("##### æ¨¡å‹ä½¿ç”¨å»ºè®®")
        if model_suggestions:
            for i, suggestion in enumerate(model_suggestions):
                st.markdown(f"{i+1}. {suggestion}")
        else:
            st.success("æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œæ²¡æœ‰ç‰¹åˆ«éœ€è¦è°ƒæ•´çš„åœ°æ–¹")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        if st.button("ç”Ÿæˆæ”¹è¿›æŠ¥å‘Š", use_container_width=True):
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ”¹è¿›æŠ¥å‘Š..."):
                report = generate_improvement_report(
                    st.session_state.dialogue_history,
                    st.session_state.evaluation_results
                )
                st.code(report, language="markdown")
                
                # æä¾›ä¸‹è½½é“¾æ¥
                st.download_button(
                    label="ä¸‹è½½æŠ¥å‘Š",
                    data=report,
                    file_name=f"dialogue_analysis_{int(time.time())}.md",
                    mime="text/markdown"
                )


def generate_improvement_report(dialogue_history: List[Dict], evaluation_results: List[Dict]) -> str:
    """ç”Ÿæˆå¯¹è¯æ”¹è¿›æŠ¥å‘Š"""
    # æå–åŸºæœ¬ä¿¡æ¯
    num_turns = len(dialogue_history)
    model_name = dialogue_history[0]["model"] if dialogue_history else "æœªçŸ¥æ¨¡å‹"
    
    # è®¡ç®—å¹³å‡åˆ†æ•°
    avg_scores = {}
    overall_scores = []
    
    for eval_result in evaluation_results:
        if eval_result and "scores" in eval_result:
            for key, score in eval_result["scores"].items():
                if key != "prompt_efficiency":
                    avg_scores[key] = avg_scores.get(key, 0) + score
            
            if "overall_score" in eval_result:
                overall_scores.append(eval_result["overall_score"])
    
    # è®¡ç®—å¹³å‡å€¼
    for key in avg_scores:
        avg_scores[key] /= len(evaluation_results) if evaluation_results else 1
    
    avg_overall = sum(overall_scores) / len(overall_scores) if overall_scores else 0
    
    # æ”¶é›†é—®é¢˜å’Œå»ºè®®
    prompt_issues = []
    model_issues = []
    
    for eval_result in evaluation_results:
        if eval_result and "issues" in eval_result:
            for issue in eval_result["issues"]:
                if issue["type"] == "prompt" and issue not in prompt_issues:
                    prompt_issues.append(issue)
                elif issue["type"] == "model" and issue not in model_issues:
                    model_issues.append(issue)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""# å¤šè½®å¯¹è¯æµ‹è¯•åˆ†ææŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **æµ‹è¯•æ—¶é—´**: {time.strftime("%Y-%m-%d %H:%M:%S")}
- **å¯¹è¯è½®æ•°**: {num_turns}
- **ä½¿ç”¨æ¨¡å‹**: {model_name}

## è¯„åˆ†æ‘˜è¦
- **æ€»ä½“è¯„åˆ†**: {avg_overall:.1f}/100
"""
    
    # æ·»åŠ å„ç»´åº¦å¹³å‡åˆ†
    report += "\n### å„ç»´åº¦å¹³å‡åˆ†\n"
    for key, score in avg_scores.items():
        report += f"- **{key}**: {score:.1f}/100\n"
    
    # æ·»åŠ é—®é¢˜åˆ†æ
    report += "\n## é—®é¢˜åˆ†æ\n"
    
    if prompt_issues:
        report += "\n### æç¤ºè¯é—®é¢˜\n"
        for issue in prompt_issues:
            report += f"- **ä¸¥é‡ç¨‹åº¦**: {issue['severity']}\n"
            report += f"  - **æè¿°**: {issue['description']}\n"
            report += f"  - **å»ºè®®**: {issue['suggestion']}\n"
    else:
        report += "\n### æç¤ºè¯é—®é¢˜\n- æœªæ£€æµ‹åˆ°æ˜æ˜¾é—®é¢˜\n"
    
    if model_issues:
        report += "\n### æ¨¡å‹é—®é¢˜\n"
        for issue in model_issues:
            report += f"- **ä¸¥é‡ç¨‹åº¦**: {issue['severity']}\n"
            report += f"  - **æè¿°**: {issue['description']}\n"
            report += f"  - **å»ºè®®**: {issue['suggestion']}\n"
    else:
        report += "\n### æ¨¡å‹é—®é¢˜\n- æœªæ£€æµ‹åˆ°æ˜æ˜¾é—®é¢˜\n"
    
    # æ·»åŠ æ”¹è¿›å»ºè®®ç»¼è¿°
    report += "\n## æ”¹è¿›å»ºè®®æ€»ç»“\n"
    
    # æç¤ºè¯æ”¹è¿›å»ºè®®
    prompt_suggestions = list(set([issue["suggestion"] for issue in prompt_issues]))
    if prompt_suggestions:
        report += "\n### æç¤ºè¯æ”¹è¿›å»ºè®®\n"
        for i, suggestion in enumerate(prompt_suggestions):
            report += f"{i+1}. {suggestion}\n"
    else:
        report += "\n### æç¤ºè¯æ”¹è¿›å»ºè®®\n- æç¤ºè¯è¡¨ç°è‰¯å¥½ï¼Œæ²¡æœ‰ç‰¹åˆ«éœ€è¦æ”¹è¿›çš„åœ°æ–¹\n"
    
    # æ¨¡å‹æ”¹è¿›å»ºè®®
    model_suggestions = list(set([issue["suggestion"] for issue in model_issues]))
    if model_suggestions:
        report += "\n### æ¨¡å‹ä½¿ç”¨å»ºè®®\n"
        for i, suggestion in enumerate(model_suggestions):
            report += f"{i+1}. {suggestion}\n"
    else:
        report += "\n### æ¨¡å‹ä½¿ç”¨å»ºè®®\n- æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œæ²¡æœ‰ç‰¹åˆ«éœ€è¦è°ƒæ•´çš„åœ°æ–¹\n"
    
    return report