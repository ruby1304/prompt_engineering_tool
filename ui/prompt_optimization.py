# prompt_optimization.py

import streamlit as st
import json
import pandas as pd
import asyncio
from datetime import datetime
import time
import plotly.express as px
import plotly.graph_objects as go

from config import get_template_list, load_template, get_test_set_list, load_test_set, save_template, get_available_models, get_all_template_names_sorted
from models.api_clients import get_client, get_provider_from_model
from models.token_counter import count_tokens, estimate_cost
from utils.evaluator import PromptEvaluator
from utils.optimizer import PromptOptimizer
from utils.common import (
    calculate_average_score, 
    get_dimension_scores, 
    create_dimension_radar_chart,
    run_test,
    save_optimized_template
)
from ui.components import (
    display_test_summary,
    display_response_tabs,
    display_evaluation_results,
    display_test_case_details
)

def render_prompt_optimization():
    tab1, tab2 = st.tabs(["ä¸“é¡¹ä¼˜åŒ–ï¼ˆæœ‰æ ·æœ¬ï¼‰", "è‡ªåŠ¨è¿­ä»£ä¼˜åŒ–"])
    with tab1:
        st.title("ğŸ” æç¤ºè¯ä¸“é¡¹ä¼˜åŒ–")
        
        st.markdown("""
        è¿™ä¸ªå·¥å…·ä¸“æ³¨äºå•æç¤ºè¯å•æ¨¡å‹çš„æ·±åº¦ä¼˜åŒ–ã€‚æ‚¨å¯ä»¥é€‰æ‹©ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿å’Œä¸€ä¸ªæ¨¡å‹ï¼Œ
        è¿è¡Œæµ‹è¯•å¹¶è·å–AIç”Ÿæˆçš„ä¼˜åŒ–ç‰ˆæœ¬æç¤ºè¯ã€‚
        """)
        
        # æ­¥éª¤1: é€‰æ‹©æç¤ºè¯å’Œæ¨¡å‹
        st.subheader("æ­¥éª¤1: é€‰æ‹©æç¤ºè¯å’Œæ¨¡å‹")
        
        col1, col2 = st.columns(2)
        
        with col1:
            template_list = get_all_template_names_sorted()
            if not template_list:
                st.warning("æœªæ‰¾åˆ°ä»»ä½•æç¤ºè¯æ¨¡æ¿ï¼ˆåŒ…æ‹¬ç³»ç»Ÿæ¨¡æ¿ï¼‰ï¼Œè¯·å…ˆåˆ›å»ºæ¨¡æ¿")
                return
                
            selected_template = st.selectbox(
                "é€‰æ‹©æç¤ºè¯æ¨¡æ¿ï¼ˆåŒ…å«ç³»ç»Ÿæ¨¡æ¿ï¼‰",
                template_list
            )
            
            if selected_template:
                template = load_template(selected_template)
                st.info(f"**æè¿°**: {template.get('description', 'æ— æè¿°')}")
                
                with st.expander("æŸ¥çœ‹æç¤ºè¯å†…å®¹"):
                    st.code(template.get("template", ""))
        
        with col2:
            # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
            available_models = get_available_models()
            all_models = []
            
            # åˆ›å»ºç»Ÿä¸€çš„æ¨¡å‹åˆ—è¡¨
            for provider, models in available_models.items():
                for model in models:
                    all_models.append((provider, model))
            
            # åˆ›å»ºä¸‹æ‹‰é€‰é¡¹
            model_options = [f"{model} ({provider})" for provider, model in all_models]
            model_map = {f"{model} ({provider})": (model, provider) for provider, model in all_models}
            
            # é€‰æ‹©æ¨¡å‹
            selected_model_option = st.selectbox(
                "é€‰æ‹©æ¨¡å‹",
                model_options
            )
            
            if selected_model_option:
                selected_model, selected_provider = model_map[selected_model_option]
            else:
                selected_model = ""
                selected_provider = ""
            
            # è¿è¡Œå‚æ•°
            st.subheader("è¿è¡Œå‚æ•°")
            temperature = st.slider("Temperature", 0.0, 2.0, 0.7, 0.1)
            max_tokens = st.slider("æœ€å¤§è¾“å‡ºToken", 100, 4000, 1000, 100)
            repeat_count = st.slider("æ¯ä¸ªæµ‹è¯•é‡å¤æ¬¡æ•°", 1, 3, 2, 1)
        
        # æ­¥éª¤2: é€‰æ‹©æµ‹è¯•é›†
        st.subheader("æ­¥éª¤2: é€‰æ‹©æµ‹è¯•é›†")
        
        test_set_list = get_test_set_list()
        if not test_set_list:
            st.warning("æœªæ‰¾åˆ°æµ‹è¯•é›†ï¼Œè¯·å…ˆåˆ›å»ºæµ‹è¯•é›†")
            return
            
        selected_test_set = st.selectbox(
            "é€‰æ‹©æµ‹è¯•é›†",
            test_set_list
        )
        
        if selected_test_set:
            test_set = load_test_set(selected_test_set)
            st.info(f"**æµ‹è¯•ç”¨ä¾‹æ•°**: {len(test_set.get('cases', []))}")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æµ‹è¯•ç»“æœ
        has_test_results = "specialized_test_results" in st.session_state
        
        # æ­¥éª¤3: è¿è¡Œæµ‹è¯•
        st.subheader("æ­¥éª¤3: è¿è¡Œæµ‹è¯•")
        
        if not has_test_results:  # ä»…åœ¨æ²¡æœ‰æµ‹è¯•ç»“æœæ—¶æ˜¾ç¤ºæµ‹è¯•æŒ‰é’®
            if st.button("â–¶ï¸ å¼€å§‹ä¸“é¡¹æµ‹è¯•", type="primary"):
                if not selected_template or not selected_model or not selected_test_set:
                    st.error("è¯·å…ˆé€‰æ‹©æç¤ºè¯æ¨¡æ¿ã€æ¨¡å‹å’Œæµ‹è¯•é›†")
                    return
                    
                # åŠ è½½æµ‹è¯•é›†
                test_set = load_test_set(selected_test_set)
                if not test_set or not test_set.get("cases"):
                    st.error(f"æ— æ³•åŠ è½½æµ‹è¯•é›† '{selected_test_set}' æˆ–æµ‹è¯•é›†ä¸ºç©º")
                    return

                # --- Progress Bar Setup ---
                total_cases = len(test_set.get("cases", []))
                total_attempts = total_cases * repeat_count
                completed_attempts = 0
                progress_bar = st.progress(0)
                status_text = st.empty()
                status_text.text(f"å‡†å¤‡å¼€å§‹... æ€»å…± {total_attempts} æ¬¡æ¨¡å‹è°ƒç”¨")

                def update_progress():
                    nonlocal completed_attempts
                    completed_attempts += 1
                    progress = completed_attempts / total_attempts if total_attempts > 0 else 0
                    progress = min(progress, 1.0)
                    progress_bar.progress(progress)
                    status_text.text(f"è¿è¡Œä¸­... å·²å®Œæˆ {completed_attempts}/{total_attempts} æ¬¡æ¨¡å‹è°ƒç”¨")
                # --- End Progress Bar Setup ---

                # å¼€å§‹æµ‹è¯•
                test_results = run_test(
                    template=template,
                    model=selected_model,
                    test_set=test_set,
                    model_provider=selected_provider,
                    repeat_count=repeat_count,
                    temperature=temperature,
                    progress_callback=update_progress # Pass callback
                )
                
                # Final progress update and status
                progress_bar.progress(1.0)
                status_text.text(f"âœ… ä¸“é¡¹æµ‹è¯•å®Œæˆ! å…±æ‰§è¡Œ {completed_attempts}/{total_attempts} æ¬¡æ¨¡å‹è°ƒç”¨ã€‚")

                if test_results:
                    # ä¿å­˜ç»“æœåˆ°ä¼šè¯çŠ¶æ€ï¼Œä»¥ä¾¿åœ¨ä¼˜åŒ–æ­¥éª¤ä¸­ä½¿ç”¨
                    st.session_state.specialized_test_results = test_results
                    st.session_state.specialized_template = template
                    st.session_state.specialized_model = selected_model
                    st.session_state.specialized_model_provider = selected_provider
                    st.session_state.specialized_test_set_name = selected_test_set
                    
                    # åˆ·æ–°é¡µé¢ä»¥æ˜¾ç¤ºç»“æœå’Œä¼˜åŒ–æŒ‰é’®
                    st.rerun()
                else:
                    st.error("ä¸“é¡¹æµ‹è¯•æœªèƒ½æˆåŠŸè·å–ç»“æœï¼Œè¯·æ£€æŸ¥é…ç½®å’ŒAPIå¯†é’¥ã€‚")
                    # Clear potentially empty state if needed
                    if "specialized_test_results" in st.session_state:
                        del st.session_state.specialized_test_results
        
        # å¦‚æœå·²æœ‰æµ‹è¯•ç»“æœï¼Œæ˜¾ç¤ºç»“æœå’Œä¼˜åŒ–æŒ‰é’®
        if has_test_results:
            # é‡æ–°è·å–ä¼šè¯çŠ¶æ€ä¸­çš„æ•°æ®
            test_results = st.session_state.specialized_test_results
            template = st.session_state.specialized_template
            selected_model = st.session_state.specialized_model
            selected_provider = st.session_state.specialized_model_provider
            
            # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
            display_test_summary(test_results, template, selected_model)
            
            # æ˜¾ç¤ºè¯¦ç»†æµ‹è¯•ç»“æœ
            st.subheader("è¯¦ç»†æµ‹è¯•ç»“æœ")
            
            for i, case in enumerate(test_results.get("test_cases", [])):
                with st.expander(f"æµ‹è¯•ç”¨ä¾‹ {i+1}: {case.get('case_description', case.get('case_id', ''))}"):
                    display_test_case_details(case, inside_expander=True)
            
            # æ·»åŠ æ¸…é™¤ç»“æœæŒ‰é’®
            if st.button("ğŸ—‘ï¸ æ¸…é™¤æµ‹è¯•ç»“æœ", key="clear_results"):
                # æ¸…é™¤ä¼šè¯çŠ¶æ€ä¸­çš„æµ‹è¯•ç»“æœ
                if "specialized_test_results" in st.session_state:
                    del st.session_state.specialized_test_results
                if "specialized_template" in st.session_state:
                    del st.session_state.specialized_template
                if "specialized_model" in st.session_state:
                    del st.session_state.specialized_model
                if "specialized_model_provider" in st.session_state:
                    del st.session_state.specialized_model_provider
                if "specialized_test_set_name" in st.session_state:
                    del st.session_state.specialized_test_set_name
                if "optimized_prompts" in st.session_state:
                    del st.session_state.optimized_prompts
                
                # åˆ·æ–°é¡µé¢
                st.rerun()
            
            # æ­¥éª¤4: ç”Ÿæˆä¼˜åŒ–æç¤ºè¯
            st.subheader("æ­¥éª¤4: ç”Ÿæˆä¼˜åŒ–æç¤ºè¯")
            
            # æ·»åŠ è‡ªåŠ¨æ‰¹é‡è¯„ä¼°é€‰é¡¹
            auto_evaluate = st.checkbox(
                "ç”Ÿæˆä¼˜åŒ–æç¤ºè¯åè‡ªåŠ¨è¿›è¡Œæ‰¹é‡è¯„ä¼°", 
                value=False,
                help="é€‰ä¸­æ­¤é€‰é¡¹å°†åœ¨ç”Ÿæˆä¼˜åŒ–æç¤ºè¯åè‡ªåŠ¨è¿›è¡Œæ‰¹é‡è¯„ä¼°"
            )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¼˜åŒ–ç»“æœ
            has_optimization_results = "optimized_prompts" in st.session_state

            optimization_strategy = st.selectbox(
                "é€‰æ‹©ä¼˜åŒ–ç­–ç•¥",
                ["balanced", "accuracy", "completeness", "conciseness"],
                format_func=lambda x: {
                    "balanced": "å¹³è¡¡ä¼˜åŒ– (å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œç®€æ´æ€§)",
                    "accuracy": "ä¼˜åŒ–å‡†ç¡®æ€§",
                    "completeness": "ä¼˜åŒ–å®Œæ•´æ€§",
                    "conciseness": "ä¼˜åŒ–ç®€æ´æ€§"
                }.get(x, x)
            )
            
            # åªæœ‰åœ¨æ²¡æœ‰ä¼˜åŒ–ç»“æœæ—¶æ˜¾ç¤ºä¼˜åŒ–æŒ‰é’®
            if not has_optimization_results:
                if st.button("ğŸ”„ ç”Ÿæˆä¼˜åŒ–æç¤ºè¯", key="optimize_button", type="primary"):
                    generate_optimized_prompts(
                        test_results, 
                        template, 
                        selected_model, 
                        optimization_strategy,
                        auto_evaluate=auto_evaluate,
                        model_provider=selected_provider
                    )
            
            # å¦‚æœå·²æœ‰ä¼˜åŒ–ç»“æœï¼Œæ˜¾ç¤ºç»“æœ
            if has_optimization_results:
                display_optimized_prompts(
                    st.session_state.optimized_prompts, 
                    template, 
                    selected_model, 
                    selected_provider
                )
                
                # æ·»åŠ é‡æ–°ä¼˜åŒ–æŒ‰é’®
                if st.button("ğŸ”„ é‡æ–°ç”Ÿæˆä¼˜åŒ–æç¤ºè¯", key="regenerate"):
                    generate_optimized_prompts(
                        test_results, 
                        template, 
                        selected_model, 
                        optimization_strategy,
                        auto_evaluate=auto_evaluate,
                        model_provider=selected_provider
                    )

    with tab2:
        render_iterative_optimization()

def render_iterative_optimization():
    st.title("ğŸ” è‡ªåŠ¨å¤šè½®æç¤ºè¯è¿­ä»£ä¼˜åŒ–")
    st.markdown("""
    æœ¬åŠŸèƒ½æ”¯æŒè‡ªåŠ¨å¤šè½®æç¤ºè¯ä¼˜åŒ–ä¸è¯„ä¼°ï¼Œè‡ªåŠ¨é€‰å‡ºæœ€ä¼˜ç‰ˆæœ¬ã€‚
    """)
    # é€‰æ‹©æ¨¡æ¿ã€æ¨¡å‹ã€æµ‹è¯•é›†
    template_list = get_all_template_names_sorted()
    if not template_list:
        st.warning("æœªæ‰¾åˆ°ä»»ä½•æç¤ºè¯æ¨¡æ¿ï¼ˆåŒ…æ‹¬ç³»ç»Ÿæ¨¡æ¿ï¼‰ï¼Œè¯·å…ˆåˆ›å»ºæ¨¡æ¿")
        return
    selected_template = st.selectbox("é€‰æ‹©æç¤ºè¯æ¨¡æ¿ï¼ˆåŒ…å«ç³»ç»Ÿæ¨¡æ¿ï¼‰", template_list, key="iter_template")
    template = load_template(selected_template) if selected_template else None
    available_models = get_available_models()
    all_models = [(provider, model) for provider, models in available_models.items() for model in models]
    model_options = [f"{model} ({provider})" for provider, model in all_models]
    model_map = {f"{model} ({provider})": (model, provider) for provider, model in all_models}
    selected_model_option = st.selectbox("é€‰æ‹©æ¨¡å‹", model_options, key="iter_model")
    selected_model, selected_provider = model_map[selected_model_option] if selected_model_option else (None, None)

    # æ­¥éª¤2: é€‰æ‹©æµ‹è¯•é›†
    st.subheader("æ­¥éª¤2: é€‰æ‹©æµ‹è¯•é›†")
    test_set_mode = st.radio(
        "æµ‹è¯•é›†æ¥æº",
        ["é€‰æ‹©å·²æœ‰æµ‹è¯•é›†", "AIè‡ªåŠ¨ç”Ÿæˆæ–°æµ‹è¯•é›†"],
        horizontal=True,
        key="iter_testset_mode"
    )
    test_set = []
    test_set_name = None
    
    if test_set_mode == "é€‰æ‹©å·²æœ‰æµ‹è¯•é›†":
        test_set_list = get_test_set_list()
        if not test_set_list:
            st.warning("æœªæ‰¾åˆ°æµ‹è¯•é›†ï¼Œè¯·å…ˆåˆ›å»ºæµ‹è¯•é›†")
            return
        selected_test_set = st.selectbox("é€‰æ‹©æµ‹è¯•é›†", test_set_list, key="iter_testset")
        if selected_test_set:
            loaded_test_set = load_test_set(selected_test_set)
            test_set = loaded_test_set  # ä¿æŒä¸ºdictï¼ŒåŒ…å«å…¨å±€å˜é‡
            # ä»…ç”¨äºæ˜¾ç¤ºç”¨ä¾‹æ•°æ—¶è¿‡æ»¤
            valid_cases = [case for case in test_set.get("cases", []) if case.get("user_input") and case.get("expected_output") and case.get("evaluation_criteria")]
            test_set_name = selected_test_set
            st.info(f"**æµ‹è¯•ç”¨ä¾‹æ•°**: {len(valid_cases)}")
    else:
        # AIè‡ªåŠ¨ç”Ÿæˆæ–°æµ‹è¯•é›†
        test_set_name = st.text_input("æ–°æµ‹è¯•é›†åç§°", value=f"AIç”Ÿæˆæµ‹è¯•é›†_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        test_set_desc = st.text_input("æ–°æµ‹è¯•é›†æè¿°", value="è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•é›†")
        gen_case_count = st.number_input("ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ•°é‡", min_value=3, max_value=1000, value=6, step=1)
        
        # æ˜¾ç¤ºæç¤ºä¿¡æ¯
        st.info("å°†æ ¹æ®å½“å‰æç¤ºè¯æ¨¡æ¿å’Œæ¨¡å‹ï¼Œè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹")
        
        # å¯ä»¥æ·»åŠ è‡ªå®šä¹‰æµ‹è¯•æ–¹å‘çš„é€‰é¡¹
        test_directions = st.text_area(
            "æµ‹è¯•æ–¹å‘ï¼ˆå¯é€‰ï¼‰", 
            placeholder="è¾“å…¥ç‰¹å®šæµ‹è¯•æ–¹å‘ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¾‹å¦‚ï¼š\nè¯­ä¹‰ç†è§£èƒ½åŠ›æµ‹è¯•\nè¾¹ç•Œæ¡ä»¶å¤„ç†æµ‹è¯•\nå¤šè¯­è¨€å¤„ç†èƒ½åŠ›æµ‹è¯•",
            help="æŒ‡å®šç‰¹å®šçš„æµ‹è¯•æ–¹å‘ï¼Œç³»ç»Ÿä¼šé’ˆå¯¹è¿™äº›æ–¹å‘ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"
        )

    # è¿­ä»£å‚æ•°
    st.subheader("æ­¥éª¤3: è®¾ç½®ä¼˜åŒ–å‚æ•°")
    max_iterations = st.slider("è¿­ä»£æ¬¡æ•°", 1, 100, 5)
    optimization_strategy = st.selectbox(
        "ä¼˜åŒ–ç­–ç•¥",
        ["balanced", "accuracy", "completeness", "conciseness"],
        format_func=lambda x: {
            "balanced": "å¹³è¡¡ä¼˜åŒ– (å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œç®€æ´æ€§)",
            "accuracy": "ä¼˜åŒ–å‡†ç¡®æ€§",
            "completeness": "ä¼˜åŒ–å®Œæ•´æ€§",
            "conciseness": "ä¼˜åŒ–ç®€æ´æ€§"
        }.get(x, x),
        key="iter_strategy"
    )
    optimization_retries = st.number_input("ä¼˜åŒ–å¤±è´¥é‡è¯•æ¬¡æ•°", min_value=0, max_value=10, value=3, step=1, key="iter_optimization_retries")
    
    # å¼€å§‹ä¼˜åŒ–æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹è‡ªåŠ¨è¿­ä»£ä¼˜åŒ–", type="primary"):
        # æ£€æŸ¥å¿…è¦å‚æ•°æ˜¯å¦å·²é€‰æ‹©
        if not template or not selected_model:
            st.error("è¯·å…ˆé€‰æ‹©æç¤ºè¯æ¨¡æ¿å’Œæ¨¡å‹")
            return
            
        # Progress Bar Setup for Test Set Generation and Iterations
        progress_container = st.container()
        with progress_container:
            generation_progress_bar = st.progress(0)
            generation_status_text = st.empty()
            # ä¿®å¤è¿™é‡Œçš„æ¶ˆæ¯æç¤ºï¼Œæ ¹æ®æµ‹è¯•é›†æ¥æºæ˜¾ç¤ºä¸åŒçš„å‡†å¤‡ä¿¡æ¯
            if test_set_mode == "é€‰æ‹©å·²æœ‰æµ‹è¯•é›†":
                generation_status_text.text("å‡†å¤‡ä½¿ç”¨å·²é€‰æ‹©çš„æµ‹è¯•é›†...")
            else:
                generation_status_text.text("å‡†å¤‡ç”Ÿæˆæµ‹è¯•é›†...")
        
        test_cases_for_optimization = None
        
        # å¦‚æœæ˜¯AIè‡ªåŠ¨ç”Ÿæˆæµ‹è¯•é›†æ¨¡å¼ï¼Œå…ˆç”Ÿæˆæµ‹è¯•é›†
        if test_set_mode == "AIè‡ªåŠ¨ç”Ÿæˆæ–°æµ‹è¯•é›†":
            with st.spinner("AIæ­£åœ¨ä¸ºæ‚¨ç”Ÿæˆæµ‹è¯•é›†..."):
                evaluator = PromptEvaluator()
                
                # å¤„ç†æµ‹è¯•æ–¹å‘
                test_purposes = []
                if test_directions:
                    # åˆ†å‰²å¤šè¡Œçš„æµ‹è¯•æ–¹å‘
                    directions = [d.strip() for d in test_directions.strip().split("\n") if d.strip()]
                    if directions:
                        # å¦‚æœæœ‰æŒ‡å®šæµ‹è¯•æ–¹å‘ï¼ŒæŒ‰æ–¹å‘ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
                        generation_status_text.text(f"æ­£åœ¨ç”Ÿæˆå¤šæ–¹å‘æµ‹è¯•é›†ï¼Œå…±{len(directions)}ä¸ªæ–¹å‘...")
                        
                        # æ¯ä¸ªæ–¹å‘ç”Ÿæˆå¯¹åº”æ•°é‡çš„æµ‹è¯•ç”¨ä¾‹
                        cases_per_direction = max(1, gen_case_count // len(directions))
                        
                        # åˆ›å»ºåŸºæœ¬çš„ç¤ºä¾‹æµ‹è¯•ç”¨ä¾‹
                        example_case = {
                            "id": "example_case",
                            "description": f"{template.get('name', 'æç¤ºè¯')}æµ‹è¯•ç”¨ä¾‹",
                            "user_input": "è¯·ç»™æˆ‘è®²è§£ä¸€ä¸‹è¿™ä¸ªæç¤ºè¯çš„ç”¨é€”",
                            "expected_output": f"è¿™ä¸ªæç¤ºè¯ç”¨äº{template.get('description', 'ç‰¹å®šä»»åŠ¡å¤„ç†')}ï¼Œé€šè¿‡ç²¾ç¡®çš„æŒ‡ä»¤å¼•å¯¼æ¨¡å‹è¾“å‡ºé«˜è´¨é‡ç»“æœã€‚",
                            "evaluation_criteria": {
                                "accuracy": "è¯„ä¼°å›ç­”çš„å‡†ç¡®æ€§",
                                "completeness": "è¯„ä¼°å›ç­”çš„å®Œæ•´æ€§",
                                "relevance": "è¯„ä¼°å›ç­”çš„ç›¸å…³æ€§",
                                "clarity": "è¯„ä¼°å›ç­”çš„æ¸…æ™°åº¦"
                            }
                        }
                        
                        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
                        def update_generation_progress(current, total):
                            progress = min(current / total, 1.0) if total > 0 else 0
                            generation_progress_bar.progress(progress)
                            generation_status_text.text(f"æ­£åœ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹... å·²å®Œæˆ: {current}/{total}")
                        
                        # æ‰¹é‡ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œä¼ å…¥è¿›åº¦å›è°ƒå‡½æ•°
                        batch_result = evaluator.generate_test_cases_batch(
                            model=selected_model,
                            test_purposes=directions,
                            example_case=example_case,
                            target_count_per_purpose=cases_per_direction,
                            progress_callback=update_generation_progress
                        )
                        
                        if "error" in batch_result:
                            st.error(f"ç”Ÿæˆæµ‹è¯•é›†å¤±è´¥: {batch_result['error']}")
                            return
                            
                        if "errors" in batch_result and batch_result["errors"]:
                            for error in batch_result["errors"]:
                                st.warning(error)
                                
                        # ä¿®æ­£ï¼šç»„è£…ä¸ºdictç»“æ„ï¼Œä¾¿äºåç»­ä¼ é€’
                        test_cases_for_optimization = {
                            "cases": batch_result.get("test_cases", []),
                            "name": test_set_name,
                            "description": test_set_desc,
                        }
                else:
                    # ç›´æ¥ç”Ÿæˆå®Œæ•´æµ‹è¯•é›†
                    generation_status_text.text("æ­£åœ¨ç”Ÿæˆé€šç”¨æµ‹è¯•é›†...")
                    
                    # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
                    def update_generation_progress(current, total):
                        progress = min(current / total, 1.0) if total > 0 else 0
                        generation_progress_bar.progress(progress)
                        generation_status_text.text(f"æ­£åœ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹... å·²å®Œæˆ: {current}/{total}")
                    
                    result = evaluator.generate_complete_test_set(
                        name=test_set_name,
                        description=test_set_desc,
                        model=selected_model,
                        count=gen_case_count
                    )
                    
                    if "error" in result:
                        st.error(f"ç”Ÿæˆæµ‹è¯•é›†å¤±è´¥: {result['error']}")
                        return
                        
                    generated_test_set = result.get("test_set", {})
                    test_cases_for_optimization = generated_test_set  # ä¿æŒä¸ºdictç»“æ„
                    
                    # ä¿å­˜ç”Ÿæˆçš„æµ‹è¯•é›†
                    from config import save_test_set
                    save_test_set(test_set_name, generated_test_set)
                    
                # æ£€æŸ¥ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ•°é‡
                if not test_cases_for_optimization or not test_cases_for_optimization.get("cases"):
                    st.error("æœªèƒ½ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œè¯·å°è¯•å…¶ä»–å‚æ•°æˆ–ä½¿ç”¨å·²æœ‰æµ‹è¯•é›†")
                    return
                    
                # æ›´æ–°ç”Ÿæˆè¿›åº¦
                generation_progress_bar.progress(1.0)
                generation_status_text.success(f"âœ… æˆåŠŸç”Ÿæˆ {len(test_cases_for_optimization['cases'])} ä¸ªæµ‹è¯•ç”¨ä¾‹")
                
                # å±•ç¤ºç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹
                with st.expander("æŸ¥çœ‹ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹"):
                    for i, case in enumerate(test_cases_for_optimization["cases"]):
                        st.write(f"**æµ‹è¯•ç”¨ä¾‹ {i+1}**: {case.get('description', '')}")
                        st.write(f"- **ç”¨æˆ·è¾“å…¥**: {case.get('user_input', '')}")
                        st.write(f"- **æœŸæœ›è¾“å‡º**: {case.get('expected_output', '')}")
                        st.write("---")
        else:
            # ä½¿ç”¨å·²æœ‰æµ‹è¯•é›†ï¼Œç›´æ¥èµ‹å€¼ä¸ºdictç»“æ„
            # å¹¶è¿‡æ»¤æ— æ•ˆç”¨ä¾‹
            filtered_cases = [case for case in test_set.get("cases", []) if case.get("user_input") and case.get("expected_output") and case.get("evaluation_criteria")]
            test_cases_for_optimization = dict(test_set)
            test_cases_for_optimization["cases"] = filtered_cases

        # æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•ç”¨ä¾‹ç”¨äºä¼˜åŒ–
        if not test_cases_for_optimization or not test_cases_for_optimization.get("cases"):
            st.error("æœªèƒ½è·å–æˆ–ç”Ÿæˆæœ‰æ•ˆçš„æµ‹è¯•ç”¨ä¾‹ï¼Œæ— æ³•å¼€å§‹ä¼˜åŒ–ã€‚è¯·ç¡®ä¿æµ‹è¯•é›†ä¸­çš„æ¯ä¸ªç”¨ä¾‹éƒ½åŒ…å« user_inputã€expected_output å’Œ evaluation_criteriaã€‚")
            return

        # è®¾ç½®åŒè¿›åº¦æ¡
        iteration_progress_bar = st.progress(0)
        iteration_status_text = st.empty()
        inner_progress_bar = st.progress(0)
        inner_status_text = st.empty()

        # è¿›åº¦å›è°ƒï¼Œæ”¯æŒä¸¤å±‚è¿›åº¦
        def iteration_progress_callback(iteration, total_iterations, inner_idx, inner_total, global_idx, global_total, stage=None, data=None):
            # ä»dataä¸­è·å–æ›´è¯¦ç»†çš„è¿›åº¦å’Œåˆ†æ•°ä¿¡æ¯
            avg_score = data.get('avg_score') if data else None 
            child_current = data.get('child_current') if data else inner_idx
            child_total = data.get('child_total') if data else inner_total
            
            # --- BEGIN MODIFICATION for effective_stage_name ---
            effective_stage_name = None
            child_data_from_parent = data.get('child_data', {}) if data else {}

            # Priority 1: 'stage_name' from child's 'complete(data_to_add=...)'
            effective_stage_name = child_data_from_parent.get('stage_name')
            
            # Update avg_score if it's more specifically available in child_data_from_parent
            if child_data_from_parent.get('avg_score') is not None:
                avg_score = child_data_from_parent.get('avg_score')

            if not effective_stage_name and data:
                # Priority 2: 'description' from ProgressTracker (passed as 'stage' in callback)
                effective_stage_name = data.get('description') # 'description' from ProgressTracker
            
            if not effective_stage_name:
                # Priority 3: 'stage' directly from callback args (less common now with ProgressTracker)
                effective_stage_name = stage

            # --- END MODIFICATION for effective_stage_name ---

            # æ›´æ–°æ€»è¿›åº¦æ¡
            iter_progress = min(global_idx / global_total, 1.0) if global_total > 0 else 0
            iteration_progress_bar.progress(iter_progress)
            
            iteration_status = f"è¿­ä»£ä¼˜åŒ–è¿›è¡Œä¸­... ç¬¬ {iteration}/{total_iterations} è½® (æ€»è¿›åº¦: {iter_progress:.2%})"
            if avg_score is not None and effective_stage_name and ('eval_done' in effective_stage_name or 'opt_eval_done' in effective_stage_name):
                iteration_status += f"ï¼Œé˜¶æ®µ '{effective_stage_name}' å¹³å‡åˆ†: {avg_score:.2f}"
            elif avg_score is not None: 
                 iteration_status += f"ï¼Œå½“å‰å¹³å‡åˆ†: {avg_score:.2f}"
            iteration_status_text.text(iteration_status)
            
            # å†…å±‚è¿›åº¦ï¼ˆå…·ä½“é˜¶æ®µçš„è¿›åº¦ï¼‰
            current_stage_progress = min(child_current / child_total, 1.0) if child_total > 0 else 0
            inner_progress_bar.progress(current_stage_progress)

            stage_text_map = {
                "gen": f"ç”Ÿæˆå“åº”: {child_current}/{child_total}",
                "eval": f"è¯„ä¼°å“åº”: {child_current}/{child_total}",
                "opt_gen": f"ç”Ÿæˆä¼˜åŒ–ç‰ˆæœ¬å“åº”: {child_current}/{child_total}",
                "opt_eval": f"è¯„ä¼°ä¼˜åŒ–ç‰ˆæœ¬: {child_current}/{child_total}",
                "eval_done": f"è¯„ä¼°å®Œæˆ! å¹³å‡åˆ†: {avg_score:.2f}" if avg_score is not None else "è¯„ä¼°å®Œæˆ!",
                "opt_eval_done": f"ä¼˜åŒ–ç‰ˆæœ¬è¯„ä¼°å®Œæˆ! å¹³å‡åˆ†: {avg_score:.2f}" if avg_score is not None else "ä¼˜åŒ–ç‰ˆæœ¬è¯„ä¼°å®Œæˆ!"
            }
            
            display_stage_key = effective_stage_name 
            if effective_stage_name: 
                if effective_stage_name.startswith("gen_") and len(effective_stage_name.split('_')) > 1 : display_stage_key = "gen"
                elif effective_stage_name.startswith("eval_") and not "done" in effective_stage_name and len(effective_stage_name.split('_')) > 1: display_stage_key = "eval"
                elif effective_stage_name.startswith("opt_gen_") and len(effective_stage_name.split('_')) > 2 : display_stage_key = "opt_gen"
                elif effective_stage_name.startswith("opt_eval_") and not "done" in effective_stage_name and len(effective_stage_name.split('_')) > 2: display_stage_key = "opt_eval"
                elif "eval_done" in effective_stage_name: display_stage_key = "eval_done"
                elif "opt_eval_done" in effective_stage_name: display_stage_key = "opt_eval_done"

            status_message = stage_text_map.get(display_stage_key, f"å¤„ç†ä¸­: {effective_stage_name} ({child_current}/{child_total})")
            
            if display_stage_key and "done" in display_stage_key:
                inner_status_text.success(status_message)
            else:
                inner_status_text.text(status_message)

        try:
            st.info(f"å³å°†å¼€å§‹è¿­ä»£ä¼˜åŒ–ï¼Œè®¡åˆ’è¿›è¡Œ {max_iterations} è½®è¿­ä»£...")
            
            # æ·»åŠ æ—¶é—´æˆ³è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # æ‰§è¡Œè¿­ä»£ä¼˜åŒ–
            optimizer = PromptOptimizer(optimization_retries=optimization_retries)
            result = optimizer.iterative_prompt_optimization_sync(
                initial_prompt=template,
                test_set_dict=test_cases_for_optimization, 
                evaluator=PromptEvaluator(),
                optimization_strategy=optimization_strategy,
                model=selected_model,
                provider=selected_provider,
                max_iterations=max_iterations,
                progress_callback=iteration_progress_callback
            )
            
            # è®°å½•å®Œæˆæ—¶é—´å’Œæ€»è€—æ—¶
            end_time = time.time()
            total_time = end_time - start_time
            st.success(f"è¿­ä»£ä¼˜åŒ–å®Œæˆï¼æ€»è€—æ—¶: {total_time:.1f} ç§’")
            
            # æ£€æŸ¥ç»“æœ
            if "error" in result:
                st.error(f"è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹å‡ºé”™: {result.get('error')}")
                if result.get("history"):
                    st.info("å°½ç®¡å‡ºç°é”™è¯¯ï¼Œä»èƒ½å±•ç¤ºéƒ¨åˆ†ç»“æœ")
                else:
                    return
                
            # è·å–å†å²è®°å½•
            history = result.get("history", [])
            if not history:
                st.warning("æœªèƒ½è·å–è¿­ä»£ä¼˜åŒ–å†å²è®°å½•")
                return
                
            # è®°å½•æ¯è½®çš„æ•°æ®é‡
            history_stats = {}
            for item in history:
                iteration = item.get('iteration', 0)
                stage = item.get('stage', 'unknown')
                if iteration not in history_stats:
                    history_stats[iteration] = {'initial': 0, 'optimized': 0}
                if stage == 'initial':
                    history_stats[iteration]['initial'] += 1
                elif stage == 'optimized':
                    history_stats[iteration]['optimized'] += 1
            # ä¿®æ­£ï¼šç¡®ä¿æœ€åä¸€è½®ä¹Ÿç»Ÿè®¡ï¼ˆå³ä½¿ä¼˜åŒ–ç‰ˆæœ¬ä¸º0ï¼‰
            for i in range(1, max_iterations + 1):
                if i not in history_stats:
                    history_stats[i] = {'initial': 0, 'optimized': 0}
            st.write("è¿­ä»£å†å²æ•°æ®ç»Ÿè®¡:")
            for iter_num in range(1, max_iterations + 1):
                stats = history_stats.get(iter_num, {'initial': 0, 'optimized': 0})
                st.write(f"- ç¬¬ {iter_num} è½®: åˆå§‹æç¤ºè¯ {stats['initial']} ä¸ª, ä¼˜åŒ–ç‰ˆæœ¬ {stats['optimized']} ä¸ª")
            
            # æ›´æ–°æœ€ç»ˆè¿›åº¦
            iteration_progress_bar.progress(1.0)
            iteration_status_text.success("âœ… è‡ªåŠ¨è¿­ä»£ä¼˜åŒ–å®Œæˆï¼")
            inner_progress_bar.progress(1.0)
            inner_status_text.success("å…¨éƒ¨è¯„ä¼°ä»»åŠ¡å·²å®Œæˆï¼")
            
            # æŒ‰è¿­ä»£è½®æ¬¡é‡ç»„ç»“æœï¼Œä¾¿äºå±•ç¤º
            iteration_results = {}
            for item in history:
                iteration = item.get('iteration', 0)
                if iteration not in iteration_results:
                    iteration_results[iteration] = {
                        'initial': None,
                        'optimized': []
                    }
                
                stage = item.get('stage', '')
                if stage == 'initial':
                    iteration_results[iteration]['initial'] = item
                elif stage == 'optimized':
                    iteration_results[iteration]['optimized'].append(item)
                    
            # è¾“å‡ºä¸€äº›è°ƒè¯•ä¿¡æ¯
            st.write(f"å…±å®Œæˆ {len(iteration_results)} è½®è¿­ä»£")
            
            # å±•ç¤ºæ¯è½®ç»“æœ
            for iter_num in sorted(iteration_results.keys()):
                iter_data = iteration_results[iter_num]
                
                with st.expander(f"ç¬¬ {iter_num} è½®è¿­ä»£", expanded=True):
                    # æ˜¾ç¤ºæœ¬è½®åˆå§‹æç¤ºè¯
                    if iter_data['initial']:
                        st.subheader(f"å½“å‰æç¤ºè¯ (å¹³å‡åˆ†: {iter_data['initial'].get('avg_score', 0):.2f})")
                        prompt_str = iter_data['initial'].get('prompt_str')
                        prompt_obj = iter_data['initial'].get('prompt_obj')
                        prompt = iter_data['initial'].get('prompt')
                        if prompt_str:
                            st.code(prompt_str, language="markdown")
                        elif prompt:
                            st.code(prompt, language="markdown")
                        elif prompt_obj:
                            st.code(json.dumps(prompt_obj, ensure_ascii=False, indent=2), language="json")
                        else:
                            st.warning("æœªæ‰¾åˆ°æœ¬è½®åˆå§‹æç¤ºè¯å†…å®¹")
                    else:
                        st.info(f"æœªæ‰¾åˆ°ç¬¬ {iter_num} è½®çš„åˆå§‹æç¤ºè¯ä¿¡æ¯")
                    
                    # å¦‚æœæ˜¯æœ€åä¸€è½®è¿­ä»£ï¼Œä¸ä¼šæœ‰ä¼˜åŒ–ç‰ˆæœ¬
                    if not iter_data['optimized'] and iter_num == max_iterations:
                        st.info("æœ€åä¸€è½®è¿­ä»£ï¼Œæ— éœ€ç”Ÿæˆä¼˜åŒ–ç‰ˆæœ¬")
                        continue
                    
                    # æ˜¾ç¤ºæœ¬è½®ç”Ÿæˆçš„ä¼˜åŒ–ç‰ˆæœ¬
                    if iter_data['optimized']:
                        st.subheader(f"æœ¬è½®ç”Ÿæˆçš„ä¼˜åŒ–ç‰ˆæœ¬ ({len(iter_data['optimized'])} ä¸ª)")
                        
                        # åªæ ‡è®°ä¸€ä¸ªæœ€ä½³ç‰ˆæœ¬
                        best_version = None
                        for version in iter_data['optimized']:
                            if version.get('is_best', False):
                                best_version = version
                                break
                        
                        if not best_version and iter_data['optimized']:
                            best_version = max(iter_data['optimized'], key=lambda x: x.get('avg_score', 0))
                        
                        best_version_id = id(best_version) if best_version else None
                        
                        # å±•ç¤ºæ¯ä¸ªä¼˜åŒ–ç‰ˆæœ¬ - ä¸å†ä½¿ç”¨åµŒå¥—expanderï¼Œè€Œæ˜¯ä½¿ç”¨å®¹å™¨å’Œåˆ†éš”çº¿
                        for idx, version in enumerate(iter_data['optimized']):
                            # åªæ ‡è®°ä¸€ä¸ªæœ€ä½³
                            is_best = (id(version) == best_version_id)
                            version_label = f"ç‰ˆæœ¬ {version.get('version', '?')}"
                            
                            if is_best:
                                version_label += " âœ… (é€‰ä¸ºä¸‹ä¸€è½®çš„æç¤ºè¯)"
                            
                            version_score = version.get('avg_score', 0)
                            version_strategy = version.get('strategy', 'æœªçŸ¥ç­–ç•¥')
                            
                            # ä½¿ç”¨å®¹å™¨æ›¿ä»£åµŒå¥—çš„expander
                            version_container = st.container()
                            with version_container:
                                st.markdown(f"### {version_label} - {version_strategy} (å¹³å‡åˆ†: {version_score:.2f})")
                                st.markdown(f"**ä¼˜åŒ–ç­–ç•¥**: {version_strategy}")
                                v_prompt_str = version.get('prompt_str')
                                v_prompt = version.get('prompt')
                                v_prompt_obj = version.get('prompt_obj')
                                if v_prompt_str:
                                    st.code(v_prompt_str, language="markdown")
                                elif v_prompt:
                                    st.code(v_prompt, language="markdown")
                                elif v_prompt_obj:
                                    st.code(json.dumps(v_prompt_obj, ensure_ascii=False, indent=2), language="json")
                                else:
                                    st.warning("æœªæ‰¾åˆ°æ­¤ç‰ˆæœ¬çš„æç¤ºè¯å†…å®¹")
                                st.markdown("---")
                    else:
                        st.info(f"ç¬¬ {iter_num} è½®æœªç”Ÿæˆä¼˜åŒ–ç‰ˆæœ¬")
            
            # å±•ç¤ºæœ€ä¼˜ç»“æœ
            st.markdown("---")
            st.header("æœ€ç»ˆæœ€ä¼˜æç¤ºè¯")
            best_prompt_obj = result.get("best_prompt_obj", None)
            best_score = result.get("best_score", 0)
            
            if best_prompt_obj and isinstance(best_prompt_obj, dict):
                best_prompt_str = best_prompt_obj.get("template", "")
                st.code(best_prompt_str, language="markdown")
                st.markdown(f"**æœ€ä¼˜å¹³å‡åˆ†**: {best_score:.2f}")
                st.markdown("**æœ€ä¼˜æç¤ºè¯å®Œæ•´å¯¹è±¡ï¼ˆå«å˜é‡ï¼‰**:")
                st.code(json.dumps(best_prompt_obj, ensure_ascii=False, indent=2), language="json")
                # è‡ªåŠ¨ä¿å­˜æœ€ä¼˜æç¤ºè¯ä¸ºæ–°æ¨¡æ¿ï¼ˆå®Œæ•´å¯¹è±¡ï¼ŒåŒ…å«å˜é‡ï¼‰
                from utils.common import save_optimized_template
                new_name = save_optimized_template(template, best_prompt_obj, 0)
                st.session_state.iter_best_prompt = best_prompt_str
                st.session_state.iter_best_score = best_score
                st.session_state.iter_best_template_name = new_name
                st.success(f"æœ€ä¼˜æç¤ºè¯å·²è‡ªåŠ¨ä¿å­˜ä¸ºæ–°æ¨¡æ¿: {new_name}")
                # æä¾›å†æ¬¡ä¿å­˜çš„é€‰é¡¹
                if st.button("ğŸ’¾ å†æ¬¡ä¿å­˜æœ€ä¼˜æç¤ºè¯ä¸ºæ–°æ¨¡æ¿"):
                    new_name2 = save_optimized_template(template, best_prompt_obj, int(time.time())%10000)
                    st.success(f"å·²ä¿å­˜ä¸ºæ–°æ¨¡æ¿: {new_name2}")
            else:
                st.warning("æœªèƒ½è·å–æœ€ä¼˜æç¤ºè¯ç»“æœ")
        except Exception as e:
            st.error(f"è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹å‡ºé”™: {str(e)}")
            import traceback
            st.code(traceback.format_exc(), language="python")

def generate_optimized_prompts(results, template, model, optimization_strategy, auto_evaluate=False, model_provider=None):
    """
    æ ¹æ®æµ‹è¯•ç»“æœç”Ÿæˆä¼˜åŒ–æç¤ºè¯
    """
    with st.spinner("AIæ­£åœ¨åˆ†ææµ‹è¯•ç»“æœå¹¶ç”Ÿæˆä¼˜åŒ–æç¤ºè¯..."):
        # æ”¶é›†è¯„ä¼°ç»“æœ
        evaluations = []
        # éå†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        for case in results.get("test_cases", []):
            responses = case.get("responses", [])
            if responses:
                for response in responses:
                    if response.get("evaluation") and not response.get("error"):
                        evaluations.append(response["evaluation"])
            elif case.get("evaluation"):
                evaluations.append(case["evaluation"])
        if not evaluations:
            st.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä¼°ç»“æœï¼Œæ— æ³•ç”Ÿæˆä¼˜åŒ–æç¤ºè¯")
            return
        optimizer = PromptOptimizer()
        optimization_result = optimizer.optimize_prompt_sync(
            template.get("template", ""),
            evaluations,
            optimization_strategy
        )
        if "error" in optimization_result:
            st.error(f"ä¼˜åŒ–å¤±è´¥: {optimization_result['error']}")
        else:
            optimized_prompts = optimization_result.get("optimized_prompts", [])
            if not optimized_prompts:
                st.warning("æœªèƒ½ç”Ÿæˆä¼˜åŒ–æç¤ºè¯")
                return
            st.session_state.optimized_prompts = optimized_prompts
            st.success(f"æˆåŠŸç”Ÿæˆ {len(optimized_prompts)} ä¸ªä¼˜åŒ–æç¤ºè¯ç‰ˆæœ¬")
            display_optimized_prompts(optimized_prompts, template, model, model_provider)
            if auto_evaluate:
                optimized_templates = []
                for i, opt_prompt in enumerate(optimized_prompts):
                    optimized_template = dict(template)
                    optimized_template["name"] = f"{template.get('name', '')}çš„ä¼˜åŒ–ç‰ˆæœ¬_{i+1}"
                    optimized_template["description"] = f"ä¼˜åŒ–ç­–ç•¥: {opt_prompt.get('strategy', '')}"
                    optimized_template["template"] = opt_prompt.get("prompt", "")
                    optimized_templates.append(optimized_template)
                st.session_state.batch_ab_test_original = template
                st.session_state.batch_ab_test_optimized = optimized_templates
                st.session_state.batch_ab_test_model = model
                st.session_state.batch_ab_test_model_provider = model_provider
                st.session_state.batch_ab_test_test_set = st.session_state.specialized_test_set_name
                st.session_state.page = "prompt_batch_ab_test"
                st.rerun()

def display_optimized_prompts(optimized_prompts, template, model, model_provider):
    """æ˜¾ç¤ºä¼˜åŒ–æç¤ºè¯ç»“æœ"""
    if not optimized_prompts:
        st.warning("æ²¡æœ‰ä¼˜åŒ–æç¤ºè¯å¯æ˜¾ç¤º")
        return
    
    # åªæœ‰åœ¨æœªé€‰æ‹©è‡ªåŠ¨è¯„ä¼°æ—¶æ‰æ˜¾ç¤ºæ‰¹é‡è¯„ä¼°æŒ‰é’®
    if st.button("ğŸ”¬ æ‰¹é‡è¯„ä¼°æ‰€æœ‰ä¼˜åŒ–ç‰ˆæœ¬", type="primary"):
        # åˆ›å»ºä¼˜åŒ–åçš„æ¨¡æ¿åˆ—è¡¨
        optimized_templates = []
        for i, opt_prompt in enumerate(optimized_prompts):
            optimized_template = dict(template)
            optimized_template["name"] = f"{template.get('name', '')}çš„ä¼˜åŒ–ç‰ˆæœ¬_{i+1}"
            optimized_template["description"] = f"ä¼˜åŒ–ç­–ç•¥: {opt_prompt.get('strategy', '')}"
            optimized_template["template"] = opt_prompt.get("prompt", "")
            optimized_templates.append(optimized_template)
        
        # ä¿å­˜æ‰¹é‡A/Bæµ‹è¯•æ‰€éœ€æ•°æ®åˆ°ä¼šè¯çŠ¶æ€
        st.session_state.batch_ab_test_original = template
        st.session_state.batch_ab_test_optimized = optimized_templates
        st.session_state.batch_ab_test_model = model
        st.session_state.batch_ab_test_model_provider = model_provider
        st.session_state.batch_ab_test_test_set = st.session_state.specialized_test_set_name
        
        # è·³è½¬åˆ°æ‰¹é‡A/Bæµ‹è¯•é¡µé¢
        st.session_state.page = "prompt_batch_ab_test"
        st.rerun()
    
    # æ˜¾ç¤ºå„ä¸ªä¼˜åŒ–æç¤ºè¯ç‰ˆæœ¬
    for i, opt_prompt in enumerate(optimized_prompts):
        with st.expander(f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}: {opt_prompt.get('strategy', 'æœªçŸ¥ç­–ç•¥')}"):
            # ä½¿ç”¨æ›´æ¸…æ™°çš„è§†è§‰åˆ†éš”
            st.divider()
            
            # ä¼˜åŒ–ç­–ç•¥éƒ¨åˆ†
            st.markdown("#### ä¼˜åŒ–ç­–ç•¥")
            st.write(opt_prompt.get("strategy", ""))
            
            # æ˜¾ç¤ºé’ˆå¯¹è§£å†³çš„é—®é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
            if "problem_addressed" in opt_prompt:
                st.markdown("#### é’ˆå¯¹è§£å†³çš„é—®é¢˜")
                st.info(opt_prompt.get("problem_addressed", ""))
            
            # é¢„æœŸæ”¹è¿›
            st.markdown("#### é¢„æœŸæ”¹è¿›")
            st.write(opt_prompt.get("expected_improvements", ""))
            
            # ä¼˜åŒ–ç†ç”±ï¼ˆå¦‚æœæœ‰ï¼‰
            if "reasoning" in opt_prompt:
                st.markdown("#### ä¼˜åŒ–ç†ç”±")
                st.info(opt_prompt.get("reasoning", ""))
            
            st.divider()
            
            # æ˜¾ç¤ºä¼˜åŒ–åçš„æç¤ºè¯
            st.markdown("#### ä¼˜åŒ–åçš„æç¤ºè¯")
            st.code(opt_prompt.get("prompt", ""), language="markdown")
            
            st.divider()
            
            # åˆ›å»ºæŒ‰é’®ï¼Œå°†ä¼˜åŒ–åçš„æç¤ºè¯ä¿å­˜ä¸ºæ–°æ¨¡æ¿æˆ–è¿è¡ŒA/Bæµ‹è¯•
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button(f"ğŸ’¾ ä¿å­˜ä¸ºæ–°æ¨¡æ¿", key=f"save_opt_{i}"):
                    new_name = save_optimized_template(template, opt_prompt, i)
                    st.success(f"å·²ä¿å­˜ä¸ºæ–°æ¨¡æ¿: {new_name}")
            
            with col2:
                if st.button(f"ğŸ” A/Bæµ‹è¯•", key=f"test_opt_{i}"):
                    # åˆ›å»ºä¼˜åŒ–åçš„æ¨¡æ¿
                    optimized_template = dict(template)
                    optimized_template["name"] = f"{template.get('name', '')}çš„ä¼˜åŒ–ç‰ˆæœ¬_{i+1}"
                    optimized_template["description"] = f"ä¼˜åŒ–ç­–ç•¥: {opt_prompt.get('strategy', '')}"
                    optimized_template["template"] = opt_prompt.get("prompt", "")
                    
                    # ä¿å­˜A/Bæµ‹è¯•æ‰€éœ€æ•°æ®åˆ°ä¼šè¯çŠ¶æ€
                    st.session_state.ab_test_original = template
                    st.session_state.ab_test_optimized = optimized_template
                    st.session_state.ab_test_model = model
                    st.session_state.ab_test_model_provider = model_provider
                    st.session_state.ab_test_test_set = st.session_state.specialized_test_set_name
                    
                    # è·³è½¬åˆ°A/Bæµ‹è¯•é¡µé¢
                    st.session_state.page = "prompt_ab_test"
                    st.rerun()
