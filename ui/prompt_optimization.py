# prompt_optimization.py

import streamlit as st
import json
import pandas as pd
import asyncio
from datetime import datetime
import time
import plotly.express as px
import plotly.graph_objects as go

from config import get_template_list, load_template, get_test_set_list, load_test_set, save_template, get_available_models
from models.api_clients import get_client, get_provider_from_model
from models.token_counter import count_tokens, estimate_cost
from utils.evaluator import PromptEvaluator
from utils.optimizer import PromptOptimizer
from utils.common import (
    calculate_average_score, 
    get_dimension_scores, 
    create_dimension_radar_chart,
    run_test,
    display_template_info,
    save_optimized_template
)
from ui.components import (
    display_test_summary,
    display_response_tabs,
    display_evaluation_results,
    display_test_case_details
)

def render_zero_shot_optimization():
    st.title("ğŸ§  0æ ·æœ¬æç¤ºè¯ä¼˜åŒ–ï¼ˆZero-shot Prompt Optimizationï¼‰")
    st.markdown("""
    æ— éœ€æµ‹è¯•é›†å’Œæ ‡æ³¨æ ·æœ¬ï¼Œä»…éœ€è¾“å…¥ä»»åŠ¡æè¿°ã€ç›®æ ‡å’Œçº¦æŸï¼Œç³»ç»Ÿå°†è‡ªåŠ¨ç”Ÿæˆå¤šç»„é«˜è´¨é‡æç¤ºè¯ã€‚
    """)
    
    with st.form("zero_shot_form"):
        task_desc = st.text_area("ä»»åŠ¡æè¿°", help="ç®€è¦æè¿°ä½ å¸Œæœ›AIå®Œæˆçš„ä»»åŠ¡")
        task_goal = st.text_area("ä»»åŠ¡ç›®æ ‡", help="æ˜ç¡®ä½ æœŸæœ›çš„è¾“å‡ºæˆ–æ•ˆæœ")
        constraints = st.text_area("çº¦æŸæ¡ä»¶ï¼ˆå¯é€‰ï¼‰", help="å¦‚é£æ ¼ã€æ ¼å¼ã€é•¿åº¦ã€ç¦æ­¢äº‹é¡¹ç­‰")
        submit = st.form_submit_button("ğŸ”„ ç”Ÿæˆä¼˜åŒ–æç¤ºè¯")
    
    if submit and task_desc and task_goal:
        with st.spinner("AIæ­£åœ¨ç”Ÿæˆæç¤ºè¯..."):
            optimizer = PromptOptimizer()
            result = optimizer.zero_shot_optimize_prompt_sync(task_desc, task_goal, constraints)
            if "error" in result:
                st.error(f"ç”Ÿæˆå¤±è´¥: {result['error']}")
            else:
                prompts = result.get("optimized_prompts", [])
                if not prompts:
                    st.warning("æœªèƒ½ç”Ÿæˆä¼˜åŒ–æç¤ºè¯")
                else:
                    st.success(f"æˆåŠŸç”Ÿæˆ {len(prompts)} ä¸ªä¼˜åŒ–æç¤ºè¯ç‰ˆæœ¬")
                    for i, opt in enumerate(prompts):
                        with st.expander(f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}"):
                            st.markdown(f"**ä¼˜åŒ–ç­–ç•¥**: {opt.get('strategy', '')}")
                            st.markdown(f"**é¢„æœŸæ”¹è¿›**: {opt.get('expected_improvements', '')}")
                            st.code(opt.get('prompt', ''), language="markdown")

def render_prompt_optimization():
    tab1, tab2, tab3 = st.tabs(["ä¸“é¡¹ä¼˜åŒ–ï¼ˆæœ‰æ ·æœ¬ï¼‰", "0æ ·æœ¬ä¼˜åŒ–", "è‡ªåŠ¨è¿­ä»£ä¼˜åŒ–"])
    with tab1:
        st.title("ğŸ” æç¤ºè¯ä¸“é¡¹ä¼˜åŒ–")
        
        st.markdown("""
        è¿™ä¸ªå·¥å…·ä¸“æ³¨äºå•æç¤ºè¯å•æ¨¡å‹çš„æ·±åº¦ä¼˜åŒ–ã€‚æ‚¨å¯ä»¥é€‰æ‹©ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿å’Œä¸€ä¸ªæ¨¡å‹ï¼Œ
        è¿è¡Œæµ‹è¯•å¹¶è·å–AIç”Ÿæˆçš„ä¼˜åŒ–ç‰ˆæœ¬æç¤ºè¯ã€‚
        """)
        
        # æ­¥éª¤1: é€‰æ‹©æç¤ºè¯å’Œæ¨¡å‹
        st.subheader("æ­¥éª¤1: é€‰æ‹©æç¤ºè¯å’Œæ¨¡å‹")
        
        col1, col2 = st.columns(2)
        
        with col1:
            template_list = get_template_list()
            if not template_list:
                st.warning("æœªæ‰¾åˆ°æç¤ºè¯æ¨¡æ¿ï¼Œè¯·å…ˆåˆ›å»ºæ¨¡æ¿")
                return
                
            selected_template = st.selectbox(
                "é€‰æ‹©æç¤ºè¯æ¨¡æ¿",
                template_list
            )
            
            if selected_template:
                template = load_template(selected_template)
                st.info(f"**æè¿°**: {template.get('description', 'æ— æè¿°')}")
                
                with st.expander("æŸ¥çœ‹æç¤ºè¯å†…å®¹"):
                    st.code(template.get("template", ""))
        
        with col2:
            # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
            available_models = get_available_models()
            all_models = []
            
            # åˆ›å»ºç»Ÿä¸€çš„æ¨¡å‹åˆ—è¡¨
            for provider, models in available_models.items():
                for model in models:
                    all_models.append((provider, model))
            
            # åˆ›å»ºä¸‹æ‹‰é€‰é¡¹
            model_options = [f"{model} ({provider})" for provider, model in all_models]
            model_map = {f"{model} ({provider})": (model, provider) for provider, model in all_models}
            
            # é€‰æ‹©æ¨¡å‹
            selected_model_option = st.selectbox(
                "é€‰æ‹©æ¨¡å‹",
                model_options
            )
            
            if selected_model_option:
                selected_model, selected_provider = model_map[selected_model_option]
            else:
                selected_model = ""
                selected_provider = ""
            
            # è¿è¡Œå‚æ•°
            st.subheader("è¿è¡Œå‚æ•°")
            temperature = st.slider("Temperature", 0.0, 2.0, 0.7, 0.1)
            max_tokens = st.slider("æœ€å¤§è¾“å‡ºToken", 100, 4000, 1000, 100)
            repeat_count = st.slider("æ¯ä¸ªæµ‹è¯•é‡å¤æ¬¡æ•°", 1, 3, 2, 1)
        
        # æ­¥éª¤2: é€‰æ‹©æµ‹è¯•é›†
        st.subheader("æ­¥éª¤2: é€‰æ‹©æµ‹è¯•é›†")
        
        test_set_list = get_test_set_list()
        if not test_set_list:
            st.warning("æœªæ‰¾åˆ°æµ‹è¯•é›†ï¼Œè¯·å…ˆåˆ›å»ºæµ‹è¯•é›†")
            return
            
        selected_test_set = st.selectbox(
            "é€‰æ‹©æµ‹è¯•é›†",
            test_set_list
        )
        
        if selected_test_set:
            test_set = load_test_set(selected_test_set)
            st.info(f"**æµ‹è¯•ç”¨ä¾‹æ•°**: {len(test_set.get('cases', []))}")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æµ‹è¯•ç»“æœ
        has_test_results = "specialized_test_results" in st.session_state
        
        # æ­¥éª¤3: è¿è¡Œæµ‹è¯•
        st.subheader("æ­¥éª¤3: è¿è¡Œæµ‹è¯•")
        
        if not has_test_results:  # ä»…åœ¨æ²¡æœ‰æµ‹è¯•ç»“æœæ—¶æ˜¾ç¤ºæµ‹è¯•æŒ‰é’®
            if st.button("â–¶ï¸ å¼€å§‹ä¸“é¡¹æµ‹è¯•", type="primary"):
                if not selected_template or not selected_model or not selected_test_set:
                    st.error("è¯·å…ˆé€‰æ‹©æç¤ºè¯æ¨¡æ¿ã€æ¨¡å‹å’Œæµ‹è¯•é›†")
                    return
                    
                # å¼€å§‹æµ‹è¯•
                with st.spinner("æµ‹è¯•è¿è¡Œä¸­..."):
                    # åŠ è½½æµ‹è¯•é›†
                    test_set = load_test_set(selected_test_set)
                    
                    test_results = run_test(
                        template=template,
                        model=selected_model,
                        test_set=test_set,
                        model_provider=selected_provider,
                        repeat_count=repeat_count,
                        temperature=temperature
                    )
                    
                    if test_results:
                        # ä¿å­˜ç»“æœåˆ°ä¼šè¯çŠ¶æ€ï¼Œä»¥ä¾¿åœ¨ä¼˜åŒ–æ­¥éª¤ä¸­ä½¿ç”¨
                        st.session_state.specialized_test_results = test_results
                        st.session_state.specialized_template = template
                        st.session_state.specialized_model = selected_model
                        st.session_state.specialized_model_provider = selected_provider
                        st.session_state.specialized_test_set_name = selected_test_set
                        
                        # åˆ·æ–°é¡µé¢ä»¥æ˜¾ç¤ºç»“æœå’Œä¼˜åŒ–æŒ‰é’®
                        st.rerun()
        
        # å¦‚æœå·²æœ‰æµ‹è¯•ç»“æœï¼Œæ˜¾ç¤ºç»“æœå’Œä¼˜åŒ–æŒ‰é’®
        if has_test_results:
            # é‡æ–°è·å–ä¼šè¯çŠ¶æ€ä¸­çš„æ•°æ®
            test_results = st.session_state.specialized_test_results
            template = st.session_state.specialized_template
            selected_model = st.session_state.specialized_model
            selected_provider = st.session_state.specialized_model_provider
            
            # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
            display_test_summary(test_results, template, selected_model)
            
            # æ˜¾ç¤ºè¯¦ç»†æµ‹è¯•ç»“æœ
            st.subheader("è¯¦ç»†æµ‹è¯•ç»“æœ")
            
            for i, case in enumerate(test_results.get("test_cases", [])):
                with st.expander(f"æµ‹è¯•ç”¨ä¾‹ {i+1}: {case.get('case_description', case.get('case_id', ''))}"):
                    display_test_case_details(case, inside_expander=True)
            
            # æ·»åŠ æ¸…é™¤ç»“æœæŒ‰é’®
            if st.button("ğŸ—‘ï¸ æ¸…é™¤æµ‹è¯•ç»“æœ", key="clear_results"):
                # æ¸…é™¤ä¼šè¯çŠ¶æ€ä¸­çš„æµ‹è¯•ç»“æœ
                if "specialized_test_results" in st.session_state:
                    del st.session_state.specialized_test_results
                if "specialized_template" in st.session_state:
                    del st.session_state.specialized_template
                if "specialized_model" in st.session_state:
                    del st.session_state.specialized_model
                if "specialized_model_provider" in st.session_state:
                    del st.session_state.specialized_model_provider
                if "specialized_test_set_name" in st.session_state:
                    del st.session_state.specialized_test_set_name
                if "optimized_prompts" in st.session_state:
                    del st.session_state.optimized_prompts
                
                # åˆ·æ–°é¡µé¢
                st.rerun()
            
            # æ­¥éª¤4: ç”Ÿæˆä¼˜åŒ–æç¤ºè¯
            st.subheader("æ­¥éª¤4: ç”Ÿæˆä¼˜åŒ–æç¤ºè¯")
            
            # æ·»åŠ è‡ªåŠ¨æ‰¹é‡è¯„ä¼°é€‰é¡¹
            auto_evaluate = st.checkbox(
                "ç”Ÿæˆä¼˜åŒ–æç¤ºè¯åè‡ªåŠ¨è¿›è¡Œæ‰¹é‡è¯„ä¼°", 
                value=False,
                help="é€‰ä¸­æ­¤é€‰é¡¹å°†åœ¨ç”Ÿæˆä¼˜åŒ–æç¤ºè¯åè‡ªåŠ¨è¿›è¡Œæ‰¹é‡è¯„ä¼°"
            )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¼˜åŒ–ç»“æœ
            has_optimization_results = "optimized_prompts" in st.session_state

            optimization_strategy = st.selectbox(
                "é€‰æ‹©ä¼˜åŒ–ç­–ç•¥",
                ["balanced", "accuracy", "completeness", "conciseness"],
                format_func=lambda x: {
                    "balanced": "å¹³è¡¡ä¼˜åŒ– (å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œç®€æ´æ€§)",
                    "accuracy": "ä¼˜åŒ–å‡†ç¡®æ€§",
                    "completeness": "ä¼˜åŒ–å®Œæ•´æ€§",
                    "conciseness": "ä¼˜åŒ–ç®€æ´æ€§"
                }.get(x, x)
            )
            
            # åªæœ‰åœ¨æ²¡æœ‰ä¼˜åŒ–ç»“æœæ—¶æ˜¾ç¤ºä¼˜åŒ–æŒ‰é’®
            if not has_optimization_results:
                if st.button("ğŸ”„ ç”Ÿæˆä¼˜åŒ–æç¤ºè¯", key="optimize_button", type="primary"):
                    generate_optimized_prompts(
                        test_results, 
                        template, 
                        selected_model, 
                        optimization_strategy,
                        auto_evaluate=auto_evaluate,
                        model_provider=selected_provider
                    )
            
            # å¦‚æœå·²æœ‰ä¼˜åŒ–ç»“æœï¼Œæ˜¾ç¤ºç»“æœ
            if has_optimization_results:
                display_optimized_prompts(
                    st.session_state.optimized_prompts, 
                    template, 
                    selected_model, 
                    selected_provider
                )
                
                # æ·»åŠ é‡æ–°ä¼˜åŒ–æŒ‰é’®
                if st.button("ğŸ”„ é‡æ–°ç”Ÿæˆä¼˜åŒ–æç¤ºè¯", key="regenerate"):
                    generate_optimized_prompts(
                        test_results, 
                        template, 
                        selected_model, 
                        optimization_strategy,
                        auto_evaluate=auto_evaluate,
                        model_provider=selected_provider
                    )
    with tab2:
        render_zero_shot_optimization()
    with tab3:
        render_iterative_optimization()

def render_iterative_optimization():
    st.title("ğŸ” è‡ªåŠ¨å¤šè½®æç¤ºè¯è¿­ä»£ä¼˜åŒ–")
    st.markdown("""
    æœ¬åŠŸèƒ½æ”¯æŒè‡ªåŠ¨å¤šè½®æç¤ºè¯ä¼˜åŒ–ä¸è¯„ä¼°ï¼Œè‡ªåŠ¨é€‰å‡ºæœ€ä¼˜ç‰ˆæœ¬ã€‚
    """)
    # é€‰æ‹©æ¨¡æ¿ã€æ¨¡å‹ã€æµ‹è¯•é›†
    template_list = get_template_list()
    if not template_list:
        st.warning("æœªæ‰¾åˆ°æç¤ºè¯æ¨¡æ¿ï¼Œè¯·å…ˆåˆ›å»ºæ¨¡æ¿")
        return
    selected_template = st.selectbox("é€‰æ‹©æç¤ºè¯æ¨¡æ¿", template_list, key="iter_template")
    template = load_template(selected_template) if selected_template else None
    available_models = get_available_models()
    all_models = [(provider, model) for provider, models in available_models.items() for model in models]
    model_options = [f"{model} ({provider})" for provider, model in all_models]
    model_map = {f"{model} ({provider})": (model, provider) for provider, model in all_models}
    selected_model_option = st.selectbox("é€‰æ‹©æ¨¡å‹", model_options, key="iter_model")
    selected_model, selected_provider = model_map[selected_model_option] if selected_model_option else (None, None)

    # æ–°å¢ï¼šæµ‹è¯•é›†é€‰æ‹©æ–¹å¼
    st.subheader("æ­¥éª¤2: é€‰æ‹©æµ‹è¯•é›†")
    test_set_mode = st.radio(
        "æµ‹è¯•é›†æ¥æº",
        ["é€‰æ‹©å·²æœ‰æµ‹è¯•é›†", "AIè‡ªåŠ¨ç”Ÿæˆæ–°æµ‹è¯•é›†"],
        horizontal=True,
        key="iter_testset_mode"
    )
    test_set = []
    test_set_name = None
    if test_set_mode == "é€‰æ‹©å·²æœ‰æµ‹è¯•é›†":
        test_set_list = get_test_set_list()
        if not test_set_list:
            st.warning("æœªæ‰¾åˆ°æµ‹è¯•é›†ï¼Œè¯·å…ˆåˆ›å»ºæµ‹è¯•é›†")
            return
        selected_test_set = st.selectbox("é€‰æ‹©æµ‹è¯•é›†", test_set_list, key="iter_testset")
        test_set = load_test_set(selected_test_set).get("cases", []) if selected_test_set else []
        test_set_name = selected_test_set
        st.info(f"**æµ‹è¯•ç”¨ä¾‹æ•°**: {len(test_set)}")
    else:
        # AIè‡ªåŠ¨ç”Ÿæˆæ–°æµ‹è¯•é›†
        test_set_name = st.text_input("æ–°æµ‹è¯•é›†åç§°", value=f"AIç”Ÿæˆæµ‹è¯•é›†_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        test_set_desc = st.text_input("æ–°æµ‹è¯•é›†æè¿°", value="è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•é›†")
        gen_case_count = st.number_input("ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ•°é‡", min_value=3, max_value=20, value=6, step=1)
        st.info("å°†æ ¹æ®å½“å‰æç¤ºè¯æ¨¡æ¿å’Œæ¨¡å‹ï¼Œè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹")

    # è¿­ä»£æ¬¡æ•°
    max_iterations = st.slider("è¿­ä»£æ¬¡æ•°", 1, 10, 3)
    optimization_strategy = st.selectbox(
        "ä¼˜åŒ–ç­–ç•¥",
        ["balanced", "accuracy", "completeness", "conciseness"],
        format_func=lambda x: {
            "balanced": "å¹³è¡¡ä¼˜åŒ– (å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œç®€æ´æ€§)",
            "accuracy": "ä¼˜åŒ–å‡†ç¡®æ€§",
            "completeness": "ä¼˜åŒ–å®Œæ•´æ€§",
            "conciseness": "ä¼˜åŒ–ç®€æ´æ€§"
        }.get(x, x),
        key="iter_strategy"
    )
    if st.button("ğŸš€ å¼€å§‹è‡ªåŠ¨è¿­ä»£ä¼˜åŒ–", type="primary"):
        with st.spinner("æ­£åœ¨è‡ªåŠ¨å¤šè½®ä¼˜åŒ–ä¸è¯„ä¼°..."):
            # è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•é›†é€»è¾‘
            if test_set_mode == "AIè‡ªåŠ¨ç”Ÿæˆæ–°æµ‹è¯•é›†":
                evaluator = PromptEvaluator()
                # æ„é€ ç¤ºä¾‹ç”¨ä¾‹
                example_case = {
                    "id": "example_case",
                    "description": "ç¤ºä¾‹ç”¨ä¾‹",
                    "user_input": "è¯·ç®€è¦ä»‹ç»äººå·¥æ™ºèƒ½ã€‚",
                    "expected_output": "äººå·¥æ™ºèƒ½æ˜¯æŒ‡ä½¿è®¡ç®—æœºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æŠ€æœ¯ã€‚",
                    "evaluation": {
                        "scores": {"accuracy": 85, "completeness": 80, "relevance": 90, "clarity": 88}
                    }
                }
                test_purpose = test_set_desc or test_set_name
                gen_result = evaluator.generate_test_cases(
                    model=selected_model,
                    test_purpose=f"{test_purpose}ï¼Œè¯·ç”Ÿæˆ{gen_case_count}ä¸ªé«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–ä¸åŒåœºæ™¯å’Œè¾¹ç•Œã€‚",
                    example_case=example_case
                )
                if "error" in gen_result:
                    st.error(f"æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥: {gen_result['error']}")
                    if "raw_response" in gen_result:
                        st.text_area("åŸå§‹AIå“åº”", value=gen_result["raw_response"], height=200)
                    return
                test_cases = gen_result.get("test_cases", [])
                if not test_cases:
                    st.error("AIæœªç”Ÿæˆä»»ä½•æµ‹è¯•ç”¨ä¾‹ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å’ŒAPIè®¾ç½®")
                    return
                test_set = test_cases
                # è‡ªåŠ¨ä¿å­˜æ–°æµ‹è¯•é›†
                new_test_set_obj = {
                    "name": test_set_name,
                    "description": test_set_desc,
                    "variables": {},
                    "cases": test_cases
                }
                from config import save_test_set
                save_test_set(test_set_name, new_test_set_obj)
                st.success(f"å·²è‡ªåŠ¨ç”Ÿæˆå¹¶ä¿å­˜æ–°æµ‹è¯•é›†ï¼š{test_set_name}ï¼Œå…±{len(test_cases)}æ¡ç”¨ä¾‹")
            # ç»§ç»­ä¼˜åŒ–æµç¨‹
            optimizer = PromptOptimizer()
            evaluator = PromptEvaluator()
            progress_bar = st.progress(0)
            def progress_callback(i, total, score):
                progress_bar.progress(i/total)
                st.info(f"ç¬¬{i}è½®å®Œæˆï¼Œå¹³å‡åˆ†ï¼š{score:.2f}")
            result = optimizer.iterative_prompt_optimization_sync(
                initial_prompt=template.get("template", ""),
                test_set=test_set,
                evaluator=evaluator,
                optimization_strategy=optimization_strategy,
                model=selected_model,
                provider=selected_provider,
                max_iterations=max_iterations,
                progress_callback=progress_callback
            )
            st.success("è¿­ä»£ä¼˜åŒ–å®Œæˆï¼")
            # å±•ç¤ºæ¯è½®ç»“æœ
            history = result.get("history", [])
            for item in history:
                with st.expander(f"ç¬¬{item['iteration']}è½® ä¼˜åŒ–ç‰ˆæœ¬"):
                    st.markdown(f"**å¹³å‡åˆ†**: {item['avg_score']:.2f}")
                    st.code(item['prompt'], language="markdown")
            # å±•ç¤ºæœ€ä¼˜ç»“æœ
            st.markdown("---")
            st.header("æœ€ä¼˜æç¤ºè¯")
            best_prompt = result.get("best_prompt", "")
            best_score = result.get("best_score", 0)
            st.code(best_prompt, language="markdown")
            st.markdown(f"**æœ€ä¼˜å¹³å‡åˆ†**: {best_score:.2f}")
            # è‡ªåŠ¨ä¿å­˜æœ€ä¼˜æç¤ºè¯ä¸ºæ–°æ¨¡æ¿ï¼Œå¹¶å­˜å…¥session_state
            from utils.common import save_optimized_template
            new_name = save_optimized_template(template, {"prompt": best_prompt}, 0)
            st.session_state.iter_best_prompt = best_prompt
            st.session_state.iter_best_score = best_score
            st.session_state.iter_best_template_name = new_name
            st.success(f"æœ€ä¼˜æç¤ºè¯å·²è‡ªåŠ¨ä¿å­˜ä¸ºæ–°æ¨¡æ¿: {new_name}")
            if st.button("ğŸ’¾ å†æ¬¡ä¿å­˜æœ€ä¼˜æç¤ºè¯ä¸ºæ–°æ¨¡æ¿"):
                new_name2 = save_optimized_template(template, {"prompt": best_prompt}, int(time.time())%10000)
                st.success(f"å·²ä¿å­˜ä¸ºæ–°æ¨¡æ¿: {new_name2}")

def generate_optimized_prompts(results, template, model, optimization_strategy, auto_evaluate=False, model_provider=None):
    """æ ¹æ®æµ‹è¯•ç»“æœç”Ÿæˆä¼˜åŒ–æç¤ºè¯"""
    
    with st.spinner("AIæ­£åœ¨åˆ†ææµ‹è¯•ç»“æœå¹¶ç”Ÿæˆä¼˜åŒ–æç¤ºè¯..."):
        # æ”¶é›†è¯„ä¼°ç»“æœ
        evaluations = []
        
        # éå†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        for case in results.get("test_cases", []):
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–°çš„å“åº”æ ¼å¼
            responses = case.get("responses", [])
            
            if responses:
                # å¤„ç†æ¯ä¸ªå“åº”çš„è¯„ä¼°
                for response in responses:
                    if response.get("evaluation") and not response.get("error"):
                        evaluations.append(response["evaluation"])
            elif case.get("evaluation"):
                # å…¼å®¹æ—§æ ¼å¼
                evaluations.append(case["evaluation"])
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœï¼Œæ— æ³•ä¼˜åŒ–
        if not evaluations:
            st.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä¼°ç»“æœï¼Œæ— æ³•ç”Ÿæˆä¼˜åŒ–æç¤ºè¯")
            return
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = PromptOptimizer()
        
        # ç”Ÿæˆä¼˜åŒ–æç¤ºè¯
        optimization_result = optimizer.optimize_prompt_sync(
            template.get("template", ""),
            evaluations,
            optimization_strategy
        )
        
        if "error" in optimization_result:
            st.error(f"ä¼˜åŒ–å¤±è´¥: {optimization_result['error']}")
        else:
            optimized_prompts = optimization_result.get("optimized_prompts", [])
            
            if not optimized_prompts:
                st.warning("æœªèƒ½ç”Ÿæˆä¼˜åŒ–æç¤ºè¯")
                return
            
            # å°†ä¼˜åŒ–ç»“æœä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.optimized_prompts = optimized_prompts
            st.success(f"æˆåŠŸç”Ÿæˆ {len(optimized_prompts)} ä¸ªä¼˜åŒ–æç¤ºè¯ç‰ˆæœ¬")
            
            # æ˜¾ç¤ºä¼˜åŒ–æç¤ºè¯
            display_optimized_prompts(optimized_prompts, template, model, model_provider)
            
            # è‡ªåŠ¨è¿›è¡Œæ‰¹é‡è¯„ä¼°
            if auto_evaluate:
                # åˆ›å»ºä¼˜åŒ–åçš„æ¨¡æ¿åˆ—è¡¨
                optimized_templates = []
                for i, opt_prompt in enumerate(optimized_prompts):
                    optimized_template = dict(template)
                    optimized_template["name"] = f"{template.get('name', '')}çš„ä¼˜åŒ–ç‰ˆæœ¬_{i+1}"
                    optimized_template["description"] = f"ä¼˜åŒ–ç­–ç•¥: {opt_prompt.get('strategy', '')}"
                    optimized_template["template"] = opt_prompt.get("prompt", "")
                    optimized_templates.append(optimized_template)
                
                # ä¿å­˜æ‰¹é‡A/Bæµ‹è¯•æ‰€éœ€æ•°æ®åˆ°ä¼šè¯çŠ¶æ€
                st.session_state.batch_ab_test_original = template
                st.session_state.batch_ab_test_optimized = optimized_templates
                st.session_state.batch_ab_test_model = model
                st.session_state.batch_ab_test_model_provider = model_provider
                st.session_state.batch_ab_test_test_set = st.session_state.specialized_test_set_name
                
                # è·³è½¬åˆ°æ‰¹é‡A/Bæµ‹è¯•é¡µé¢
                st.session_state.page = "prompt_batch_ab_test"
                st.rerun()

def display_optimized_prompts(optimized_prompts, template, model, model_provider):
    """æ˜¾ç¤ºä¼˜åŒ–æç¤ºè¯ç»“æœ"""
    if not optimized_prompts:
        st.warning("æ²¡æœ‰ä¼˜åŒ–æç¤ºè¯å¯æ˜¾ç¤º")
        return
    
    # åªæœ‰åœ¨æœªé€‰æ‹©è‡ªåŠ¨è¯„ä¼°æ—¶æ‰æ˜¾ç¤ºæ‰¹é‡è¯„ä¼°æŒ‰é’®
    if st.button("ğŸ”¬ æ‰¹é‡è¯„ä¼°æ‰€æœ‰ä¼˜åŒ–ç‰ˆæœ¬", type="primary"):
        # åˆ›å»ºä¼˜åŒ–åçš„æ¨¡æ¿åˆ—è¡¨
        optimized_templates = []
        for i, opt_prompt in enumerate(optimized_prompts):
            optimized_template = dict(template)
            optimized_template["name"] = f"{template.get('name', '')}çš„ä¼˜åŒ–ç‰ˆæœ¬_{i+1}"
            optimized_template["description"] = f"ä¼˜åŒ–ç­–ç•¥: {opt_prompt.get('strategy', '')}"
            optimized_template["template"] = opt_prompt.get("prompt", "")
            optimized_templates.append(optimized_template)
        
        # ä¿å­˜æ‰¹é‡A/Bæµ‹è¯•æ‰€éœ€æ•°æ®åˆ°ä¼šè¯çŠ¶æ€
        st.session_state.batch_ab_test_original = template
        st.session_state.batch_ab_test_optimized = optimized_templates
        st.session_state.batch_ab_test_model = model
        st.session_state.batch_ab_test_model_provider = model_provider
        st.session_state.batch_ab_test_test_set = st.session_state.specialized_test_set_name
        
        # è·³è½¬åˆ°æ‰¹é‡A/Bæµ‹è¯•é¡µé¢
        st.session_state.page = "prompt_batch_ab_test"
        st.rerun()
    
    # æ˜¾ç¤ºå„ä¸ªä¼˜åŒ–æç¤ºè¯ç‰ˆæœ¬
    for i, opt_prompt in enumerate(optimized_prompts):
        with st.expander(f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}: {opt_prompt.get('strategy', 'æœªçŸ¥ç­–ç•¥')}"):
            # ä½¿ç”¨æ›´æ¸…æ™°çš„è§†è§‰åˆ†éš”
            st.divider()
            
            # ä¼˜åŒ–ç­–ç•¥éƒ¨åˆ†
            st.markdown("#### ä¼˜åŒ–ç­–ç•¥")
            st.write(opt_prompt.get("strategy", ""))
            
            # æ˜¾ç¤ºé’ˆå¯¹è§£å†³çš„é—®é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
            if "problem_addressed" in opt_prompt:
                st.markdown("#### é’ˆå¯¹è§£å†³çš„é—®é¢˜")
                st.info(opt_prompt.get("problem_addressed", ""))
            
            # é¢„æœŸæ”¹è¿›
            st.markdown("#### é¢„æœŸæ”¹è¿›")
            st.write(opt_prompt.get("expected_improvements", ""))
            
            # ä¼˜åŒ–ç†ç”±ï¼ˆå¦‚æœæœ‰ï¼‰
            if "reasoning" in opt_prompt:
                st.markdown("#### ä¼˜åŒ–ç†ç”±")
                st.info(opt_prompt.get("reasoning", ""))
            
            st.divider()
            
            # æ˜¾ç¤ºä¼˜åŒ–åçš„æç¤ºè¯
            st.markdown("#### ä¼˜åŒ–åçš„æç¤ºè¯")
            st.code(opt_prompt.get("prompt", ""), language="markdown")
            
            st.divider()
            
            # åˆ›å»ºæŒ‰é’®ï¼Œå°†ä¼˜åŒ–åçš„æç¤ºè¯ä¿å­˜ä¸ºæ–°æ¨¡æ¿æˆ–è¿è¡ŒA/Bæµ‹è¯•
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button(f"ğŸ’¾ ä¿å­˜ä¸ºæ–°æ¨¡æ¿", key=f"save_opt_{i}"):
                    new_name = save_optimized_template(template, opt_prompt, i)
                    st.success(f"å·²ä¿å­˜ä¸ºæ–°æ¨¡æ¿: {new_name}")
            
            with col2:
                if st.button(f"ğŸ” A/Bæµ‹è¯•", key=f"test_opt_{i}"):
                    # åˆ›å»ºä¼˜åŒ–åçš„æ¨¡æ¿
                    optimized_template = dict(template)
                    optimized_template["name"] = f"{template.get('name', '')}çš„ä¼˜åŒ–ç‰ˆæœ¬_{i+1}"
                    optimized_template["description"] = f"ä¼˜åŒ–ç­–ç•¥: {opt_prompt.get('strategy', '')}"
                    optimized_template["template"] = opt_prompt.get("prompt", "")
                    
                    # ä¿å­˜A/Bæµ‹è¯•æ‰€éœ€æ•°æ®åˆ°ä¼šè¯çŠ¶æ€
                    st.session_state.ab_test_original = template
                    st.session_state.ab_test_optimized = optimized_template
                    st.session_state.ab_test_model = model
                    st.session_state.ab_test_model_provider = model_provider
                    st.session_state.ab_test_test_set = st.session_state.specialized_test_set_name
                    
                    # è·³è½¬åˆ°A/Bæµ‹è¯•é¡µé¢
                    st.session_state.page = "prompt_ab_test"
                    st.rerun()
