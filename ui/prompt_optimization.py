# prompt_optimization.py

import streamlit as st
import json
import pandas as pd
import asyncio
from datetime import datetime
import time
import plotly.express as px
import plotly.graph_objects as go

from config import get_template_list, load_template, get_test_set_list, load_test_set, save_template, get_available_models
from models.api_clients import get_client, get_provider_from_model
from models.token_counter import count_tokens, estimate_cost
from utils.evaluator import PromptEvaluator
from utils.optimizer import PromptOptimizer
from utils.common import (
    calculate_average_score, 
    get_dimension_scores, 
    create_dimension_radar_chart,
    run_test,
    display_template_info
)
from ui.components import (
    display_test_summary,
    display_response_tabs,
    display_evaluation_results,
    display_test_case_details
)

def render_prompt_optimization():
    st.title("ğŸ” æç¤ºè¯ä¸“é¡¹ä¼˜åŒ–")
    
    st.markdown("""
    è¿™ä¸ªå·¥å…·ä¸“æ³¨äºå•æç¤ºè¯å•æ¨¡å‹çš„æ·±åº¦ä¼˜åŒ–ã€‚æ‚¨å¯ä»¥é€‰æ‹©ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿å’Œä¸€ä¸ªæ¨¡å‹ï¼Œ
    è¿è¡Œæµ‹è¯•å¹¶è·å–AIç”Ÿæˆçš„ä¼˜åŒ–ç‰ˆæœ¬æç¤ºè¯ã€‚
    """)
    
    # æ­¥éª¤1: é€‰æ‹©æç¤ºè¯å’Œæ¨¡å‹
    st.subheader("æ­¥éª¤1: é€‰æ‹©æç¤ºè¯å’Œæ¨¡å‹")
    
    col1, col2 = st.columns(2)
    
    with col1:
        template_list = get_template_list()
        if not template_list:
            st.warning("æœªæ‰¾åˆ°æç¤ºè¯æ¨¡æ¿ï¼Œè¯·å…ˆåˆ›å»ºæ¨¡æ¿")
            return
            
        selected_template = st.selectbox(
            "é€‰æ‹©æç¤ºè¯æ¨¡æ¿",
            template_list
        )
        
        if selected_template:
            template = load_template(selected_template)
            st.info(f"**æè¿°**: {template.get('description', 'æ— æè¿°')}")
            
            with st.expander("æŸ¥çœ‹æç¤ºè¯å†…å®¹"):
                st.code(template.get("template", ""))
    
    with col2:
        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        available_models = get_available_models()
        all_models = []
        
        # åˆ›å»ºç»Ÿä¸€çš„æ¨¡å‹åˆ—è¡¨
        for provider, models in available_models.items():
            for model in models:
                all_models.append((provider, model))
        
        # åˆ›å»ºä¸‹æ‹‰é€‰é¡¹
        model_options = [f"{model} ({provider})" for provider, model in all_models]
        model_map = {f"{model} ({provider})": (model, provider) for provider, model in all_models}
        
        # é€‰æ‹©æ¨¡å‹
        selected_model_option = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            model_options
        )
        
        if selected_model_option:
            selected_model, selected_provider = model_map[selected_model_option]
        else:
            selected_model = ""
            selected_provider = ""
        
        # è¿è¡Œå‚æ•°
        st.subheader("è¿è¡Œå‚æ•°")
        temperature = st.slider("Temperature", 0.0, 2.0, 0.7, 0.1)
        max_tokens = st.slider("æœ€å¤§è¾“å‡ºToken", 100, 4000, 1000, 100)
        repeat_count = st.slider("æ¯ä¸ªæµ‹è¯•é‡å¤æ¬¡æ•°", 1, 3, 2, 1)
    
    # æ­¥éª¤2: é€‰æ‹©æµ‹è¯•é›†
    st.subheader("æ­¥éª¤2: é€‰æ‹©æµ‹è¯•é›†")
    
    test_set_list = get_test_set_list()
    if not test_set_list:
        st.warning("æœªæ‰¾åˆ°æµ‹è¯•é›†ï¼Œè¯·å…ˆåˆ›å»ºæµ‹è¯•é›†")
        return
        
    selected_test_set = st.selectbox(
        "é€‰æ‹©æµ‹è¯•é›†",
        test_set_list
    )
    
    if selected_test_set:
        test_set = load_test_set(selected_test_set)
        st.info(f"**æµ‹è¯•ç”¨ä¾‹æ•°**: {len(test_set.get('cases', []))}")
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æµ‹è¯•ç»“æœ
    has_test_results = "specialized_test_results" in st.session_state
    
    # æ­¥éª¤3: è¿è¡Œæµ‹è¯•
    st.subheader("æ­¥éª¤3: è¿è¡Œæµ‹è¯•")
    
    if not has_test_results:  # ä»…åœ¨æ²¡æœ‰æµ‹è¯•ç»“æœæ—¶æ˜¾ç¤ºæµ‹è¯•æŒ‰é’®
        if st.button("â–¶ï¸ å¼€å§‹ä¸“é¡¹æµ‹è¯•", type="primary"):
            if not selected_template or not selected_model or not selected_test_set:
                st.error("è¯·å…ˆé€‰æ‹©æç¤ºè¯æ¨¡æ¿ã€æ¨¡å‹å’Œæµ‹è¯•é›†")
                return
                
            # å¼€å§‹æµ‹è¯•
            with st.spinner("æµ‹è¯•è¿è¡Œä¸­..."):
                # åŠ è½½æµ‹è¯•é›†
                test_set = load_test_set(selected_test_set)
                
                test_results = run_test(
                    template=template,
                    model=selected_model,
                    test_set=test_set,
                    model_provider=selected_provider,
                    repeat_count=repeat_count,
                    temperature=temperature
                )
                
                if test_results:
                    # ä¿å­˜ç»“æœåˆ°ä¼šè¯çŠ¶æ€ï¼Œä»¥ä¾¿åœ¨ä¼˜åŒ–æ­¥éª¤ä¸­ä½¿ç”¨
                    st.session_state.specialized_test_results = test_results
                    st.session_state.specialized_template = template
                    st.session_state.specialized_model = selected_model
                    st.session_state.specialized_model_provider = selected_provider
                    st.session_state.specialized_test_set_name = selected_test_set
                    
                    # åˆ·æ–°é¡µé¢ä»¥æ˜¾ç¤ºç»“æœå’Œä¼˜åŒ–æŒ‰é’®
                    st.experimental_rerun()
    
    # å¦‚æœå·²æœ‰æµ‹è¯•ç»“æœï¼Œæ˜¾ç¤ºç»“æœå’Œä¼˜åŒ–æŒ‰é’®
    if has_test_results:
        # é‡æ–°è·å–ä¼šè¯çŠ¶æ€ä¸­çš„æ•°æ®
        test_results = st.session_state.specialized_test_results
        template = st.session_state.specialized_template
        selected_model = st.session_state.specialized_model
        selected_provider = st.session_state.specialized_model_provider
        
        # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
        display_test_summary(test_results, template, selected_model)
        
        # æ˜¾ç¤ºè¯¦ç»†æµ‹è¯•ç»“æœ
        st.subheader("è¯¦ç»†æµ‹è¯•ç»“æœ")
        
        for i, case in enumerate(test_results.get("test_cases", [])):
            with st.expander(f"æµ‹è¯•ç”¨ä¾‹ {i+1}: {case.get('case_description', case.get('case_id', ''))}"):
                display_test_case_details(case)
        
        # æ·»åŠ æ¸…é™¤ç»“æœæŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æµ‹è¯•ç»“æœ", key="clear_results"):
            # æ¸…é™¤ä¼šè¯çŠ¶æ€ä¸­çš„æµ‹è¯•ç»“æœ
            if "specialized_test_results" in st.session_state:
                del st.session_state.specialized_test_results
            if "specialized_template" in st.session_state:
                del st.session_state.specialized_template
            if "specialized_model" in st.session_state:
                del st.session_state.specialized_model
            if "specialized_model_provider" in st.session_state:
                del st.session_state.specialized_model_provider
            if "specialized_test_set_name" in st.session_state:
                del st.session_state.specialized_test_set_name
            if "optimized_prompts" in st.session_state:
                del st.session_state.optimized_prompts
            
            # åˆ·æ–°é¡µé¢
            st.experimental_rerun()
        
        # æ­¥éª¤4: ç”Ÿæˆä¼˜åŒ–æç¤ºè¯
        st.subheader("æ­¥éª¤4: ç”Ÿæˆä¼˜åŒ–æç¤ºè¯")
        
        # æ·»åŠ è‡ªåŠ¨æ‰¹é‡è¯„ä¼°é€‰é¡¹
        auto_evaluate = st.checkbox(
            "ç”Ÿæˆä¼˜åŒ–æç¤ºè¯åè‡ªåŠ¨è¿›è¡Œæ‰¹é‡è¯„ä¼°", 
            value=False,
            help="é€‰ä¸­æ­¤é€‰é¡¹å°†åœ¨ç”Ÿæˆä¼˜åŒ–æç¤ºè¯åè‡ªåŠ¨è¿›è¡Œæ‰¹é‡è¯„ä¼°"
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¼˜åŒ–ç»“æœ
        has_optimization_results = "optimized_prompts" in st.session_state

        optimization_strategy = st.selectbox(
            "é€‰æ‹©ä¼˜åŒ–ç­–ç•¥",
            ["balanced", "accuracy", "completeness", "conciseness"],
            format_func=lambda x: {
                "balanced": "å¹³è¡¡ä¼˜åŒ– (å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œç®€æ´æ€§)",
                "accuracy": "ä¼˜åŒ–å‡†ç¡®æ€§",
                "completeness": "ä¼˜åŒ–å®Œæ•´æ€§",
                "conciseness": "ä¼˜åŒ–ç®€æ´æ€§"
            }.get(x, x)
        )
        
        # åªæœ‰åœ¨æ²¡æœ‰ä¼˜åŒ–ç»“æœæ—¶æ˜¾ç¤ºä¼˜åŒ–æŒ‰é’®
        if not has_optimization_results:
            if st.button("ğŸ”„ ç”Ÿæˆä¼˜åŒ–æç¤ºè¯", key="optimize_button", type="primary"):
                generate_optimized_prompts(
                    test_results, 
                    template, 
                    selected_model, 
                    optimization_strategy,
                    auto_evaluate=auto_evaluate,
                    model_provider=selected_provider
                )
        
        # å¦‚æœå·²æœ‰ä¼˜åŒ–ç»“æœï¼Œæ˜¾ç¤ºç»“æœ
        if has_optimization_results:
            display_optimized_prompts(
                st.session_state.optimized_prompts, 
                template, 
                selected_model, 
                selected_provider
            )
            
            # æ·»åŠ é‡æ–°ä¼˜åŒ–æŒ‰é’®
            if st.button("ğŸ”„ é‡æ–°ç”Ÿæˆä¼˜åŒ–æç¤ºè¯", key="regenerate"):
                generate_optimized_prompts(
                    test_results, 
                    template, 
                    selected_model, 
                    optimization_strategy,
                    auto_evaluate=auto_evaluate,
                    model_provider=selected_provider
                )

def generate_optimized_prompts(results, template, model, optimization_strategy, auto_evaluate=False, model_provider=None):
    """æ ¹æ®æµ‹è¯•ç»“æœç”Ÿæˆä¼˜åŒ–æç¤ºè¯"""
    
    with st.spinner("AIæ­£åœ¨åˆ†ææµ‹è¯•ç»“æœå¹¶ç”Ÿæˆä¼˜åŒ–æç¤ºè¯..."):
        # æ”¶é›†è¯„ä¼°ç»“æœ
        evaluations = []
        
        # éå†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        for case in results.get("test_cases", []):
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–°çš„å“åº”æ ¼å¼
            responses = case.get("responses", [])
            
            if responses:
                # å¤„ç†æ¯ä¸ªå“åº”çš„è¯„ä¼°
                for response in responses:
                    if response.get("evaluation") and not response.get("error"):
                        evaluations.append(response["evaluation"])
            elif case.get("evaluation"):
                # å…¼å®¹æ—§æ ¼å¼
                evaluations.append(case["evaluation"])
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœï¼Œæ— æ³•ä¼˜åŒ–
        if not evaluations:
            st.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä¼°ç»“æœï¼Œæ— æ³•ç”Ÿæˆä¼˜åŒ–æç¤ºè¯")
            return
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = PromptOptimizer()
        
        # ç”Ÿæˆä¼˜åŒ–æç¤ºè¯
        optimization_result = optimizer.optimize_prompt_sync(
            template.get("template", ""),
            evaluations,
            optimization_strategy
        )
        
        if "error" in optimization_result:
            st.error(f"ä¼˜åŒ–å¤±è´¥: {optimization_result['error']}")
        else:
            optimized_prompts = optimization_result.get("optimized_prompts", [])
            
            if not optimized_prompts:
                st.warning("æœªèƒ½ç”Ÿæˆä¼˜åŒ–æç¤ºè¯")
                return
            
            # å°†ä¼˜åŒ–ç»“æœä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.optimized_prompts = optimized_prompts
            st.success(f"æˆåŠŸç”Ÿæˆ {len(optimized_prompts)} ä¸ªä¼˜åŒ–æç¤ºè¯ç‰ˆæœ¬")
            
            # æ˜¾ç¤ºä¼˜åŒ–æç¤ºè¯
            display_optimized_prompts(optimized_prompts, template, model, model_provider)
            
            # è‡ªåŠ¨è¿›è¡Œæ‰¹é‡è¯„ä¼°
            if auto_evaluate:
                # åˆ›å»ºä¼˜åŒ–åçš„æ¨¡æ¿åˆ—è¡¨
                optimized_templates = []
                for i, opt_prompt in enumerate(optimized_prompts):
                    optimized_template = dict(template)
                    optimized_template["name"] = f"{template.get('name', '')}çš„ä¼˜åŒ–ç‰ˆæœ¬_{i+1}"
                    optimized_template["description"] = f"ä¼˜åŒ–ç­–ç•¥: {opt_prompt.get('strategy', '')}"
                    optimized_template["template"] = opt_prompt.get("prompt", "")
                    optimized_templates.append(optimized_template)
                
                # ä¿å­˜æ‰¹é‡A/Bæµ‹è¯•æ‰€éœ€æ•°æ®åˆ°ä¼šè¯çŠ¶æ€
                st.session_state.batch_ab_test_original = template
                st.session_state.batch_ab_test_optimized = optimized_templates
                st.session_state.batch_ab_test_model = model
                st.session_state.batch_ab_test_model_provider = model_provider
                st.session_state.batch_ab_test_test_set = st.session_state.specialized_test_set_name
                
                # è·³è½¬åˆ°æ‰¹é‡A/Bæµ‹è¯•é¡µé¢
                st.session_state.page = "prompt_batch_ab_test"
                st.experimental_rerun()

def display_optimized_prompts(optimized_prompts, template, model, model_provider):
    """æ˜¾ç¤ºä¼˜åŒ–æç¤ºè¯ç»“æœ"""
    if not optimized_prompts:
        st.warning("æ²¡æœ‰ä¼˜åŒ–æç¤ºè¯å¯æ˜¾ç¤º")
        return
        
    st.success(f"ç”Ÿæˆäº† {len(optimized_prompts)} ä¸ªä¼˜åŒ–æç¤ºè¯ç‰ˆæœ¬")
    
    # åªæœ‰åœ¨æœªé€‰æ‹©è‡ªåŠ¨è¯„ä¼°æ—¶æ‰æ˜¾ç¤ºæ‰¹é‡è¯„ä¼°æŒ‰é’®
    if st.button("ğŸ”¬ æ‰¹é‡è¯„ä¼°æ‰€æœ‰ä¼˜åŒ–ç‰ˆæœ¬", type="primary"):
        # åˆ›å»ºä¼˜åŒ–åçš„æ¨¡æ¿åˆ—è¡¨
        optimized_templates = []
        for i, opt_prompt in enumerate(optimized_prompts):
            optimized_template = dict(template)
            optimized_template["name"] = f"{template.get('name', '')}çš„ä¼˜åŒ–ç‰ˆæœ¬_{i+1}"
            optimized_template["description"] = f"ä¼˜åŒ–ç­–ç•¥: {opt_prompt.get('strategy', '')}"
            optimized_template["template"] = opt_prompt.get("prompt", "")
            optimized_templates.append(optimized_template)
        
        # ä¿å­˜æ‰¹é‡A/Bæµ‹è¯•æ‰€éœ€æ•°æ®åˆ°ä¼šè¯çŠ¶æ€
        st.session_state.batch_ab_test_original = template
        st.session_state.batch_ab_test_optimized = optimized_templates
        st.session_state.batch_ab_test_model = model
        st.session_state.batch_ab_test_model_provider = model_provider
        st.session_state.batch_ab_test_test_set = st.session_state.specialized_test_set_name
        
        # è·³è½¬åˆ°æ‰¹é‡A/Bæµ‹è¯•é¡µé¢
        st.session_state.page = "prompt_batch_ab_test"
        st.experimental_rerun()
    
    # æ˜¾ç¤ºå„ä¸ªä¼˜åŒ–æç¤ºè¯ç‰ˆæœ¬
    for i, opt_prompt in enumerate(optimized_prompts):
        with st.expander(f"ä¼˜åŒ–ç‰ˆæœ¬ {i+1}: {opt_prompt.get('strategy', 'æœªçŸ¥ç­–ç•¥')}"):
            # ä¼˜åŒ–ç­–ç•¥éƒ¨åˆ†
            st.markdown("**ä¼˜åŒ–ç­–ç•¥:**")
            st.write(opt_prompt.get("strategy", ""))
            
            # æ˜¾ç¤ºé’ˆå¯¹è§£å†³çš„é—®é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
            if "problem_addressed" in opt_prompt:
                st.markdown("**é’ˆå¯¹è§£å†³çš„é—®é¢˜:**")
                st.info(opt_prompt.get("problem_addressed", ""))
            
            # é¢„æœŸæ”¹è¿›
            st.markdown("**é¢„æœŸæ”¹è¿›:**")
            st.write(opt_prompt.get("expected_improvements", ""))
            
            # ä¼˜åŒ–ç†ç”±ï¼ˆå¦‚æœæœ‰ï¼‰
            if "reasoning" in opt_prompt:
                st.markdown("**ä¼˜åŒ–ç†ç”±:**")
                st.info(opt_prompt.get("reasoning", ""))
            
            # æ˜¾ç¤ºä¼˜åŒ–åçš„æç¤ºè¯
            st.markdown("**ä¼˜åŒ–åçš„æç¤ºè¯:**")
            st.code(opt_prompt.get("prompt", ""))
            
            # åˆ›å»ºæŒ‰é’®ï¼Œå°†ä¼˜åŒ–åçš„æç¤ºè¯ä¿å­˜ä¸ºæ–°æ¨¡æ¿æˆ–è¿è¡ŒA/Bæµ‹è¯•
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button(f"ä¿å­˜ä¸ºæ–°æ¨¡æ¿", key=f"save_opt_{i}"):
                    # å¤åˆ¶åŸå§‹æ¨¡æ¿ï¼Œæ›¿æ¢æç¤ºè¯å†…å®¹
                    new_template = dict(template)
                    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
                    new_template["name"] = f"{template.get('name', 'template')}_{current_time}_v{i+1}"
                    new_template["description"] = f"ä» '{template.get('name', 'unknown')}' ä¼˜åŒ–: {opt_prompt.get('strategy', '')}"
                    new_template["template"] = opt_prompt.get("prompt", "")
                    
                    save_template(new_template["name"], new_template)
                    st.success(f"å·²ä¿å­˜ä¸ºæ–°æ¨¡æ¿: {new_template['name']}")
            
            with col2:
                if st.button(f"A/Bæµ‹è¯•", key=f"test_opt_{i}"):
                    # åˆ›å»ºä¼˜åŒ–åçš„æ¨¡æ¿
                    optimized_template = dict(template)
                    optimized_template["name"] = f"{template.get('name', '')}çš„ä¼˜åŒ–ç‰ˆæœ¬_{i+1}"
                    optimized_template["description"] = f"ä¼˜åŒ–ç­–ç•¥: {opt_prompt.get('strategy', '')}"
                    optimized_template["template"] = opt_prompt.get("prompt", "")
                    
                    # ä¿å­˜A/Bæµ‹è¯•æ‰€éœ€æ•°æ®åˆ°ä¼šè¯çŠ¶æ€
                    st.session_state.ab_test_original = template
                    st.session_state.ab_test_optimized = optimized_template
                    st.session_state.ab_test_model = model
                    st.session_state.ab_test_model_provider = model_provider
                    st.session_state.ab_test_test_set = st.session_state.specialized_test_set_name
                    
                    # è·³è½¬åˆ°A/Bæµ‹è¯•é¡µé¢
                    st.session_state.page = "prompt_ab_test"
                    st.experimental_rerun()
