import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from typing import Dict, List, Any, Optional
import json
import streamlit as st

def create_score_comparison_chart(results: Dict[str, Dict]) -> go.Figure:
    """åˆ›å»ºä¸åŒæç¤ºè¯ç‰ˆæœ¬å¾—åˆ†å¯¹æ¯”å›¾"""
    # å‡†å¤‡æ•°æ®
    prompts = []
    scores = []
    categories = []
    
    for prompt_name, prompt_results in results.items():
        for test_case in prompt_results.get("test_cases", []):
            evaluation = test_case.get("evaluation") or {}
            score_dict = evaluation.get("scores") or {}
            for score_name, score_value in score_dict.items():
                prompts.append(prompt_name)
                scores.append(score_value)
                categories.append(score_name)

    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¿”å›æç¤ºå›¾è¡¨
    if not prompts:
        fig = go.Figure()
        fig.add_annotation(
            text="æ— æœ‰æ•ˆè¯„ä¼°æ•°æ®",
            showarrow=False,
            font=dict(size=20)
        )
        return fig

    df = pd.DataFrame({
        "æç¤ºè¯": prompts,
        "åˆ†æ•°": scores,
        "ç±»åˆ«": categories
    })
    
    # åˆ›å»ºå›¾è¡¨
    fig = px.bar(
        df, 
        x="æç¤ºè¯", 
        y="åˆ†æ•°", 
        color="ç±»åˆ«",
        barmode="group",
        title="æç¤ºè¯æ€§èƒ½å¯¹æ¯”",
        labels={"æç¤ºè¯": "æç¤ºè¯ç‰ˆæœ¬", "åˆ†æ•°": "è¯„åˆ† (0-100)", "ç±»åˆ«": "è¯„ä¼°ç»´åº¦"},
        height=500
    )
    
    return fig

def create_token_comparison_chart(results: Dict[str, Dict]) -> go.Figure:
    """åˆ›å»ºä¸åŒæç¤ºè¯ç‰ˆæœ¬tokenä½¿ç”¨å¯¹æ¯”å›¾"""
    # å‡†å¤‡æ•°æ®
    prompts = []
    token_counts = []
    
    for prompt_name, prompt_results in results.items():
        avg_tokens = 0
        count = 0
        
        for test_case in prompt_results.get("test_cases", []):
            if "prompt_info" in test_case.get("evaluation", {}):
                avg_tokens += test_case["evaluation"]["prompt_info"]["token_count"]
                count += 1
        
        if count > 0:
            prompts.append(prompt_name)
            token_counts.append(avg_tokens / count)
    
    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¿”å›æç¤ºå›¾è¡¨
    if not prompts:
        fig = go.Figure()
        fig.add_annotation(
            text="æ— æœ‰æ•ˆè¯„ä¼°æ•°æ®",
            showarrow=False,
            font=dict(size=20)
        )
        return fig
        
    # åˆ›å»ºå›¾è¡¨
    fig = px.bar(
        x=prompts,
        y=token_counts,
        title="æç¤ºè¯Tokené•¿åº¦å¯¹æ¯”",
        labels={"x": "æç¤ºè¯ç‰ˆæœ¬", "y": "å¹³å‡Tokenæ•°"},
        height=400
    )
    
    return fig

def create_radar_chart(results: Dict[str, Dict]) -> go.Figure:
    """åˆ›å»ºé›·è¾¾å›¾å±•ç¤ºä¸åŒæç¤ºè¯åœ¨å„ç»´åº¦çš„è¡¨ç°"""
    # å‡†å¤‡æ•°æ®
    prompt_names = list(results.keys())
    categories = ["accuracy", "completeness", "relevance", "clarity"]
    category_names = {
        "accuracy": "å‡†ç¡®æ€§",
        "completeness": "å®Œæ•´æ€§",
        "relevance": "ç›¸å…³æ€§", 
        "clarity": "æ¸…æ™°åº¦"
    }
    
    fig = go.Figure()
    
    for prompt_name in prompt_names:
        prompt_results = results[prompt_name]
        
        # è®¡ç®—å„ç»´åº¦çš„å¹³å‡åˆ†
        category_scores = {cat: 0 for cat in categories}
        count = 0
        
        for test_case in prompt_results.get("test_cases", []):
            for cat in categories:
                if cat in test_case.get("evaluation", {}).get("scores", {}):
                    category_scores[cat] += test_case["evaluation"]["scores"][cat]
            count += 1
        
        if count > 0:
            for cat in categories:
                category_scores[cat] /= count
        
        # æ·»åŠ é›·è¾¾å›¾
        fig.add_trace(go.Scatterpolar(
            r=[category_scores[cat] for cat in categories],
            theta=[category_names[cat] for cat in categories],
            fill='toself',
            name=prompt_name
        ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100]
            )
        ),
        showlegend=True,
        title="æç¤ºè¯å¤šç»´åº¦æ€§èƒ½å¯¹æ¯”"
    )
    
    return fig

def generate_report(results: Dict[str, Dict]) -> Dict:
    """ç”Ÿæˆæµ‹è¯•ç»“æœåˆ†ææŠ¥å‘Š"""
    report = {
        "summary": {},
        "prompt_comparison": [],
        "recommendations": []
    }
    
    # è®¡ç®—æ¯ä¸ªæç¤ºè¯çš„å¹³å‡å¾—åˆ†
    for prompt_name, prompt_results in results.items():
        overall_scores = []
        for test_case in prompt_results.get("test_cases", []):
            if "overall_score" in test_case.get("evaluation", {}):
                overall_scores.append(test_case["evaluation"]["overall_score"])
        
        if overall_scores:
            avg_score = sum(overall_scores) / len(overall_scores)
            report["summary"][prompt_name] = {
                "average_score": avg_score,
                "test_cases_count": len(overall_scores)
            }
    
    # æ‰¾å‡ºæœ€ä½³æç¤ºè¯
    if report["summary"]:
        best_prompt = max(report["summary"].items(), key=lambda x: x[1]["average_score"])
        report["best_prompt"] = {
            "name": best_prompt[0],
            "score": best_prompt[1]["average_score"]
        }
    
    # æç¤ºè¯å¯¹æ¯”
    for prompt_name, prompt_data in report["summary"].items():
        strengths = []
        weaknesses = []
        
        # åˆ†æå¼ºé¡¹å’Œå¼±é¡¹
        prompt_results = results[prompt_name]
        dimension_scores = {"accuracy": 0, "completeness": 0, "relevance": 0, "clarity": 0}
        count = 0
        
        for test_case in prompt_results.get("test_cases", []):
            for dim, score in test_case.get("evaluation", {}).get("scores", {}).items():
                dimension_scores[dim] += score
            count += 1
        
        if count > 0:
            for dim in dimension_scores:
                dimension_scores[dim] /= count
                if dimension_scores[dim] >= 85:
                    strengths.append(f"{dim}ç»´åº¦è¡¨ç°å‡ºè‰²ï¼ˆ{dimension_scores[dim]:.1f}åˆ†ï¼‰")
                elif dimension_scores[dim] < 70:
                    weaknesses.append(f"{dim}ç»´åº¦éœ€è¦æ”¹è¿›ï¼ˆ{dimension_scores[dim]:.1f}åˆ†ï¼‰")
        
        # è®¡ç®—Tokenæ•ˆç‡
        avg_tokens = 0
        token_count = 0
        for test_case in prompt_results.get("test_cases", []):
            if "prompt_info" in test_case.get("evaluation", {}):
                avg_tokens += test_case["evaluation"]["prompt_info"]["token_count"]
                token_count += 1
        
        token_efficiency = "æœªçŸ¥"
        if token_count > 0:
            avg_tokens = avg_tokens / token_count
            if prompt_data["average_score"] > 0:
                # å®šä¹‰æ•ˆç‡ä¸ºåˆ†æ•°/tokenæ•°çš„æ¯”ç‡
                efficiency = prompt_data["average_score"] / avg_tokens * 100
                if efficiency > 0.5:
                    token_efficiency = "é«˜"
                elif efficiency > 0.3:
                    token_efficiency = "ä¸­"
                else:
                    token_efficiency = "ä½"
        
        report["prompt_comparison"].append({
            "name": prompt_name,
            "average_score": prompt_data["average_score"],
            "strengths": strengths,
            "weaknesses": weaknesses,
            "token_count": avg_tokens if token_count > 0 else "æœªçŸ¥",
            "token_efficiency": token_efficiency
        })
    
    # ç”Ÿæˆå»ºè®®
    if report["prompt_comparison"]:
        # æŒ‰å¹³å‡åˆ†æ’åº
        sorted_prompts = sorted(report["prompt_comparison"], 
                               key=lambda x: x["average_score"], reverse=True)
        
        # æå–æœ€ä½³æç¤ºè¯çš„ç‰¹ç‚¹
        if sorted_prompts:
            best_prompt = sorted_prompts[0]
            report["recommendations"].append(
                f"æœ€ä½³æç¤ºè¯æ˜¯ '{best_prompt['name']}'ï¼Œå¹³å‡å¾—åˆ†ä¸º {best_prompt['average_score']:.1f}åˆ†ã€‚"
            )
            
            if best_prompt["strengths"]:
                report["recommendations"].append(
                    f"å…¶ä¸»è¦ä¼˜åŠ¿åœ¨äº: {', '.join(best_prompt['strengths'])}"
                )
            
            # å¦‚æœæœ‰å¤šä¸ªæç¤ºè¯ï¼Œåˆ†æå·®å¼‚
            if len(sorted_prompts) > 1:
                worst_prompt = sorted_prompts[-1]
                score_diff = best_prompt["average_score"] - worst_prompt["average_score"]
                
                if score_diff > 15:
                    report["recommendations"].append(
                        f"æç¤ºè¯ä¹‹é—´çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œæœ€ä½³ä¸æœ€å·®æç¤ºè¯çš„åˆ†æ•°ç›¸å·® {score_diff:.1f} åˆ†ã€‚"
                    )
                    
                    if worst_prompt["weaknesses"]:
                        report["recommendations"].append(
                            f"è¡¨ç°æœ€å·®çš„æç¤ºè¯ '{worst_prompt['name']}' ä¸»è¦é—®é¢˜åœ¨äº: "
                            f"{', '.join(worst_prompt['weaknesses'])}"
                        )
    
    # å¯¹tokenæ•ˆç‡çš„å»ºè®®
    token_efficiencies = [p.get("token_efficiency") for p in report["prompt_comparison"] 
                         if p.get("token_efficiency") != "æœªçŸ¥"]
    
    if "ä½" in token_efficiencies:
        report["recommendations"].append(
            "éƒ¨åˆ†æç¤ºè¯çš„tokenæ•ˆç‡è¾ƒä½ï¼Œå»ºè®®ç²¾ç®€æç¤ºè¯ç»“æ„ï¼Œå‡å°‘å†—ä½™å†…å®¹ï¼Œæé«˜ä¿¡æ¯å¯†åº¦ã€‚"
        )
    
    return report

def display_report(report: Dict) -> None:
    """åœ¨Streamlitä¸­å±•ç¤ºæŠ¥å‘Š"""
    st.header("ğŸ“Š æµ‹è¯•ç»“æœåˆ†ææŠ¥å‘Š")
    
    # æ˜¾ç¤ºæœ€ä½³æç¤ºè¯
    if "best_prompt" in report:
        st.subheader("ğŸ† æœ€ä½³æç¤ºè¯")
        st.success(
            f"**{report['best_prompt']['name']}** (å¹³å‡å¾—åˆ†: "
            f"{report['best_prompt']['score']:.1f}åˆ†)"
        )
    
    # æ˜¾ç¤ºæç¤ºè¯å¯¹æ¯”
    if report["prompt_comparison"]:
        st.subheader("ğŸ“ˆ æç¤ºè¯å¯¹æ¯”")
        
        # åˆ›å»ºè¡¨æ ¼æ•°æ®
        comparison_data = {
            "æç¤ºè¯": [],
            "å¹³å‡å¾—åˆ†": [],
            "Tokenæ•°": [],
            "Tokenæ•ˆç‡": [],
            "ä¼˜åŠ¿": [],
            "åŠ£åŠ¿": []
        }
        
        for prompt in report["prompt_comparison"]:
            comparison_data["æç¤ºè¯"].append(prompt["name"])
            comparison_data["å¹³å‡å¾—åˆ†"].append(f"{prompt['average_score']:.1f}")
            comparison_data["Tokenæ•°"].append(str(prompt["token_count"]))
            comparison_data["Tokenæ•ˆç‡"].append(prompt["token_efficiency"])
            comparison_data["ä¼˜åŠ¿"].append("\n".join(prompt["strengths"]) if prompt["strengths"] else "æ— ç‰¹åˆ«ä¼˜åŠ¿")
            comparison_data["åŠ£åŠ¿"].append("\n".join(prompt["weaknesses"]) if prompt["weaknesses"] else "æ— æ˜æ˜¾åŠ£åŠ¿")
        
        st.dataframe(pd.DataFrame(comparison_data))
    
    # æ˜¾ç¤ºå»ºè®®
    if report["recommendations"]:
        st.subheader("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
        for i, rec in enumerate(report["recommendations"]):
            st.markdown(f"{i+1}. {rec}")