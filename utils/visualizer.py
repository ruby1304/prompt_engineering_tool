import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from typing import Dict, List, Any, Optional
import json
import streamlit as st

def create_score_comparison_chart(results: Dict[str, Dict]) -> go.Figure:
    """åˆ›å»ºä¸åŒæç¤ºè¯ç‰ˆæœ¬å¾—åˆ†å¯¹æ¯”å›¾"""
    # å‡†å¤‡æ•°æ®
    prompts = []
    scores = []
    categories = []

    for prompt_name, prompt_results in results.items():
        for test_case in prompt_results.get("test_cases", []):
            # ä» responses[0] è·å– evaluation
            response_list = test_case.get("responses", [])
            if not response_list:
                continue
            evaluation = response_list[0].get("evaluation")  # Get evaluation from the first response

            if evaluation:  # Check if evaluation exists
                score_dict = evaluation.get("scores")  # Get scores if evaluation exists
                if score_dict: # Check if score_dict exists and is not empty
                    for score_name, score_value in score_dict.items():
                        if score_value is not None: # Ensure score value is valid
                            prompts.append(prompt_name)
                            scores.append(score_value)
                            categories.append(score_name)

    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¿”å›æç¤ºå›¾è¡¨
    if not prompts:
        fig = go.Figure()
        fig.add_annotation(
            text="æ— æœ‰æ•ˆè¯„ä¼°æ•°æ®",
            showarrow=False,
            font=dict(size=20)
        )
        return fig

    df = pd.DataFrame({
        "æç¤ºè¯": prompts,
        "åˆ†æ•°": scores,
        "ç±»åˆ«": categories
    })

    # åˆ›å»ºå›¾è¡¨
    fig = px.bar(
        df,
        x="æç¤ºè¯",
        y="åˆ†æ•°",
        color="ç±»åˆ«",
        barmode="group",
        title="æç¤ºè¯æ€§èƒ½å¯¹æ¯”",
        labels={"æç¤ºè¯": "æç¤ºè¯ç‰ˆæœ¬", "åˆ†æ•°": "è¯„åˆ† (0-100)", "ç±»åˆ«": "è¯„ä¼°ç»´åº¦"},
        height=500
    )

    return fig

def create_token_comparison_chart(results: Dict[str, Dict]) -> go.Figure:
    """åˆ›å»ºä¸åŒæç¤ºè¯ç‰ˆæœ¬tokenä½¿ç”¨å¯¹æ¯”å›¾"""
    # å‡†å¤‡æ•°æ®
    prompts = []
    token_counts = []

    for prompt_name, prompt_results in results.items():
        total_tokens = 0
        count = 0

        for test_case in prompt_results.get("test_cases", []):
            # ä» responses[0] è·å– evaluation
            response_list = test_case.get("responses", [])
            if not response_list:
                continue
            evaluation = response_list[0].get("evaluation")  # Get evaluation from the first response

            if evaluation: # Check if evaluation exists
                prompt_info = evaluation.get("prompt_info") # Get prompt_info if evaluation exists
                if prompt_info and "token_count" in prompt_info and prompt_info["token_count"] is not None: # Check if prompt_info and token_count exist and are valid
                    total_tokens += prompt_info["token_count"]
                    count += 1

        if count > 0:
            prompts.append(prompt_name)
            token_counts.append(total_tokens / count)

    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¿”å›æç¤ºå›¾è¡¨
    if not prompts:
        fig = go.Figure()
        fig.add_annotation(
            text="æ— æœ‰æ•ˆè¯„ä¼°æ•°æ®",
            showarrow=False,
            font=dict(size=20)
        )
        return fig

    # åˆ›å»ºå›¾è¡¨
    fig = px.bar(
        x=prompts,
        y=token_counts,
        title="æç¤ºè¯Tokené•¿åº¦å¯¹æ¯”",
        labels={"x": "æç¤ºè¯ç‰ˆæœ¬", "y": "å¹³å‡Tokenæ•°"},
        height=400
    )

    return fig

def create_radar_chart(results: Dict[str, Dict]) -> go.Figure:
    """åˆ›å»ºé›·è¾¾å›¾å±•ç¤ºä¸åŒæç¤ºè¯åœ¨å„ç»´åº¦çš„è¡¨ç°"""
    # å‡†å¤‡æ•°æ®
    prompt_names = list(results.keys())
    # Try to dynamically get categories from the first valid evaluation scores
    categories = []
    for prompt_name in prompt_names:
        for test_case in results[prompt_name].get("test_cases", []):
            # ä» responses[0] è·å– evaluation
            response_list = test_case.get("responses", [])
            if not response_list:
                continue
            evaluation = response_list[0].get("evaluation")  # Get evaluation from the first response

            if evaluation and evaluation.get("scores"):
                categories = list(evaluation["scores"].keys())
                break
        if categories:
            break

    # Default categories if none found
    if not categories:
        categories = ["accuracy", "completeness", "relevance", "clarity"]

    category_names = { # Keep default names, can be expanded if needed
        "accuracy": "å‡†ç¡®æ€§",
        "completeness": "å®Œæ•´æ€§",
        "relevance": "ç›¸å…³æ€§",
        "clarity": "æ¸…æ™°åº¦"
    }
    # Add any new categories found with their own name
    for cat in categories:
        if cat not in category_names:
            category_names[cat] = cat.capitalize()


    fig = go.Figure()
    data_found = False # Flag to check if any data was added

    for prompt_name in prompt_names:
        prompt_results = results[prompt_name]

        # è®¡ç®—å„ç»´åº¦çš„å¹³å‡åˆ†
        category_scores = {cat: 0 for cat in categories}
        category_counts = {cat: 0 for cat in categories} # Count per category

        for test_case in prompt_results.get("test_cases", []):
            # ä» responses[0] è·å– evaluation
            response_list = test_case.get("responses", [])
            if not response_list:
                continue
            evaluation = response_list[0].get("evaluation")  # Get evaluation from the first response

            if evaluation: # Check if evaluation exists
                scores = evaluation.get("scores") # Get scores if evaluation exists
                if scores: # Check if scores exist
                    for cat in categories:
                        if cat in scores and scores[cat] is not None: # Check category exists and score is valid
                            category_scores[cat] += scores[cat]
                            category_counts[cat] += 1

        # Calculate average scores only for categories with data
        avg_category_scores = []
        valid_categories_names = []
        prompt_has_data = False
        for cat in categories:
            if category_counts[cat] > 0:
                avg_score = category_scores[cat] / category_counts[cat]
                avg_category_scores.append(avg_score)
                valid_categories_names.append(category_names.get(cat, cat.capitalize()))
                prompt_has_data = True
            else:
                # Handle missing data - append 0 or None, depending on desired visualization
                avg_category_scores.append(0) # Or None
                valid_categories_names.append(category_names.get(cat, cat.capitalize()))


        # æ·»åŠ é›·è¾¾å›¾ trace only if this prompt had some valid data
        if prompt_has_data:
            fig.add_trace(go.Scatterpolar(
                r=avg_category_scores,
                theta=valid_categories_names, # Use names corresponding to the scores calculated
                fill='toself',
                name=prompt_name
            ))
            data_found = True # Mark that we added at least one trace

    # Add annotation if no traces were added at all
    if not data_found:
        fig.add_annotation(
            text="æ— æœ‰æ•ˆè¯„ä¼°æ•°æ®",
            showarrow=False,
            font=dict(size=20)
        )
        # Add dummy trace to show layout if no data
        fig.add_trace(go.Scatterpolar(r=[0]*len(categories), theta=[category_names.get(cat, cat.capitalize()) for cat in categories]))


    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100] # Assuming scores are 0-100
            )
        ),
        showlegend=True,
        title="æç¤ºè¯å¤šç»´åº¦æ€§èƒ½å¯¹æ¯”"
    )

    return fig

def generate_report(results: Dict[str, Dict]) -> Dict:
    """ç”Ÿæˆæµ‹è¯•ç»“æœåˆ†ææŠ¥å‘Š"""
    report = {
        "summary": {},
        "prompt_comparison": [],
        "model_comparison": [],
        "recommendations": [],
        "is_model_comparison": False
    }

    # Determine the scenario: multi-prompt or single-prompt/multi-model
    prompt_names = list(results.keys())
    is_single_prompt = len(prompt_names) == 1
    models_in_single_prompt = set()
    if is_single_prompt:
        prompt_data = results[prompt_names[0]]
        for case in prompt_data.get("test_cases", []):
            models_in_single_prompt.add(case.get("model"))
        if len(models_in_single_prompt) > 1:
            report["is_model_comparison"] = True

    # --- Model Comparison Logic ---
    if report["is_model_comparison"]:
        prompt_name = prompt_names[0]
        prompt_data = results[prompt_name]
        model_results = {model: {"scores": [], "dimensions": {}, "token_counts": []} for model in models_in_single_prompt}

        # Aggregate results per model
        for case in prompt_data.get("test_cases", []):
            model = case.get("model")
            if not model:
                continue
            
            response_list = case.get("responses", [])
            if not response_list:
                continue
            evaluation = response_list[0].get("evaluation")
            if not evaluation:
                continue

            if "overall_score" in evaluation:
                model_results[model]["scores"].append(evaluation["overall_score"])
            
            if "scores" in evaluation:
                for dim, score in evaluation["scores"].items():
                    if score is not None:
                        if dim not in model_results[model]["dimensions"]:
                            model_results[model]["dimensions"][dim] = []
                        model_results[model]["dimensions"][dim].append(score)

            if "prompt_info" in evaluation and "token_count" in evaluation["prompt_info"]:
                 token_count = evaluation["prompt_info"]["token_count"]
                 if token_count is not None:
                    model_results[model]["token_counts"].append(token_count)

        # Calculate averages and analyze per model
        for model, data in model_results.items():
            avg_score = sum(data["scores"]) / len(data["scores"]) if data["scores"] else 0
            report["summary"][model] = {
                "average_score": avg_score,
                "test_cases_count": len(data["scores"])
            }

            strengths = []
            weaknesses = []
            avg_dimension_scores = {}
            for dim, scores in data["dimensions"].items():
                avg_dim_score = sum(scores) / len(scores) if scores else 0
                avg_dimension_scores[dim] = avg_dim_score
                dim_name = {"accuracy": "å‡†ç¡®æ€§", "completeness": "å®Œæ•´æ€§", "relevance": "ç›¸å…³æ€§", "clarity": "æ¸…æ™°åº¦"}.get(dim, dim.capitalize())
                if avg_dim_score >= 85:
                    strengths.append(f"{dim_name}ç»´åº¦è¡¨ç°å‡ºè‰²ï¼ˆ{avg_dim_score:.1f}åˆ†ï¼‰")
                elif avg_dim_score < 70:
                    weaknesses.append(f"{dim_name}ç»´åº¦éœ€è¦æ”¹è¿›ï¼ˆ{avg_dim_score:.1f}åˆ†ï¼‰")

            avg_tokens = sum(data["token_counts"]) / len(data["token_counts"]) if data["token_counts"] else 0
            token_efficiency = "æœªçŸ¥"
            if avg_tokens > 0 and avg_score > 0:
                efficiency = avg_score / avg_tokens * 100
                if efficiency > 0.5:
                    token_efficiency = "é«˜"
                elif efficiency > 0.3:
                    token_efficiency = "ä¸­"
                else:
                    token_efficiency = "ä½"

            report["model_comparison"].append({
                "name": model,
                "average_score": avg_score,
                "strengths": strengths,
                "weaknesses": weaknesses,
                "token_count": avg_tokens if data["token_counts"] else "æœªçŸ¥",
                "token_efficiency": token_efficiency
            })
        
        # Find best model
        if report["summary"]:
            best_model_item = max(report["summary"].items(), key=lambda x: x[1]["average_score"])
            report["best_model"] = {
                "name": best_model_item[0],
                "score": best_model_item[1]["average_score"]
            }
            
        # Generate model recommendations
        if report["model_comparison"]:
            sorted_models = sorted(report["model_comparison"], key=lambda x: x["average_score"], reverse=True)
            if sorted_models:
                best_model = sorted_models[0]
                report["recommendations"].append(f"è¡¨ç°æœ€ä½³çš„æ¨¡å‹æ˜¯ '{best_model['name']}'ï¼Œå¹³å‡å¾—åˆ†ä¸º {best_model['average_score']:.1f}åˆ†ã€‚")
                if best_model["strengths"]:
                    report["recommendations"].append(f"å…¶ä¸»è¦ä¼˜åŠ¿åœ¨äº: {', '.join(best_model['strengths'])}")
                
                if len(sorted_models) > 1:
                    worst_model = sorted_models[-1]
                    score_diff = best_model["average_score"] - worst_model["average_score"]
                    if score_diff > 10: # Threshold for significant difference
                        report["recommendations"].append(f"æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚è¾ƒæ˜æ˜¾ï¼Œæœ€ä½³ä¸æœ€å·®æ¨¡å‹çš„åˆ†æ•°ç›¸å·® {score_diff:.1f} åˆ†ã€‚")
                        if worst_model["weaknesses"]:
                            report["recommendations"].append(f"è¡¨ç°è¾ƒå·®çš„æ¨¡å‹ '{worst_model['name']}' ä¸»è¦é—®é¢˜åœ¨äº: {', '.join(worst_model['weaknesses'])}")
            
            token_efficiencies = [m.get("token_efficiency") for m in report["model_comparison"] if m.get("token_efficiency") != "æœªçŸ¥"]
            if "ä½" in token_efficiencies:
                 report["recommendations"].append("éƒ¨åˆ†æ¨¡å‹çš„tokenæ•ˆç‡è¾ƒä½ï¼Œè€ƒè™‘é€‰æ‹©æ•ˆç‡æ›´é«˜çš„æ¨¡å‹æˆ–ä¼˜åŒ–æç¤ºè¯ä»¥å‡å°‘tokenæ¶ˆè€—ã€‚")

    # --- Prompt Comparison Logic (Existing Logic) ---
    else:
        # Calculate average score per prompt (using the corrected access pattern)
        for prompt_name, prompt_results in results.items():
            overall_scores = []
            for test_case in prompt_results.get("test_cases", []):
                response_list = test_case.get("responses", [])
                if not response_list:
                    continue
                evaluation = response_list[0].get("evaluation")
                if evaluation and "overall_score" in evaluation:
                    overall_scores.append(evaluation["overall_score"])
            
            if overall_scores:
                avg_score = sum(overall_scores) / len(overall_scores)
                report["summary"][prompt_name] = {
                    "average_score": avg_score,
                    "test_cases_count": len(overall_scores)
                }
        
        # Find best prompt
        if report["summary"]:
            best_prompt_item = max(report["summary"].items(), key=lambda x: x[1]["average_score"])
            report["best_prompt"] = {
                "name": best_prompt_item[0],
                "score": best_prompt_item[1]["average_score"]
            }
        
        # Prompt comparison details (using corrected access pattern)
        for prompt_name, prompt_data_summary in report["summary"] .items(): # Iterate through calculated summary
            strengths = []
            weaknesses = []
            prompt_results = results[prompt_name]
            
            # Dynamically get categories
            first_eval_scores = {}
            for tc in prompt_results.get("test_cases", []):
                resp_list = tc.get("responses", [])
                if resp_list and resp_list[0].get("evaluation") and resp_list[0]["evaluation"].get("scores"):
                    first_eval_scores = resp_list[0]["evaluation"]["scores"]
                    break
            
            current_categories = list(first_eval_scores.keys()) if first_eval_scores else ["accuracy", "completeness", "relevance", "clarity"]
            dimension_scores = {cat: 0 for cat in current_categories}
            count = 0

            for test_case in prompt_results.get("test_cases", []):
                response_list = test_case.get("responses", [])
                if not response_list:
                    continue
                evaluation = response_list[0].get("evaluation")
                if evaluation and evaluation.get("scores"):
                    scores = evaluation["scores"]
                    for dim in current_categories:
                        if dim in scores and scores[dim] is not None:
                            dimension_scores[dim] += scores[dim]
                    count += 1
            
            if count > 0:
                for dim in dimension_scores:
                    dimension_scores[dim] /= count
                    dim_name = {"accuracy": "å‡†ç¡®æ€§", "completeness": "å®Œæ•´æ€§", "relevance": "ç›¸å…³æ€§", "clarity": "æ¸…æ™°åº¦"}.get(dim, dim.capitalize())
                    if dimension_scores[dim] >= 85:
                        strengths.append(f"{dim_name}ç»´åº¦è¡¨ç°å‡ºè‰²ï¼ˆ{dimension_scores[dim]:.1f}åˆ†ï¼‰")
                    elif dimension_scores[dim] < 70:
                        weaknesses.append(f"{dim_name}ç»´åº¦éœ€è¦æ”¹è¿›ï¼ˆ{dimension_scores[dim]:.1f}åˆ†ï¼‰")

            avg_tokens = 0
            token_count = 0
            for test_case in prompt_results.get("test_cases", []):
                response_list = test_case.get("responses", [])
                if not response_list:
                    continue
                evaluation = response_list[0].get("evaluation")
                if evaluation and "prompt_info" in evaluation:
                    prompt_info = evaluation["prompt_info"]
                    if "token_count" in prompt_info and prompt_info["token_count"] is not None:
                        avg_tokens += prompt_info["token_count"]
                        token_count += 1
            
            token_efficiency = "æœªçŸ¥"
            avg_score_for_eff = prompt_data_summary["average_score"] 
            if token_count > 0 and avg_score_for_eff > 0: 
                avg_tokens = avg_tokens / token_count
                efficiency = avg_score_for_eff / avg_tokens * 100
                if efficiency > 0.5:
                    token_efficiency = "é«˜"
                elif efficiency > 0.3:
                    token_efficiency = "ä¸­"
                else:
                    token_efficiency = "ä½"

            report["prompt_comparison"].append({
                "name": prompt_name,
                "average_score": avg_score_for_eff,
                "strengths": strengths,
                "weaknesses": weaknesses,
                "token_count": avg_tokens if token_count > 0 else "æœªçŸ¥",
                "token_efficiency": token_efficiency
            })
        
        if report["prompt_comparison"]:
            sorted_prompts = sorted(report["prompt_comparison"], key=lambda x: x["average_score"], reverse=True)
            if sorted_prompts:
                best_prompt = sorted_prompts[0]
                report["recommendations"].append(f"æœ€ä½³æç¤ºè¯æ˜¯ '{best_prompt['name']}'ï¼Œå¹³å‡å¾—åˆ†ä¸º {best_prompt['average_score']:.1f}åˆ†ã€‚")
                if best_prompt["strengths"]:
                    report["recommendations"].append(f"å…¶ä¸»è¦ä¼˜åŠ¿åœ¨äº: {', '.join(best_prompt['strengths'])}")
                
                if len(sorted_prompts) > 1:
                    worst_prompt = sorted_prompts[-1]
                    score_diff = best_prompt["average_score"] - worst_prompt["average_score"]
                    if score_diff > 15:
                        report["recommendations"].append(f"æç¤ºè¯ä¹‹é—´çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œæœ€ä½³ä¸æœ€å·®æç¤ºè¯çš„åˆ†æ•°ç›¸å·® {score_diff:.1f} åˆ†ã€‚")
                        if worst_prompt["weaknesses"]:
                            report["recommendations"].append(f"è¡¨ç°æœ€å·®çš„æç¤ºè¯ '{worst_prompt['name']}' ä¸»è¦é—®é¢˜åœ¨äº: {', '.join(worst_prompt['weaknesses'])}")

            token_efficiencies = [p.get("token_efficiency") for p in report["prompt_comparison"] if p.get("token_efficiency") != "æœªçŸ¥"]
            if "ä½" in token_efficiencies:
                report["recommendations"].append("éƒ¨åˆ†æç¤ºè¯çš„tokenæ•ˆç‡è¾ƒä½ï¼Œå»ºè®®ç²¾ç®€æç¤ºè¯ç»“æ„ï¼Œå‡å°‘å†—ä½™å†…å®¹ï¼Œæé«˜ä¿¡æ¯å¯†åº¦ã€‚")

    return report

def display_report(report: Dict) -> None:
    """åœ¨Streamlitä¸­å±•ç¤ºæŠ¥å‘Š"""
    st.header("ğŸ“Š æµ‹è¯•ç»“æœåˆ†ææŠ¥å‘Š")

    if report.get("is_model_comparison"):
        if "best_model" in report:
            st.subheader("ğŸ† æœ€ä½³æ¨¡å‹")
            st.success(
                f"**{report['best_model']['name']}** (å¹³å‡å¾—åˆ†: "
                f"{report['best_model']['score']:.1f}åˆ†)"
            )
        
        if report["model_comparison"]:
            st.subheader("ğŸ“ˆ æ¨¡å‹å¯¹æ¯”")
            comparison_data = {
                "æ¨¡å‹": [],
                "å¹³å‡å¾—åˆ†": [],
                "Tokenæ•°": [],
                "Tokenæ•ˆç‡": [],
                "ä¼˜åŠ¿": [],
                "åŠ£åŠ¿": []
            }
            for model_comp in report["model_comparison"]:
                comparison_data["æ¨¡å‹"].append(model_comp["name"])
                comparison_data["å¹³å‡å¾—åˆ†"].append(f"{model_comp['average_score']:.1f}")
                comparison_data["Tokenæ•°"].append(str(model_comp["token_count"]))
                comparison_data["Tokenæ•ˆç‡"].append(model_comp["token_efficiency"])
                comparison_data["ä¼˜åŠ¿"].append("\n".join(model_comp["strengths"]) if model_comp["strengths"] else "æ— ç‰¹åˆ«ä¼˜åŠ¿")
                comparison_data["åŠ£åŠ¿"].append("\n".join(model_comp["weaknesses"]) if model_comp["weaknesses"] else "æ— æ˜æ˜¾åŠ£åŠ¿")
            st.dataframe(pd.DataFrame(comparison_data))

    else:
        if "best_prompt" in report:
            st.subheader("ğŸ† æœ€ä½³æç¤ºè¯")
            st.success(
                f"**{report['best_prompt']['name']}** (å¹³å‡å¾—åˆ†: "
                f"{report['best_prompt']['score']:.1f}åˆ†)"
            )
        
        if report["prompt_comparison"]:
            st.subheader("ğŸ“ˆ æç¤ºè¯å¯¹æ¯”")
            comparison_data = {
                "æç¤ºè¯": [],
                "å¹³å‡å¾—åˆ†": [],
                "Tokenæ•°": [],
                "Tokenæ•ˆç‡": [],
                "ä¼˜åŠ¿": [],
                "åŠ£åŠ¿": []
            }
            for prompt in report["prompt_comparison"]:
                comparison_data["æç¤ºè¯"].append(prompt["name"])
                comparison_data["å¹³å‡å¾—åˆ†"].append(f"{prompt['average_score']:.1f}")
                comparison_data["Tokenæ•°"].append(str(prompt["token_count"]))
                comparison_data["Tokenæ•ˆç‡"].append(prompt["token_efficiency"])
                comparison_data["ä¼˜åŠ¿"].append("\n".join(prompt["strengths"]) if prompt["strengths"] else "æ— ç‰¹åˆ«ä¼˜åŠ¿")
                comparison_data["åŠ£åŠ¿"].append("\n".join(prompt["weaknesses"]) if prompt["weaknesses"] else "æ— æ˜æ˜¾åŠ£åŠ¿")
            st.dataframe(pd.DataFrame(comparison_data))

    if report["recommendations"]:
        st.subheader("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
        for i, rec in enumerate(report["recommendations"]):
            st.markdown(f"{i+1}. {rec}")